<?xml version="1.0" encoding="iso-8859-1"?>
<rss version="2.0"><channel><title>Relevant arXiv Papers</title><link>https://Lybarger.github.io/arxiv_rss/rss_relevant.xml</link><description>Relevant arXiv Papers</description><lastBuildDate>Wed, 05 Feb 2025 06:53:31 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>All Entities are Not Created Equal: Examining the Long Tail for Fine-Grained Entity Typing</title><link>https://arxiv.org/abs/2410.17355</link><description>https://arxiv.org/abs/2410.17355&lt;br /&gt;The Language Model Linguistic Personality Assessment (LMLPA) system evaluates the distinct personality traits of Large Language Models (LLMs) by adapting a personality assessment questionnaire to their outputs, enabling quantitative measurement of their linguistic personalities and contributing to Human-Computer Interaction research.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Artificial Intelligence in Brazilian News: A Mixed-Methods Analysis</title><link>https://arxiv.org/abs/2410.17423</link><description>https://arxiv.org/abs/2410.17423&lt;br /&gt;The LMLPA introduces a system for assessing the linguistic personality traits of Large Language Models (LLMs) using an adapted version of the Big Five Inventory, demonstrating that LLMs exhibit distinct personality traits that can be quantitatively analyzed, thereby contributing to Human-Computer Interaction and Human-Centered AI.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Navigate Complex Physical Worlds via Geometrically Constrained LLM</title><link>https://arxiv.org/abs/2410.17529</link><description>https://arxiv.org/abs/2410.17529&lt;br /&gt;The Language Model Linguistic Personality Assessment (LMLPA) introduces a framework for evaluating the linguistic personalities of Large Language Models (LLMs) using an adapted version of the Big Five Inventory, enabling the quantification of personality traits reflected in their language generation outputs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models</title><link>https://arxiv.org/abs/2410.17578</link><description>https://arxiv.org/abs/2410.17578&lt;br /&gt;LMLPA introduces a framework for assessing the linguistic personalities of Large Language Models (LLMs) by adapting the Big Five Inventory to evaluate LLM-generated text, enhancing understanding of LLMs' language capabilities and contributing to fields like Human-Computer Interaction.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Cross-model Control: Improving Multiple Large Language Models in One-time Training</title><link>https://arxiv.org/abs/2410.17599</link><description>https://arxiv.org/abs/2410.17599&lt;br /&gt;The Language Model Linguistic Personality Assessment (LMLPA) introduces a system for evaluating the linguistic personalities of Large Language Models (LLMs) by adapting personality assessment frameworks to quantify personality traits reflected in their language generation capabilities.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Together We Can: Multilingual Automatic Post-Editing for Low-Resource Languages</title><link>https://arxiv.org/abs/2410.17973</link><description>https://arxiv.org/abs/2410.17973&lt;br /&gt;The paper presents a CPI+DMC approach for the early detection of signs of anorexia, emphasizing a time-aware methodology that integrates temporal factors into the learning process, leading to significant improvements in detection precision and speed as measured by the ERDE metrics.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Cross-lingual Transfer of Reward Models in Multilingual Alignment</title><link>https://arxiv.org/abs/2410.18027</link><description>https://arxiv.org/abs/2410.18027&lt;br /&gt;This research paper presents a time-aware approach for the early detection of anorexia that combines precision and speed as a single objective, utilizing temporal metrics in the learning process to achieve strong performance in the eRisk 2024 competition.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for Russian Scientific Keyphrases</title><link>https://arxiv.org/abs/2410.18040</link><description>https://arxiv.org/abs/2410.18040&lt;br /&gt;The paper presents a time-aware approach for early detection of anorexia, implementing a CPI+DMC methodology that balances precision and speed, and integrates temporal metrics in the learning process, achieving notable performance in risk detection tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering</title><link>https://arxiv.org/abs/2410.18050</link><description>https://arxiv.org/abs/2410.18050&lt;br /&gt;This research paper presents a time-aware approach to early detection of anorexia in an online environment, utilizing a CPI+DMC framework and incorporating temporal metrics to enhance both precision and speed in identifying early risk signs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Temporal Relational Reasoning of Large Language Models for Detecting Stock Portfolio Crashes</title><link>https://arxiv.org/abs/2410.17266</link><description>https://arxiv.org/abs/2410.17266&lt;br /&gt;This paper presents a time-aware approach for the early detection of anorexia, utilizing a combined model that optimizes both precision and speed in identifying risk signs, achieving notable results in the context of the eRisk 2024 competition.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Are Large Language Models Ready for Travel Planning?</title><link>https://arxiv.org/abs/2410.17333</link><description>https://arxiv.org/abs/2410.17333&lt;br /&gt;This paper presents a time-aware approach for the early detection of anorexia, integrating precision and speed into a combined objective during the learning process, leading to improved performance metrics in detecting early signs of anorexia.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Intera\c{c}\~ao entre rob\^os humanoides: desenvolvendo a colabora\c{c}\~ao e comunica\c{c}\~ao aut\^onoma</title><link>https://arxiv.org/abs/2410.17450</link><description>https://arxiv.org/abs/2410.17450&lt;br /&gt;The study proposes a time-aware approach to early detection of anorexia by integrating temporal factors into the learning process, focusing on both precision and speed in risk detection, and achieving strong results in established evaluation metrics.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation</title><link>https://arxiv.org/abs/2410.17462</link><description>https://arxiv.org/abs/2410.17462&lt;br /&gt;This paper presents a time-aware method for early detection of anorexia, incorporating precision and speed as combined objectives during training, which yielded strong performance in the eRisk 2024 competition.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Which Client is Reliable?: A Reliable and Personalized Prompt-based Federated Learning for Medical Image Question Answering</title><link>https://arxiv.org/abs/2410.17484</link><description>https://arxiv.org/abs/2410.17484&lt;br /&gt;This paper presents a time-aware approach for the early detection of anorexia, optimizing precision and speed in risk detection through a combined objective learning process and achieving exceptional results on relevant metrics.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Mechanisms of Symbol Processing for In-Context Learning in Transformer Networks</title><link>https://arxiv.org/abs/2410.17498</link><description>https://arxiv.org/abs/2410.17498&lt;br /&gt;This paper presents a time-aware approach for the early detection of anorexia within the eRisk 2024 framework, using a CPI+DMC methodology and integrating temporal metrics to improve both precision and speed in detecting early risk signs on the web.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile Device Control</title><link>https://arxiv.org/abs/2410.17520</link><description>https://arxiv.org/abs/2410.17520&lt;br /&gt;This research paper presents a time-aware approach for the early detection of anorexia, focusing on precision and speed in the detection process, where temporal metrics were integrated into the learning phase to optimize model performance on relevant tasks in the eRisk 2024 competition.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Differentially Private Learning Needs Better Model Initialization and Self-Distillation</title><link>https://arxiv.org/abs/2410.17566</link><description>https://arxiv.org/abs/2410.17566&lt;br /&gt;This paper presents a time-aware approach for the early detection of anorexia, focusing on precision and speed by integrating temporal aspects into the learning process, leading to high performance on relevant evaluation metrics.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Markov Chain of Thought for Efficient Mathematical Reasoning</title><link>https://arxiv.org/abs/2410.17635</link><description>https://arxiv.org/abs/2410.17635&lt;br /&gt;This paper discusses a time-aware approach for the early detection of anorexia within an online risk detection framework, highlighting a novel CPI+DMC strategy that integrates temporal metrics in the learning process to enhance both precision and speed in identifying signs of the disorder.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>A Time-Aware Approach to Early Detection of Anorexia: UNSL at eRisk 2024</title><link>https://arxiv.org/abs/2410.17963</link><description>https://arxiv.org/abs/2410.17963&lt;br /&gt;This paper presents a time-aware approach for the early detection of anorexia, focusing on optimizing both precision and speed in the detection process through a CPI+DMC method and the integration of temporal metrics during model training.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>RaTEScore: A Metric for Radiology Report Generation</title><link>https://arxiv.org/abs/2406.16845</link><description>https://arxiv.org/abs/2406.16845&lt;br /&gt;This paper explores the security challenges associated with the proliferation of Generative AI across various industries and proposes potential research directions for mitigating these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual Argument Understanding</title><link>https://arxiv.org/abs/2406.18925</link><description>https://arxiv.org/abs/2406.18925&lt;br /&gt;This paper explores the security challenges associated with the growing use of Generative AI across various industries and suggests potential research directions to address these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Attribute or Abstain: Large Language Models as Long Document Assistants</title><link>https://arxiv.org/abs/2407.07799</link><description>https://arxiv.org/abs/2407.07799&lt;br /&gt;This paper discusses the security challenges introduced by generative AI technologies across various industries and proposes potential research directions for addressing these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>S2-Attention: Hardware-Aware Context Sharding Among Attention Heads</title><link>https://arxiv.org/abs/2407.17678</link><description>https://arxiv.org/abs/2407.17678&lt;br /&gt;This paper discusses the security challenges associated with the growing adoption of Generative AI across various industries and suggests potential research directions for mitigating these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>NVLM: Open Frontier-Class Multimodal LLMs</title><link>https://arxiv.org/abs/2409.11402</link><description>https://arxiv.org/abs/2409.11402&lt;br /&gt;This paper examines the unique security challenges associated with Generative AI's proliferation across various industries and proposes research directions to address these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MADial-Bench: Towards Real-world Evaluation of Memory-Augmented Dialogue Generation</title><link>https://arxiv.org/abs/2409.15240</link><description>https://arxiv.org/abs/2409.15240&lt;br /&gt;This paper analyzes the security challenges introduced by Generative AI across various industries and proposes potential research directions for addressing these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>A Comparative Study on Reasoning Patterns of OpenAI's o1 Model</title><link>https://arxiv.org/abs/2410.13639</link><description>https://arxiv.org/abs/2410.13639&lt;br /&gt;This paper addresses the security challenges associated with the proliferation of Generative AI technologies across various industries and proposes potential research directions for mitigating these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs</title><link>https://arxiv.org/abs/2410.16144</link><description>https://arxiv.org/abs/2410.16144&lt;br /&gt;This paper explores the security challenges associated with the growing use of Generative AI across various industries and suggests potential research directions to address these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Linear Adversarial Concept Erasure</title><link>https://arxiv.org/abs/2201.12091</link><description>https://arxiv.org/abs/2201.12091&lt;br /&gt;This paper discusses the security challenges posed by Generative AI as it becomes more prevalent across various industries and proposes potential research directions to manage these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Captions Speak Louder than Images (CASLIE): Generalizing Foundation Models for E-commerce from High-quality Multimodal Instruction Data</title><link>https://arxiv.org/abs/2410.17337</link><description>https://arxiv.org/abs/2410.17337&lt;br /&gt;The paper introduces the Language Model Linguistic Personality Assessment (LMLPA), a system to quantitatively assess the linguistic personalities of Large Language Models (LLMs) using a modified Big Five Inventory, highlighting distinct personality traits in LLMs for Human-Computer Interaction and Human-Centered AI applications.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>AMUSD: Asynchronous Multi-Device Speculative Decoding for LLM Acceleration</title><link>https://arxiv.org/abs/2410.17375</link><description>https://arxiv.org/abs/2410.17375&lt;br /&gt;The paper introduces the Language Model Linguistic Personality Assessment (LMLPA), a system for evaluating the linguistic personalities of large language models (LLMs) using an adapted Big Five Inventory to provide quantitative assessments of personality traits in LLMs' outputs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities</title><link>https://arxiv.org/abs/2410.17385</link><description>https://arxiv.org/abs/2410.17385&lt;br /&gt;The paper introduces the Language Model Linguistic Personality Assessment (LMLPA), a system for evaluating the personality traits of Large Language Models using a modified version of the Big Five Inventory, contributing to Human-Computer Interaction research by providing quantifiable measures of AI personality traits.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Scalable Influence and Fact Tracing for Large Language Model Pretraining</title><link>https://arxiv.org/abs/2410.17413</link><description>https://arxiv.org/abs/2410.17413&lt;br /&gt;The paper introduces the Language Model Linguistic Personality Assessment (LMLPA) to evaluate the personality traits of large language models (LLMs) through linguistic outputs, applying an adapted Big Five Inventory framework and demonstrating their distinct personality profiles.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Evaluating AI-Generated Essays with GRE Analytical Writing Assessment</title><link>https://arxiv.org/abs/2410.17439</link><description>https://arxiv.org/abs/2410.17439&lt;br /&gt;The paper introduces the Language Model Linguistic Personality Assessment (LMLPA), a system for evaluating personality traits of large language models based on their linguistic outputs, using a modified version of the Big Five Inventory to provide insights into their conversational capabilities.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>In Context Learning and Reasoning for Symbolic Regression with Large Language Models</title><link>https://arxiv.org/abs/2410.17448</link><description>https://arxiv.org/abs/2410.17448&lt;br /&gt;The paper introduces the Language Model Linguistic Personality Assessment (LMLPA), a system designed to assess the personality traits of Large Language Models (LLMs) by adapting human-centric psychometric methods to evaluate their linguistic outputs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination</title><link>https://arxiv.org/abs/2410.17477</link><description>https://arxiv.org/abs/2410.17477&lt;br /&gt;The paper introduces the Language Model Linguistic Personality Assessment (LMLPA), a system to evaluate and quantify the distinct personality traits of Large Language Models (LLMs) using adapted psychometric techniques.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Is artificial intelligence still intelligence? LLMs generalize to novel adjective-noun pairs, but don't mimic the full human distribution</title><link>https://arxiv.org/abs/2410.17482</link><description>https://arxiv.org/abs/2410.17482&lt;br /&gt;The LMLPA system evaluates and quantifies the linguistic personalities of Large Language Models using a modified Big Five Inventory, contributing to the understanding of LLMs in conversational interactions.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>VoiceTextBlender: Augmenting Large Language Models with Speech Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning</title><link>https://arxiv.org/abs/2410.17485</link><description>https://arxiv.org/abs/2410.17485&lt;br /&gt;The paper introduces the Language Model Linguistic Personality Assessment (LMLPA), a system for evaluating the linguistic personalities of large language models (LLMs) using a questionnaire based on the Big Five Inventory, adapted for LLMs to assess their language generation capabilities in conversational settings.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Large Language Models Still Exhibit Bias in Long Text</title><link>https://arxiv.org/abs/2410.17519</link><description>https://arxiv.org/abs/2410.17519&lt;br /&gt;The paper introduces the Language Model Linguistic Personality Assessment (LMLPA), a system designed to evaluate and quantify the linguistic personality traits of large language models using the Big Five Inventory for better understanding of their language generation capabilities.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact</title><link>https://arxiv.org/abs/2410.17532</link><description>https://arxiv.org/abs/2410.17532&lt;br /&gt;The paper presents the Language Model Linguistic Personality Assessment (LMLPA), a system for evaluating the distinct personality traits of LLMs through adapted Big Five Inventory-based psychometrics, contributing insights into LLMs' language generation capabilities and their applications in Human-Computer Interaction and AI.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>ProtoLens: Advancing Prototype Learning for Fine-Grained Interpretability in Text Classification</title><link>https://arxiv.org/abs/2410.17546</link><description>https://arxiv.org/abs/2410.17546&lt;br /&gt;The paper introduces the Language Model Linguistic Personality Assessment (LMLPA) to evaluate and quantify distinct personality traits of Large Language Models (LLMs) through a modified Big Five Inventory, contributing to improvements in Human-Computer Interaction and Human-Centered AI.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>ESpeW: Robust Copyright Protection for LLM-based EaaS via Embedding-Specific Watermark</title><link>https://arxiv.org/abs/2410.17552</link><description>https://arxiv.org/abs/2410.17552&lt;br /&gt;LMLPA introduces a system to evaluate the linguistic personalities of LLMs using an adapted Big Five Inventory questionnaire to quantitatively assess personality traits in AI-generated text, contributing to Human-Computer Interaction and Human-Centered AI.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Graphusion: A RAG Framework for Knowledge Graph Construction with a Global Perspective</title><link>https://arxiv.org/abs/2410.17600</link><description>https://arxiv.org/abs/2410.17600&lt;br /&gt;The paper introduces the Language Model Linguistic Personality Assessment (LMLPA) to evaluate the linguistic personalities of large language models (LLMs), adapting the Big Five Inventory for measuring personality traits in LLMs to improve understanding of their language generation capabilities.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>LMLPA: Language Model Linguistic Personality Assessment</title><link>https://arxiv.org/abs/2410.17632</link><description>https://arxiv.org/abs/2410.17632&lt;br /&gt;This paper introduces the Language Model Linguistic Personality Assessment (LMLPA), a framework for evaluating the linguistic personalities of Large Language Models (LLMs) by adapting a personality assessment questionnaire to measure distinct personality traits in their linguistic outputs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents</title><link>https://arxiv.org/abs/2410.17657</link><description>https://arxiv.org/abs/2410.17657&lt;br /&gt;The paper analyzes changes in the German political discourse from 1871 to present using a time-series variant of LDA to identify significant events and shifts in political topics, leveraging digitized plenary session logs of the German Bundestag.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Quantifying the Risks of Tool-assisted Rephrasing to Linguistic Diversity</title><link>https://arxiv.org/abs/2410.17670</link><description>https://arxiv.org/abs/2410.17670&lt;br /&gt;The paper utilizes a time series variant of LDA to analyze digitized plenary sessions of the German Bundestag, identifying changes and lasting impacts on political discourse from 1949 to the present.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Towards a Similarity-adjusted Surprisal Theory</title><link>https://arxiv.org/abs/2410.17676</link><description>https://arxiv.org/abs/2410.17676&lt;br /&gt;The paper uses a time series variant of the topic model LDA to analyze changes in the German political discourse from the formation of the Federal Republic of Germany in 1949 onwards, identifying key events that have impacted political topics over the years.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>An Adaptive Framework for Generating Systematic Explanatory Answer in Online Q&amp;A Platforms</title><link>https://arxiv.org/abs/2410.17694</link><description>https://arxiv.org/abs/2410.17694&lt;br /&gt;The paper analyzes texts from the German Bundestag using a time series variant of the topic model LDA to identify significant changes in political discourse over time.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Beware of Calibration Data for Pruning Large Language Models</title><link>https://arxiv.org/abs/2410.17711</link><description>https://arxiv.org/abs/2410.17711&lt;br /&gt;The paper investigates changes in the German political discourse over time by analyzing texts from German Bundestag sessions using a time series variant of topic modeling LDA to detect shifts in political topics and key discussion points.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>CogSteer: Cognition-Inspired Selective Layer Intervention for Efficient Semantic Steering in Large Language Models</title><link>https://arxiv.org/abs/2410.17714</link><description>https://arxiv.org/abs/2410.17714&lt;br /&gt;The paper examines changes in German political discourse over time using a time series model of LDA to analyze digitized texts from the Bundestag, focusing on how events have influenced political topics and word frequency.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Dialectal and Low Resource Machine Translation for Aromanian</title><link>https://arxiv.org/abs/2410.17728</link><description>https://arxiv.org/abs/2410.17728&lt;br /&gt;The paper investigates changes in political discourse within the German Bundestag from 1871 onwards by using a time series variant of the LDA topic model to analyze the digitized transcripts of plenary sessions.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Local Contrastive Editing of Gender Stereotypes</title><link>https://arxiv.org/abs/2410.17739</link><description>https://arxiv.org/abs/2410.17739&lt;br /&gt;This paper uses a time series variant of the topic model LDA to analyze changes in German political discourse by examining digitized plenary session texts from the German Bundestag over time.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Latent Structures of Intertextuality in French Fiction</title><link>https://arxiv.org/abs/2410.17759</link><description>https://arxiv.org/abs/2410.17759&lt;br /&gt;The paper utilizes a time series variant of the topic model LDA to analyze the digitized plenary session logs of the German Bundestag, examining how political discourse and topics have evolved over time in response to significant events.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Understanding When Tree of Thoughts Succeeds: Larger Models Excel in Generation, Not Discrimination</title><link>https://arxiv.org/abs/2410.17820</link><description>https://arxiv.org/abs/2410.17820&lt;br /&gt;This paper investigates changes in the German political discourse over time using a time series variant of the topic model LDA on digitized texts from the German Bundestag sessions.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Understanding Layer Significance in LLM Alignment</title><link>https://arxiv.org/abs/2410.17875</link><description>https://arxiv.org/abs/2410.17875&lt;br /&gt;The paper analyzes the evolution of political discourse in the German Bundestag from 1949 onwards using a time series variant of the topic model LDA to detect changes in word frequency and key discussion points over time.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>SpeakGer: A meta-data enriched speech corpus of German state and federal parliaments</title><link>https://arxiv.org/abs/2410.17886</link><description>https://arxiv.org/abs/2410.17886&lt;br /&gt;The paper explores changes in the German political discourse over time by applying a time series variant of the topic model LDA to Bundestag session logs, identifying key events and shifts in political topics.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Scaling Diffusion Language Models via Adaptation from Autoregressive Models</title><link>https://arxiv.org/abs/2410.17891</link><description>https://arxiv.org/abs/2410.17891&lt;br /&gt;The paper analyzes changes in the German political discourse over time using a time series variant of the topic model LDA, examining how significant events influenced political topics in the Bundestag's digitized plenary sessions.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Value Residual Learning For Alleviating Attention Concentration In Transformers</title><link>https://arxiv.org/abs/2410.17897</link><description>https://arxiv.org/abs/2410.17897&lt;br /&gt;This paper analyzes the German political discourse from 1949 onwards using a time series variant of the LDA topic model to detect changes in word frequency and key discussion points over time.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Zeitenwenden: Detecting changes in the German political discourse</title><link>https://arxiv.org/abs/2410.17960</link><description>https://arxiv.org/abs/2410.17960&lt;br /&gt;The paper analyzes changes in the German political discourse over time using a time series variant of the topic model LDA to detect shifts in word frequency and key discussion points in the Bundestag plenary session texts.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Dependency Graph Parsing as Sequence Labeling</title><link>https://arxiv.org/abs/2410.17972</link><description>https://arxiv.org/abs/2410.17972&lt;br /&gt;This research paper presents a time-aware approach for the early detection of anorexia by integrating time into the learning process, demonstrating effective performance in precision and speed using temporal metrics.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning</title><link>https://arxiv.org/abs/2410.18035</link><description>https://arxiv.org/abs/2410.18035&lt;br /&gt;This paper presents a time-aware approach for early detection of anorexia by integrating temporal metrics during training, achieving notable results in precision and speed for early risk detection tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Literature Meets Data: A Synergistic Approach to Hypothesis Generation</title><link>https://arxiv.org/abs/2410.17309</link><description>https://arxiv.org/abs/2410.17309&lt;br /&gt;The paper discusses a time-aware approach to early detection of anorexia, focusing on the balance between precision and speed by integrating temporal metrics during training to optimize models for early risk detection on the web.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents</title><link>https://arxiv.org/abs/2410.17401</link><description>https://arxiv.org/abs/2410.17401&lt;br /&gt;The paper presents a time-aware approach for early detection of anorexia that integrates time in the learning process to optimize precision and speed, achieving significant results on the ERDE50 metric.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers</title><link>https://arxiv.org/abs/2410.17492</link><description>https://arxiv.org/abs/2410.17492&lt;br /&gt;The paper presents a time-aware approach for the early detection of anorexia focused on enhancing precision and speed, using explicit time integration during the learning process to excel in early risk detection (ERD) metrics.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>ExpertFlow: Optimized Expert Activation and Token Allocation for Efficient Mixture-of-Experts Inference</title><link>https://arxiv.org/abs/2410.17954</link><description>https://arxiv.org/abs/2410.17954&lt;br /&gt;The paper presents a time-aware method for the early detection of anorexia signs by integrating time explicitly during the learning process to optimize precision and speed using the ERDE metrics as objectives, achieving strong results in the eRisk 2024 challenge.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>A Bi-consolidating Model for Joint Relational Triple Extraction</title><link>https://arxiv.org/abs/2404.03881</link><description>https://arxiv.org/abs/2404.03881&lt;br /&gt;The paper introduces a unified framework called Fast and Slow Generating (FS-GEN) to collaboratively decode between large and small language models, addressing challenges like latency and hallucinations by analyzing the effective conditions for their cooperation based on parameter scaling laws and uncertainty management.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Annotator-Centric Active Learning for Subjective NLP Tasks</title><link>https://arxiv.org/abs/2404.15720</link><description>https://arxiv.org/abs/2404.15720&lt;br /&gt;The paper explores a collaborative decoding framework, Fast and Slow Generating (FS-GEN), leveraging large and small language models to mitigate challenges like high inference latency and generation of hallucinations, focusing on the distinct roles of fast, intuitive systems and slow, deliberate ones.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs</title><link>https://arxiv.org/abs/2406.14282</link><description>https://arxiv.org/abs/2406.14282&lt;br /&gt;The paper explores the unique security challenges associated with Generative AI and suggests potential research directions for managing these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics</title><link>https://arxiv.org/abs/2406.14703</link><description>https://arxiv.org/abs/2406.14703&lt;br /&gt;This paper explores the security challenges associated with Generative AI and suggests potential research directions for mitigating these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>AlleNoise: large-scale text classification benchmark dataset with real-world label noise</title><link>https://arxiv.org/abs/2407.10992</link><description>https://arxiv.org/abs/2407.10992&lt;br /&gt;This paper explores the security challenges posed by Generative AI and outlines potential research directions to manage these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>NutriBench: A Dataset for Evaluating Large Language Models in Carbohydrate Estimation from Meal Descriptions</title><link>https://arxiv.org/abs/2407.12843</link><description>https://arxiv.org/abs/2407.12843&lt;br /&gt;The paper explores the security challenges presented by Generative AI as it becomes more prevalent across various industries, and suggests research directions for mitigating these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Task Prompt Vectors: Effective Initialization through Multi-Task Soft-Prompt Transfer</title><link>https://arxiv.org/abs/2408.01119</link><description>https://arxiv.org/abs/2408.01119&lt;br /&gt;The paper discusses the security challenges posed by Generative AI and suggests potential research directions to manage these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Can Language Models Induce Grammatical Knowledge from Indirect Evidence?</title><link>https://arxiv.org/abs/2410.06022</link><description>https://arxiv.org/abs/2410.06022&lt;br /&gt;The paper discusses the unique security challenges posed by Generative AI and suggests potential research directions to manage these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems</title><link>https://arxiv.org/abs/2410.13334</link><description>https://arxiv.org/abs/2410.13334&lt;br /&gt;The paper discusses the unique security challenges of Generative AI and proposes research directions to manage these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>On the Diversity of Synthetic Data and its Impact on Training Large Language Models</title><link>https://arxiv.org/abs/2410.15226</link><description>https://arxiv.org/abs/2410.15226&lt;br /&gt;The paper explores the security challenges associated with Generative AI across various industries and suggests research directions for risk management.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Do Large Language Models Have an English Accent? Evaluating and Improving the Naturalness of Multilingual LLMs</title><link>https://arxiv.org/abs/2410.15956</link><description>https://arxiv.org/abs/2410.15956&lt;br /&gt;The paper explores the security challenges posed by Generative AI across various industries and suggests research directions to manage these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S</title><link>https://arxiv.org/abs/2410.16451</link><description>https://arxiv.org/abs/2410.16451&lt;br /&gt;The paper explores the unique security challenges posed by Generative AI and outlines potential research directions for managing these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Generative AI Security: Challenges and Countermeasures</title><link>https://arxiv.org/abs/2402.12617</link><description>https://arxiv.org/abs/2402.12617&lt;br /&gt;The paper explores the security challenges associated with Generative AI and suggests potential research directions for managing these risks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>LLMScan: Causal Scan for LLM Misbehavior Detection</title><link>https://arxiv.org/abs/2410.16638</link><description>https://arxiv.org/abs/2410.16638&lt;br /&gt;LLMScan is a technique utilizing causality analysis to monitor and detect misbehavior in Large Language Models (LLMs) by analyzing causal contributions of input tokens and transformer layers.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Non-myopic Generation of Language Model for Reasoning and Planning</title><link>https://arxiv.org/abs/2410.17195</link><description>https://arxiv.org/abs/2410.17195&lt;br /&gt;LLMScan introduces a causal inference-based technique to monitor and detect misbehavior in Large Language Models (LLMs) by analyzing the causal contributions of input tokens and transformer layers.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>R2Gen-Mamba: A Selective State Space Model for Radiology Report Generation</title><link>https://arxiv.org/abs/2410.18135</link><description>https://arxiv.org/abs/2410.18135&lt;br /&gt;DoG is a novel framework that combines LLMs and Knowledge Graphs to generate well-formed chains for reliable and robust question answering through a process called graph-aware constrained decoding.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Analyzing Nobel Prize Literature with Large Language Models</title><link>https://arxiv.org/abs/2410.18142</link><description>https://arxiv.org/abs/2410.18142&lt;br /&gt;DoG (Decoding on Graphs) is a framework that enhances question answering with large language models by using knowledge graphs to guide the generation of well-formed reasoning chains via graph-aware constrained decoding.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Meaning Typed Prompting: A Technique for Efficient, Reliable Structured Output Generation</title><link>https://arxiv.org/abs/2410.18146</link><description>https://arxiv.org/abs/2410.18146&lt;br /&gt;The paper presents DoG, a framework that combines large language models and knowledge graphs for improved question answering, by generating well-formed chains of fact triplets to ensure faithful and sound reasoning.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Future Token Prediction -- Causal Language Modelling with Per-Token Semantic State Vector for Multi-Token Prediction</title><link>https://arxiv.org/abs/2410.18160</link><description>https://arxiv.org/abs/2410.18160&lt;br /&gt;DoG is a framework that combines large language models with knowledge graphs through constrained decoding to generate well-formed reasoning chains for enhanced question answering.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Gazelle: An Instruction Dataset for Arabic Writing Assistance</title><link>https://arxiv.org/abs/2410.18163</link><description>https://arxiv.org/abs/2410.18163&lt;br /&gt;The paper introduces DoG, a framework that enhances question answering by integrating large language models with knowledge graphs to generate well-formed reasoning chains using constrained decoding.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>CorrectionLM: Self-Corrections with SLM for Dialogue State Tracking</title><link>https://arxiv.org/abs/2410.18209</link><description>https://arxiv.org/abs/2410.18209&lt;br /&gt;DoG is a framework that enhances question answering on knowledge graphs by utilizing large language models to generate well-formed reasoning chains through graph-aware constrained decoding, improving the synergy between LLMs' reasoning capabilities and the structured nature of knowledge graphs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Towards Understanding the Fragility of Multilingual LLMs against Fine-Tuning Attacks</title><link>https://arxiv.org/abs/2410.18210</link><description>https://arxiv.org/abs/2410.18210&lt;br /&gt;Decoding on Graphs (DoG) is a framework that enhances the reasoning abilities of large language models by generating well-formed chains from knowledge graphs, facilitating reliable question answering through graph-aware constrained decoding.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Generalizations across filler-gap dependencies in neural language models</title><link>https://arxiv.org/abs/2410.18225</link><description>https://arxiv.org/abs/2410.18225&lt;br /&gt;DoG (Decoding on Graphs) introduces a method for improving question answering by combining large language models with knowledge graphs through a graph-aware constrained decoding process that generates well-formed reasoning chains.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits</title><link>https://arxiv.org/abs/2410.18234</link><description>https://arxiv.org/abs/2410.18234&lt;br /&gt;The paper presents DoG (Decoding on Graphs), a novel framework that integrates large language models with knowledge graphs to generate well-formed chains of reasoning, enhancing the accuracy and reliability of knowledge graph-based question answering.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Multilingual Hallucination Gaps in Large Language Models</title><link>https://arxiv.org/abs/2410.18270</link><description>https://arxiv.org/abs/2410.18270&lt;br /&gt;DoG (Decoding on Graphs) is a framework that enhances the synergy between large language models (LLMs) and knowledge graphs (KGs) by employing graph-aware constrained decoding to generate well-formed chains for faithful and sound reasoning in KG-based question answering.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>LEGO: Language Model Building Blocks</title><link>https://arxiv.org/abs/2410.18287</link><description>https://arxiv.org/abs/2410.18287&lt;br /&gt;DoG is a novel framework that uses constrained decoding on knowledge graphs to enhance large language models' reasoning capabilities, generating well-formed chains for more reliable question-answering systems.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Measuring individual semantic networks: A simulation study</title><link>https://arxiv.org/abs/2410.18326</link><description>https://arxiv.org/abs/2410.18326&lt;br /&gt;The paper introduces DoG (Decoding on Graphs), a framework that enhances large language models' reasoning on knowledge graphs by generating well-formed chains of fact triplets to ensure faithful and sound question answering.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Assessing the Creativity of LLMs in Proposing Novel Solutions to Mathematical Problems</title><link>https://arxiv.org/abs/2410.18336</link><description>https://arxiv.org/abs/2410.18336&lt;br /&gt;The paper introduces DoG, a novel framework that enhances the integration of large language models (LLMs) with knowledge graphs (KGs) to generate accurate and structured answers through graph-aware constrained decoding.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Aggregated Knowledge Model: Enhancing Domain-Specific QA with Fine-Tuned and Retrieval-Augmented Generation Models</title><link>https://arxiv.org/abs/2410.18344</link><description>https://arxiv.org/abs/2410.18344&lt;br /&gt;The paper presents DoG (Decoding on Graphs), a novel framework that integrates large language models (LLMs) with Knowledge Graphs (KGs) to facilitate structured, faithful, and sound reasoning by generating well-formed chains within KGs for question answering tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>AdaEDL: Early Draft Stopping for Speculative Decoding of Large Language Models via an Entropy-based Lower Bound on Token Acceptance Probability</title><link>https://arxiv.org/abs/2410.18351</link><description>https://arxiv.org/abs/2410.18351&lt;br /&gt;DoG is a novel framework that integrates large language models with knowledge graphs to generate well-formed reasoning chains for question answering tasks, improving reasoning by using graph-aware constrained decoding.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Improving Model Factuality with Fine-grained Critique-based Evaluator</title><link>https://arxiv.org/abs/2410.18359</link><description>https://arxiv.org/abs/2410.18359&lt;br /&gt;DoG (Decoding on Graphs) utilizes graph-aware constrained decoding to enable large language models (LLMs) to generate faithful and sound reasoning paths on Knowledge Graphs (KGs) for question answering.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Monolingual and Multilingual Misinformation Detection for Low-Resource Languages: A Comprehensive Survey</title><link>https://arxiv.org/abs/2410.18390</link><description>https://arxiv.org/abs/2410.18390&lt;br /&gt;The paper introduces DoG, a framework that integrates large language models with knowledge graphs through graph-aware constrained decoding to generate well-formed chains for faithful and sound reasoning in knowledge graph-based question answering.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness</title><link>https://arxiv.org/abs/2410.18393</link><description>https://arxiv.org/abs/2410.18393&lt;br /&gt;The paper introduces DoG (Decoding on Graphs), a framework that synergizes the reasoning abilities of large language models with the structured knowledge of knowledge graphs to generate faithful reasoning chains for question answering.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MoMQ: Mixture-of-Experts Enhances Multi-Dialect Query Generation across Relational and Non-Relational Databases</title><link>https://arxiv.org/abs/2410.18406</link><description>https://arxiv.org/abs/2410.18406&lt;br /&gt;DoG (Decoding on Graphs) is a novel framework that enhances knowledge graph question answering by integrating large language models' step-wise reasoning with graph-aware constrained decoding to generate well-formed reasoning chains.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Decoding on Graphs: Faithful and Sound Reasoning on Knowledge Graphs through Generation of Well-Formed Chains</title><link>https://arxiv.org/abs/2410.18415</link><description>https://arxiv.org/abs/2410.18415&lt;br /&gt;DoG is a framework that enhances question answering by combining large language models with knowledge graphs through graph-aware constrained decoding to generate well-formed reasoning chains.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Building Dialogue Understanding Models for Low-resource Language Indonesian from Scratch</title><link>https://arxiv.org/abs/2410.18430</link><description>https://arxiv.org/abs/2410.18430&lt;br /&gt;The paper introduces SPEED, a framework that aligns small open-source models to efficiently generate large-scale, high-quality synthetic embedding data, reducing reliance on costly proprietary models like GPT-4.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent Dialogue Synthesis</title><link>https://arxiv.org/abs/2410.18447</link><description>https://arxiv.org/abs/2410.18447&lt;br /&gt;The paper presents SPEED, a framework for aligning open-source models for efficient generation of large-scale synthetic embedding data, surpassing existing methods while significantly reducing reliance on expensive proprietary models.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>LOGO -- Long cOntext aliGnment via efficient preference Optimization</title><link>https://arxiv.org/abs/2410.18533</link><description>https://arxiv.org/abs/2410.18533&lt;br /&gt;The paper introduces SPEED, a framework for synthesizing high-quality text embedding data using small open-source models, significantly reducing reliance on costly proprietary models like GPT-4 while maintaining performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>STTATTS: Unified Speech-To-Text And Text-To-Speech Model</title><link>https://arxiv.org/abs/2410.18607</link><description>https://arxiv.org/abs/2410.18607&lt;br /&gt;This paper introduces SPEED, a framework that efficiently generates large-scale high-quality synthetic embedding data using small open-source models, significantly reducing reliance on expensive GPT models and enhancing data quality through fine-tuning and preference optimization.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>BioMistral-NLU: Towards More Generalizable Medical Language Understanding through Instruction Tuning</title><link>https://arxiv.org/abs/2410.18955</link><description>https://arxiv.org/abs/2410.18955&lt;br /&gt;CoreInfer is an inference method that accelerates large language model processing using sentence-level adaptive sparse activation to identify critical neurons, leading to significant performance improvements without additional computational costs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs</title><link>https://arxiv.org/abs/2410.18451</link><description>https://arxiv.org/abs/2410.18451&lt;br /&gt;XC-Cache enhances efficiency in large language model inference by using cross-attention to reference cached context, reducing space requirements significantly while maintaining performance in conditional generation tasks like Question-Answering.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing</title><link>https://arxiv.org/abs/2410.18517</link><description>https://arxiv.org/abs/2410.18517&lt;br /&gt;The paper introduces XC-Cache, a model that uses cross-attention to efficiently perform conditional generation for in-context learning with significantly reduced caching space requirements, demonstrated on Question-Answering tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Speech perception: a model of word recognition</title><link>https://arxiv.org/abs/2410.18590</link><description>https://arxiv.org/abs/2410.18590&lt;br /&gt;The paper introduces XC-Cache, a model architecture that uses cross-attention for efficient in-context learning in large language models during inference, outperforming traditional methods and significantly reducing memory requirements.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation</title><link>https://arxiv.org/abs/2410.18652</link><description>https://arxiv.org/abs/2410.18652&lt;br /&gt;The paper introduces LABE, a benchmark to evaluate social biases in language agency in content generated by large language models (LLMs), revealing significant gender, racial, and intersectional biases, particularly against demographic groups such as Black females.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Health Misinformation in Social Networks: A Survey of IT Approaches</title><link>https://arxiv.org/abs/2410.18670</link><description>https://arxiv.org/abs/2410.18670&lt;br /&gt;XC-Cache introduces a cross-attention framework for efficient large language model inference, improving conditional generation and reducing space requirements by avoiding traditional context caching.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs</title><link>https://arxiv.org/abs/2410.18779</link><description>https://arxiv.org/abs/2410.18779&lt;br /&gt;The paper introduces XC-Cache, a cross-attention mechanism for efficient LLM inference that reduces space requirements by using encoder-decoder inspired architecture to condition generation on reference text, outperforming traditional in-context learning in question-answering tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>An LLM Agent for Automatic Geospatial Data Analysis</title><link>https://arxiv.org/abs/2410.18792</link><description>https://arxiv.org/abs/2410.18792&lt;br /&gt;XC-Cache introduces an efficient inference method for large language models in question-answering tasks by using cross-attention mechanisms to condition generation on reference context, drastically reducing memory requirements compared to traditional in-context learning approaches.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>A Combinatorial Approach to Neural Emergent Communication</title><link>https://arxiv.org/abs/2410.18806</link><description>https://arxiv.org/abs/2410.18806&lt;br /&gt;XC-Cache introduces a method using cross-attention to efficiently condition language model generation on reference text, drastically reducing the space needed compared to standard key-value caching, and demonstrating effectiveness in question-answering tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Demystifying Large Language Models for Medicine: A Primer</title><link>https://arxiv.org/abs/2410.18856</link><description>https://arxiv.org/abs/2410.18856&lt;br /&gt;The paper introduces XC-Cache, a model that uses cross-attention instead of prompts to condition large language model generation on reference text, thereby improving efficiency and reducing cache space requirements, and demonstrates its effectiveness in question-answering tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Provably Robust Watermarks for Open-Source Language Models</title><link>https://arxiv.org/abs/2410.18861</link><description>https://arxiv.org/abs/2410.18861&lt;br /&gt;XC-Cache introduces a cross-attention based approach to efficiently condition large language model inference on reference text without prompts, reducing cache requirements and improving performance on question-answering tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Schema-Guided Culture-Aware Complex Event Simulation with Multi-Agent Role-Play</title><link>https://arxiv.org/abs/2410.18935</link><description>https://arxiv.org/abs/2410.18935&lt;br /&gt;The paper introduces XC-Cache, a model that utilizes cross-attention for conditional generation in LLMs to efficiently perform question-answering tasks, significantly reducing memory usage compared to conventional caching methods.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>OSCAR: Operating System Control via State-Aware Reasoning and Re-Planning</title><link>https://arxiv.org/abs/2410.18963</link><description>https://arxiv.org/abs/2410.18963&lt;br /&gt;XC-Cache introduces a model that leverages cross-attention for efficient language model inference by using cached context without relying on prompts, offering improved performance and reduced space requirements compared to traditional caching methods.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms</title><link>https://arxiv.org/abs/2410.18967</link><description>https://arxiv.org/abs/2410.18967&lt;br /&gt;The paper introduces XC-Cache, a model that uses cross-attention to condition generation on reference text, improving efficiency and storage in in-context learning without sacrificing performance compared to traditional methods.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Unbounded: A Generative Infinite Game of Character Life Simulation</title><link>https://arxiv.org/abs/2410.18975</link><description>https://arxiv.org/abs/2410.18975&lt;br /&gt;The paper introduces XC-Cache, a model using cross-attention to efficiently condition decoder-only LLM generation on reference contexts without prompting, reducing cache space requirements while maintaining performance in question-answering tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>CAMEL-Bench: A Comprehensive Arabic LMM Benchmark</title><link>https://arxiv.org/abs/2410.18976</link><description>https://arxiv.org/abs/2410.18976&lt;br /&gt;XC-Cache introduces an efficient in-context learning approach by using cross-attention to condition language model generation on reference text, significantly reducing memory footprint compared to traditional KV caching in self-attention operations.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>RET-LLM: Towards a General Read-Write Memory for Large Language Models</title><link>https://arxiv.org/abs/2305.14322</link><description>https://arxiv.org/abs/2305.14322&lt;br /&gt;XC-Cache introduces cross-attention mechanisms for efficient language model inference by conditioning generation on reference text without prompts, improving performance and reducing cache space requirements in question-answering tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Head-wise Shareable Attention for Large Language Models</title><link>https://arxiv.org/abs/2402.11819</link><description>https://arxiv.org/abs/2402.11819&lt;br /&gt;XC-Cache introduces a model that uses cross-attention for efficient large language model inference, reducing memory usage and enhancing conditional text generation without traditional prompts.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontologies-enhanced Large Language Models</title><link>https://arxiv.org/abs/2403.00953</link><description>https://arxiv.org/abs/2403.00953&lt;br /&gt;XC-Cache enhances efficient LLM inference by using a cross-attention mechanism to leverage cached contexts without prompts, significantly reducing memory space requirements and maintaining competitive performance in question-answering tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>White Men Lead, Black Women Help? Benchmarking Language Agency Social Biases in LLMs</title><link>https://arxiv.org/abs/2404.10508</link><description>https://arxiv.org/abs/2404.10508&lt;br /&gt;XC-Cache introduces a model that uses cross-attention to condition language model generation on reference text without using prompts, improving efficiency in Question-Answering tasks while significantly reducing memory requirements.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference</title><link>https://arxiv.org/abs/2404.15420</link><description>https://arxiv.org/abs/2404.15420&lt;br /&gt;XC-Cache introduces a cross-attention mechanism in transformer models to improve efficiency in language model inference by reducing the space footprint of in-context learning, and demonstrates its effectiveness in question-answering tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>AutoPSV: Automated Process-Supervised Verifier</title><link>https://arxiv.org/abs/2405.16802</link><description>https://arxiv.org/abs/2405.16802&lt;br /&gt;The paper presents a scaling law that describes the cross-entropy loss curves of neural language models during training, incorporating learning rate annealing, and demonstrates its application in predicting model loss across different learning rate schedulers, thereby improving training efficiency and understanding.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>On the Noise Robustness of In-Context Learning for Text Generation</title><link>https://arxiv.org/abs/2405.17264</link><description>https://arxiv.org/abs/2405.17264&lt;br /&gt;The paper presents ALT (ALignment with Textual feedback), a method to align language models with user preferences expressed in text, showing its effectiveness in various tasks like toxicity reduction and summarization by conditioning model generation on textual feedback.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Analyzing Human Questioning Behavior and Causal Curiosity through Natural Queries</title><link>https://arxiv.org/abs/2405.20318</link><description>https://arxiv.org/abs/2405.20318&lt;br /&gt;The paper introduces ALT (ALignment with Textual feedback), an approach that aligns language models with user preferences expressed through textual feedback, achieving improvements in tasks like toxicity reduction and summarization while using fewer samples.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis</title><link>https://arxiv.org/abs/2406.12665</link><description>https://arxiv.org/abs/2406.12665&lt;br /&gt;The paper presents ALT, a method aligning language models with user preferences through textual feedback, offering expressiveness and efficiency across tasks like toxicity reduction and summarization, while outperforming traditional approaches.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Cutting Through the Noise: Boosting LLM Performance on Math Word Problems</title><link>https://arxiv.org/abs/2406.15444</link><description>https://arxiv.org/abs/2406.15444&lt;br /&gt;The paper presents ALT, a method that aligns language models with user preferences through textual feedback, demonstrating superior performance and efficiency across various tasks such as toxicity reduction and summarization compared to traditional PPO approaches.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>"Vorbe\c{s}ti Rom\^ane\c{s}te?" A Recipe to Train Powerful Romanian LLMs with English Instructions</title><link>https://arxiv.org/abs/2406.18266</link><description>https://arxiv.org/abs/2406.18266&lt;br /&gt;The paper introduces ALT, an approach that aligns language models with user preferences expressed in text, demonstrating superior performance to PPO in tasks like toxicity reduction and exploring efficiency and effectiveness in various tasks through textual feedback.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval</title><link>https://arxiv.org/abs/2407.03585</link><description>https://arxiv.org/abs/2407.03585&lt;br /&gt;The paper introduces ALT, an approach for aligning language models with textual feedback to improve performance on tasks like toxicity reduction, summarization, and dialogue response, and argues that text provides richer feedback than simple preferences.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Self-training Language Models for Arithmetic Reasoning</title><link>https://arxiv.org/abs/2407.08400</link><description>https://arxiv.org/abs/2407.08400&lt;br /&gt;The paper presents a scaling law for neural language models that accounts for learning rate annealing over training steps, allowing for accurate prediction of validation loss across different learning rate schedulers, thereby reducing computational costs and improving understanding of training dynamics.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MoESD: Mixture of Experts Stable Diffusion to Mitigate Gender Bias</title><link>https://arxiv.org/abs/2407.11002</link><description>https://arxiv.org/abs/2407.11002&lt;br /&gt;This paper introduces a new approach called ALT (Alignment with Textual feedback) to align language models with user preferences expressed in text, showing improvements in tasks such as toxicity reduction and summarization by utilizing more expressive feedback mechanisms.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Localizing and Mitigating Errors in Long-form Question Answering</title><link>https://arxiv.org/abs/2407.11930</link><description>https://arxiv.org/abs/2407.11930&lt;br /&gt;The paper proposes ALT (Alignment with Textual feedback), a method to align language models with user preferences expressed in text, demonstrating that text feedback can enhance model alignment in various tasks such as toxicity reduction, summarization, and dialog response generation, outperforming traditional methods.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval</title><link>https://arxiv.org/abs/2407.12883</link><description>https://arxiv.org/abs/2407.12883&lt;br /&gt;This paper introduces ALT, a method that aligns language models with user preferences through rich textual feedback, demonstrating effectiveness in tasks like toxicity reduction, summarization, and dialog response generation compared to traditional methods.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Learning Goal-Conditioned Representations for Language Reward Models</title><link>https://arxiv.org/abs/2407.13887</link><description>https://arxiv.org/abs/2407.13887&lt;br /&gt;The paper proposes ALT (ALignment with Textual feedback), a method that enhances language model alignment using user-preference textual feedback, demonstrating efficiency and effectiveness across tasks like toxicity reduction, summarization, and dialogue response generation.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Towards Aligning Language Models with Textual Feedback</title><link>https://arxiv.org/abs/2407.16970</link><description>https://arxiv.org/abs/2407.16970&lt;br /&gt;This paper presents ALT (Alignment with Textual feedback), a method for aligning language models with user preferences using textual feedback, demonstrating improved performance in reducing toxicity and matching summarization capabilities while offering insights into natural language feedback alignment.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Scaling Law with Learning Rate Annealing</title><link>https://arxiv.org/abs/2408.11029</link><description>https://arxiv.org/abs/2408.11029&lt;br /&gt;The paper introduces ALT, an approach that leverages textual feedback to align language models with user preferences, enhancing efficiency and effectiveness in tasks such as toxicity reduction and dialog response generation.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Effects of Scale on Language Model Robustness</title><link>https://arxiv.org/abs/2407.18213</link><description>https://arxiv.org/abs/2407.18213&lt;br /&gt;This study explores the interactions between humanoid robots NAO and Pepper, demonstrating their potential in educational settings for autonomous communication, collaboration, and the integration of AI to enhance learning and social skills among students.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Uncovering Biases with Reflective Large Language Models</title><link>https://arxiv.org/abs/2408.13464</link><description>https://arxiv.org/abs/2408.13464&lt;br /&gt;This study explores the autonomous communication and collaboration between humanoid robots NAO and Pepper in educational settings, highlighting their potential to enhance learning environments and develop social and emotional skills in students.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets</title><link>https://arxiv.org/abs/2409.04286</link><description>https://arxiv.org/abs/2409.04286&lt;br /&gt;The study explores the interaction between humanoid robots NAO and Pepper in educational settings, demonstrating their potential for autonomous communication and collaboration to enhance learning environments.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>DENEB: A Hallucination-Robust Automatic Evaluation Metric for Image Captioning</title><link>https://arxiv.org/abs/2409.19255</link><description>https://arxiv.org/abs/2409.19255&lt;br /&gt;This study explores interactions between humanoid robots NAO and Pepper for autonomous communication and collaboration, highlighting potential applications in enhancing educational environments with AI integration.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in Neural Nets</title><link>https://arxiv.org/abs/2410.01779</link><description>https://arxiv.org/abs/2410.01779&lt;br /&gt;This study explores the interactions between humanoid robots NAO and Pepper in educational settings, highlighting their capabilities for autonomous communication and collaboration to enhance learning environments through artificial intelligence integration.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Learning Code Preference via Synthetic Evolution</title><link>https://arxiv.org/abs/2410.03837</link><description>https://arxiv.org/abs/2410.03837&lt;br /&gt;The study examines the autonomous communication and collaborative capabilities of humanoid robots NAO and Pepper in educational environments, highlighting their potential to enhance learning and social skill development through AI integration.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Chain of Ideas: Revolutionizing Research in Novel Idea Development with LLM Agents</title><link>https://arxiv.org/abs/2410.13185</link><description>https://arxiv.org/abs/2410.13185&lt;br /&gt;This study explores the interaction between humanoid robots NAO and Pepper, showcasing their capabilities for autonomous communication and collaboration in educational settings, and highlights their potential for enhancing learning environments with AI integration.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>ViConsFormer: Constituting Meaningful Phrases of Scene Texts using Transformer-based Method in Vietnamese Text-based Visual Question Answering</title><link>https://arxiv.org/abs/2410.14132</link><description>https://arxiv.org/abs/2410.14132&lt;br /&gt;The study explores the interaction between humanoid robots NAO and Pepper to develop autonomous communication and collaboration, showcasing their potential as educational tools for enhancing learning and developing social skills.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>RSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework</title><link>https://arxiv.org/abs/2410.19109</link><description>https://arxiv.org/abs/2410.19109&lt;br /&gt;This paper investigates the impact of pruning large language models on reducing hallucinations during abstractive summarization, finding that pruned models exhibit less hallucination by relying more on the source document.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant</title><link>https://arxiv.org/abs/2410.19144</link><description>https://arxiv.org/abs/2410.19144&lt;br /&gt;The paper investigates the impact of model pruning on hallucination occurrences in large language models used for abstractive summarization, finding that pruned models tend to hallucinate less and rely more on source documents.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Adversarial Attacks on Large Language Models Using Regularized Relaxation</title><link>https://arxiv.org/abs/2410.19160</link><description>https://arxiv.org/abs/2410.19160&lt;br /&gt;The paper investigates the effect of pruning on hallucinations in large language models for abstractive summarization and finds that pruned models exhibit fewer hallucinations, likely due to increased reliance on the source document.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark</title><link>https://arxiv.org/abs/2410.19168</link><description>https://arxiv.org/abs/2410.19168&lt;br /&gt;This paper explores the impact of pruning on large language models used for abstractive summarization, finding that pruned models exhibit fewer hallucinations by depending more on the source document.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Indication Finding: a novel use case for representation learning</title><link>https://arxiv.org/abs/2410.19174</link><description>https://arxiv.org/abs/2410.19174&lt;br /&gt;The paper investigates the impact of pruning on large language models (LLMs) used for abstractive summarization, finding that pruned models may reduce hallucinations by relying more on the source document.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Making Social Platforms Accessible: Emotion-Aware Speech Generation with Integrated Text Analysis</title><link>https://arxiv.org/abs/2410.19199</link><description>https://arxiv.org/abs/2410.19199&lt;br /&gt;This paper provides an empirical study on the effects of pruning large language models on hallucinations during abstractive summarization, revealing that pruned models exhibit fewer hallucinations by relying more on source documents.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Inference time LLM alignment in single and multidomain preference spectrum</title><link>https://arxiv.org/abs/2410.19206</link><description>https://arxiv.org/abs/2410.19206&lt;br /&gt;The paper investigates the effect of pruning on large language models for abstractive summarization, finding that pruned models exhibit fewer hallucinations due to increased reliance on the source document.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Mirror Matrix on the Wall: coding and vector notation as tools for introspection</title><link>https://arxiv.org/abs/2410.19549</link><description>https://arxiv.org/abs/2410.19549&lt;br /&gt;This paper investigates the impact of pruning on hallucinations in large language models for abstractive summarization, finding that pruned models tend to exhibit fewer hallucinations due to increased reliance on the source document.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>IPPON: Common Sense Guided Informative Path Planning for Object Goal Navigation</title><link>https://arxiv.org/abs/2410.19697</link><description>https://arxiv.org/abs/2410.19697&lt;br /&gt;This paper investigates the impact of pruning on hallucinations in large language models used for abstractive summarization, finding that pruned models impose less hallucinatory content due to increased reliance on the source document.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Towards End-to-End Open Conversational Machine Reading</title><link>https://arxiv.org/abs/2210.07113</link><description>https://arxiv.org/abs/2210.07113&lt;br /&gt;This paper explores the impact of pruning on large language models (LLMs) for abstractive summarization, finding that pruned models exhibit fewer hallucinations by relying more on source documents, thus producing summaries with higher lexical overlap.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Demonstration-based learning for few-shot biomedical named entity recognition under machine reading comprehension</title><link>https://arxiv.org/abs/2308.06454</link><description>https://arxiv.org/abs/2308.06454&lt;br /&gt;This paper investigates the impact of pruning on hallucinations in large language models used for abstractive summarization, finding that pruned models rely more on source documents, reducing hallucinations.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Hate speech detection in algerian dialect using deep learning</title><link>https://arxiv.org/abs/2309.11611</link><description>https://arxiv.org/abs/2309.11611&lt;br /&gt;This paper investigates the impact of pruning on hallucinations in large language models used for abstractive summarization, finding that pruned models produce fewer hallucinations by relying more on source documents.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Large Language Models Can Be Contextual Privacy Protection Learners</title><link>https://arxiv.org/abs/2310.02469</link><description>https://arxiv.org/abs/2310.02469&lt;br /&gt;The paper investigates how pruning reduces hallucinations in large language models (LLMs) used for abstractive summarization, finding that pruned models rely more on the source document and thus produce more accurate summaries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models</title><link>https://arxiv.org/abs/2406.01436</link><description>https://arxiv.org/abs/2406.01436&lt;br /&gt;This paper explores the impact of data augmentation techniques on improving confidence calibration and uncertainty estimation in Named Entity Recognition (NER) tasks, especially in safety-critical fields like healthcare and finance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Shortcomings of LLMs for Low-Resource Translation: Retrieval and Understanding are Both the Problem</title><link>https://arxiv.org/abs/2406.15625</link><description>https://arxiv.org/abs/2406.15625&lt;br /&gt;The paper examines the use of data augmentation methods to enhance confidence calibration and uncertainty estimation in Named Entity Recognition (NER) tasks, focusing on improving the applicability of DNNs in safety-critical fields like healthcare and finance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>TinyAgent: Function Calling at the Edge</title><link>https://arxiv.org/abs/2409.00608</link><description>https://arxiv.org/abs/2409.00608&lt;br /&gt;The paper proposes LoReKT, a low-resource knowledge tracing framework that employs supervised pre-training and importance mechanism fine-tuning to improve the transferability and performance of DLKT models in scenarios with limited student interaction data.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Bayesian scaling laws for in-context learning</title><link>https://arxiv.org/abs/2410.16531</link><description>https://arxiv.org/abs/2410.16531&lt;br /&gt;The paper introduces LoReKT, a framework employing pre-training and an importance mechanism to enhance the performance of deep learning-based knowledge tracing models in low-resource settings by leveraging data from rich-resource datasets.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning</title><link>https://arxiv.org/abs/2403.06725</link><description>https://arxiv.org/abs/2403.06725&lt;br /&gt;The paper introduces LoReKT, a framework for improving knowledge tracing tasks in low-resource settings by leveraging supervised pre-training and importance mechanism fine-tuning to enhance parameter adaptation and model performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Balancing Transparency and Accuracy: A Comparative Analysis of Rule-Based and Deep Learning Models in Political Bias Classification</title><link>https://arxiv.org/abs/2411.04328</link><description>https://arxiv.org/abs/2411.04328&lt;br /&gt;This paper surveys 133 summarization datasets across over 100 languages, proposes an ontology to enhance resource discovery and coherence in research, and highlights challenges like lack of high-quality datasets for low-resource languages.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models</title><link>https://arxiv.org/abs/2411.04329</link><description>https://arxiv.org/abs/2411.04329&lt;br /&gt;This paper surveys 133 summarization datasets across over 100 languages, highlighting the lack of high-quality resources for low-resource languages and the reliance on the news domain and automatically collected data, while proposing an ontology to facilitate dataset discovery and research coherence.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>DELIFT: Data Efficient Language model Instruction Fine Tuning</title><link>https://arxiv.org/abs/2411.04425</link><description>https://arxiv.org/abs/2411.04425&lt;br /&gt;The paper surveys 133 summarization datasets across over 100 languages to develop an ontology addressing gaps in quality and coverage of summarization resources, especially for low-resource languages, and offers a web tool for exploring dataset properties.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>ACCIO: Table Understanding Enhanced via Contrastive Learning with Aggregations</title><link>https://arxiv.org/abs/2411.04443</link><description>https://arxiv.org/abs/2411.04443&lt;br /&gt;The paper surveys 133 summarization datasets in over 100 languages, proposing a novel ontology to address the diversity in annotation and terminology, highlighting the need for high-quality datasets in low-resource languages and over-reliance on the news domain.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model</title><link>https://arxiv.org/abs/2411.04496</link><description>https://arxiv.org/abs/2411.04496&lt;br /&gt;The paper addresses challenges in the field of automatic summarization by surveying 133 datasets across over 100 languages, creating an ontology to categorize these resources, and highlighting issues such as a lack of high-quality datasets for low-resource languages and an over-reliance on certain domains.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Meta-Reasoning Improves Tool Use in Large Language Models</title><link>https://arxiv.org/abs/2411.04535</link><description>https://arxiv.org/abs/2411.04535&lt;br /&gt;The paper surveys 133 summarization datasets to create a novel ontology addressing issues like lack of high-quality datasets for low-resource languages and over-reliance on news domains, and provides a web interface for exploring these resources.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Multistage Fine-tuning Strategies for Automatic Speech Recognition in Low-resource Languages</title><link>https://arxiv.org/abs/2411.04573</link><description>https://arxiv.org/abs/2411.04573&lt;br /&gt;The paper provides a comprehensive survey of 133 summarization datasets across over 100 languages, highlighting the need for standardized terminology and better access to high-quality datasets, especially for low-resource languages, and offers a web interface and template to support future research.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>NaturalConv: A Chinese Dialogue Dataset Towards Multi-turn Topic-driven Conversation</title><link>https://arxiv.org/abs/2103.02548</link><description>https://arxiv.org/abs/2103.02548&lt;br /&gt;This paper investigates how language models interpret linguistic expressions of uncertainty and evaluates their ability to mimic human-like probabilistic responses, highlighting a susceptibility to bias based on prior knowledge.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Pre-Finetuning for Few-Shot Emotional Speech Recognition</title><link>https://arxiv.org/abs/2302.12921</link><description>https://arxiv.org/abs/2302.12921&lt;br /&gt;The paper explores how language models interpret linguistic expressions of uncertainty, finding that while most models can map these expressions to numeric probabilities similarly to humans, they show biases based on their prior knowledge of a statement's truth.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Textless Speech-to-Speech Translation With Limited Parallel Data</title><link>https://arxiv.org/abs/2305.15405</link><description>https://arxiv.org/abs/2305.15405&lt;br /&gt;The paper investigates how language models interpret and map linguistic expressions of uncertainty to numerical responses, demonstrating that most models can do so in a human-like manner but are biased by their prior knowledge.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research</title><link>https://arxiv.org/abs/2308.13149</link><description>https://arxiv.org/abs/2308.13149&lt;br /&gt;The paper investigates how language models interpret linguistic expressions of uncertainty, finding that most models can map these expressions to probabilistic responses similarly to humans, but with a systematic bias influenced by the model's prior knowledge.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MILPaC: A Novel Benchmark for Evaluating Translation of Legal Text to Indian Languages</title><link>https://arxiv.org/abs/2310.09765</link><description>https://arxiv.org/abs/2310.09765&lt;br /&gt;This paper investigates how language models interpret linguistic expressions of uncertainty to produce numerical responses and how their interpretations compare to human understanding, emphasizing the need to consider biases influenced by their prior knowledge.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Long-form factuality in large language models</title><link>https://arxiv.org/abs/2403.18802</link><description>https://arxiv.org/abs/2403.18802&lt;br /&gt;This paper investigates how language models perceive and map linguistic uncertainty expressions to numerical responses, compared to humans, revealing that most models perform similarly to humans but show biases based on their prior knowledge.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Dynamic Speculation Lookahead Accelerates Speculative Decoding of Large Language Models</title><link>https://arxiv.org/abs/2405.04304</link><description>https://arxiv.org/abs/2405.04304&lt;br /&gt;The paper investigates how language models map linguistic expressions of uncertainty to numerical responses and find that they can mimic human interpretations, but show biases based on prior knowledge.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning</title><link>https://arxiv.org/abs/2406.00922</link><description>https://arxiv.org/abs/2406.00922&lt;br /&gt;The paper examines how language models map linguistic expressions of uncertainty to numerical responses and compares this capability to human interpretations, highlighting biases based on the models' prior knowledge.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition</title><link>https://arxiv.org/abs/2406.14894</link><description>https://arxiv.org/abs/2406.14894&lt;br /&gt;This paper investigates how language models interpret linguistic expressions of uncertainty compared to humans, revealing that many models can map these expressions to probabilistic responses but exhibit systematic biases influenced by their prior knowledge.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>TinyStyler: Efficient Few-Shot Text Style Transfer with Authorship Embeddings</title><link>https://arxiv.org/abs/2406.15586</link><description>https://arxiv.org/abs/2406.15586&lt;br /&gt;This paper investigates how language models map linguistic expressions of uncertainty to numerical responses, comparing their abilities to human interpretations and exposing biases based on prior knowledge.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Teach Better or Show Smarter? On Instructions and Exemplars in Automatic Prompt Optimization</title><link>https://arxiv.org/abs/2406.15708</link><description>https://arxiv.org/abs/2406.15708&lt;br /&gt;This paper investigates how language models interpret linguistic expressions of uncertainty and compares their ability to map these to numerical responses with human understanding, revealing model susceptibilities to biases based on prior knowledge.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need</title><link>https://arxiv.org/abs/2406.18064</link><description>https://arxiv.org/abs/2406.18064&lt;br /&gt;This paper investigates how language models interpret linguistic expressions of uncertainty and find that most models can map these to probabilistic responses similarly to humans, although they show biases based on prior knowledge.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Perceptions to Beliefs: Exploring Precursory Inferences for Theory of Mind in Large Language Models</title><link>https://arxiv.org/abs/2407.06004</link><description>https://arxiv.org/abs/2407.06004&lt;br /&gt;The paper investigates how large language models interpret linguistic uncertainty expressions and compares their performance with humans, revealing that while many models can emulate human-like probabilistic mappings, they exhibit biases based on prior knowledge.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Interpretable Differential Diagnosis with Dual-Inference Large Language Models</title><link>https://arxiv.org/abs/2407.07330</link><description>https://arxiv.org/abs/2407.07330&lt;br /&gt;The paper investigates how language models map linguistic expressions of uncertainty to numerical responses, finding that while most models interpret uncertainty in a human-like manner, they display bias influenced by their prior knowledge.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>A Survey on Employing Large Language Models for Text-to-SQL Tasks</title><link>https://arxiv.org/abs/2407.15186</link><description>https://arxiv.org/abs/2407.15186&lt;br /&gt;This paper investigates how language models and humans interpret linguistic expressions of uncertainty and finds that while many models map these expressions to probabilities similarly to humans, their interpretations are biased by prior knowledge.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Perceptions of Linguistic Uncertainty by Language Models and Humans</title><link>https://arxiv.org/abs/2407.15814</link><description>https://arxiv.org/abs/2407.15814&lt;br /&gt;The paper investigates how language models interpret linguistic expressions of uncertainty and compares their mappings to human interpretations, revealing models' biases based on prior knowledge unlike humans.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL</title><link>https://arxiv.org/abs/2408.07930</link><description>https://arxiv.org/abs/2408.07930&lt;br /&gt;Gradient Cuff is a method designed to detect jailbreak attempts on Large Language Models (LLMs) by analyzing the refusal loss landscape to improve rejection capabilities against malicious queries while maintaining performance on benign queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench (Automated Multi-shot Jailbreaks)</title><link>https://arxiv.org/abs/2408.16163</link><description>https://arxiv.org/abs/2408.16163&lt;br /&gt;Gradient Cuff is a method developed to detect jailbreak attacks on Large Language Models (LLMs) by analyzing the refusal loss landscape, improving the model's rejection of malicious attempts while maintaining performance on benign queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists</title><link>https://arxiv.org/abs/2408.17437</link><description>https://arxiv.org/abs/2408.17437&lt;br /&gt;Gradient Cuff is a method for detecting jailbreak attacks on Large Language Models (LLMs) by analyzing refusal loss landscapes to enhance the models' ability to reject adversarial queries while retaining performance for benign queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations</title><link>https://arxiv.org/abs/2409.02449</link><description>https://arxiv.org/abs/2409.02449&lt;br /&gt;Gradient Cuff is a method to detect jailbreak attacks on Large Language Models (LLMs) by exploring the refusal loss landscapes, aiming to enhance safety by improving the rejection of malicious queries without affecting benign performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>SciDFM: A Large Language Model with Mixture-of-Experts for Science</title><link>https://arxiv.org/abs/2409.18412</link><description>https://arxiv.org/abs/2409.18412&lt;br /&gt;Gradient Cuff is a method developed to detect jailbreak attacks on Large Language Models by analyzing refusal loss landscapes to enhance the models' ability to reject harmful queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare Conversations</title><link>https://arxiv.org/abs/2409.19487</link><description>https://arxiv.org/abs/2409.19487&lt;br /&gt;The paper introduces 'Gradient Cuff', a method that enhances the ability of Large Language Models (LLMs) to detect and counteract jailbreak attacks by analyzing the refusal loss landscape, improving their safety while preserving performance on benign queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>PAD: Personalized Alignment of LLMs at Decoding-Time</title><link>https://arxiv.org/abs/2410.04070</link><description>https://arxiv.org/abs/2410.04070&lt;br /&gt;The Gradient Cuff method is introduced to detect jailbreak attacks on Large Language Models (LLMs) by analyzing refusal loss landscapes, improving the models' ability to reject malicious inputs while preserving performance on benign queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>On the Rigour of Scientific Writing: Criteria, Analysis, and Insights</title><link>https://arxiv.org/abs/2410.04981</link><description>https://arxiv.org/abs/2410.04981&lt;br /&gt;The paper introduces Gradient Cuff, a method to detect adversarial jailbreak attacks on Large Language Models (LLMs) by analyzing the refusal loss landscape to enhance rejection capabilities while maintaining performance on benign queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>GRSQA -- Graph Reasoning-Structured Question Answering Dataset</title><link>https://arxiv.org/abs/2411.00369</link><description>https://arxiv.org/abs/2411.00369&lt;br /&gt;Gradient Cuff is a method for detecting jailbreak attacks on large language models by exploring the refusal loss landscapes to improve rejection of adversarial queries while maintaining performance on legitimate queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Birdie: Advancing State Space Models with Reward-Driven Objectives and Curricula</title><link>https://arxiv.org/abs/2411.01030</link><description>https://arxiv.org/abs/2411.01030&lt;br /&gt;Gradient Cuff is a method designed to detect jailbreak attacks on large language models by analyzing the refusal loss landscape to enhance safety while maintaining performance for benign queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>$B^4$: A Black-Box Scrubbing Attack on LLM Watermarks</title><link>https://arxiv.org/abs/2411.01222</link><description>https://arxiv.org/abs/2411.01222&lt;br /&gt;Gradient Cuff utilizes the refusal loss landscape of Large Language Models (LLMs) to effectively detect and mitigate jailbreak attacks while preserving the models' performance for benign queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Evaluating Creative Short Story Generation in Humans and Large Language Models</title><link>https://arxiv.org/abs/2411.02316</link><description>https://arxiv.org/abs/2411.02316&lt;br /&gt;Gradient Cuff is a method developed to improve the detection of jailbreak attacks on Large Language Models (LLMs) by analyzing the unique properties of refusal loss landscapes to enhance safety without impacting performance on benign queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>FactTest: Factuality Testing in Large Language Models with Finite-Sample and Distribution-Free Guarantees</title><link>https://arxiv.org/abs/2411.02603</link><description>https://arxiv.org/abs/2411.02603&lt;br /&gt;The research presents Gradient Cuff, a method to detect jailbreak attacks on Large Language Models by exploring refusal loss landscapes, improving their rejection capability against adversarial queries while retaining performance for benign inputs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Wave Network: An Ultra-Small Language Model</title><link>https://arxiv.org/abs/2411.02674</link><description>https://arxiv.org/abs/2411.02674&lt;br /&gt;Gradient Cuff is a method that utilizes the unique properties of Refusal Loss in Large Language Models (LLMs) to effectively detect jailbreak attempts, enhancing safety without compromising normal performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>The Translation of Circumlocution in Arabic Short Stories into English</title><link>https://arxiv.org/abs/2411.02887</link><description>https://arxiv.org/abs/2411.02887&lt;br /&gt;Gradient Cuff is a method for detecting jailbreak attacks on Large Language Models (LLMs) by exploring the refusal loss landscapes to improve rejection capabilities against adversarial queries while preserving performance for benign queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Deploying Multi-task Online Server with Large Language Model</title><link>https://arxiv.org/abs/2411.03644</link><description>https://arxiv.org/abs/2411.03644&lt;br /&gt;Gradient Cuff is a method developed to detect adversarial jailbreak attempts on Large Language Models by exploring the unique properties of refusal loss landscapes, enhancing the LLM's rejection capability against malicious queries while preserving performance on benign queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MEG: Medical Knowledge-Augmented Large Language Models for Question Answering</title><link>https://arxiv.org/abs/2411.03883</link><description>https://arxiv.org/abs/2411.03883&lt;br /&gt;Gradient Cuff proposes a two-step detection method leveraging the unique properties of the refusal loss landscape to detect and prevent jailbreak attacks on large language models while maintaining performance on benign queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement</title><link>https://arxiv.org/abs/2411.04090</link><description>https://arxiv.org/abs/2411.04090&lt;br /&gt;Gradient Cuff is a method that enhances the detection of jailbreak attacks on Large Language Models (LLMs) by analyzing refusal loss landscapes, improving the models' ability to reject malicious queries while maintaining performance on legitimate queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model</title><link>https://arxiv.org/abs/2305.15265</link><description>https://arxiv.org/abs/2305.15265&lt;br /&gt;The paper introduces Gradient Cuff, a method for detecting jailbreak attacks on Large Language Models by exploring refusal loss landscapes, enhancing the models' ability to reject malicious queries while maintaining performance on benign requests.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes</title><link>https://arxiv.org/abs/2403.00867</link><description>https://arxiv.org/abs/2403.00867&lt;br /&gt;Gradient Cuff is a method designed to detect jailbreak attempts on large language models by analyzing refusal loss landscapes and improving rejection capabilities while maintaining performance for benign queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>LOVA3: Learning to Visual Question Answering, Asking and Assessment</title><link>https://arxiv.org/abs/2405.14974</link><description>https://arxiv.org/abs/2405.14974&lt;br /&gt;This paper investigates how transformers solve propositional logic problems by identifying the planning and reasoning circuits within the model and analyzing their role in logical reasoning and planning tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Do Large Language Models Truly Grasp Mathematics? An Empirical Exploration From A Psychological Perspective</title><link>https://arxiv.org/abs/2410.14979</link><description>https://arxiv.org/abs/2410.14979&lt;br /&gt;The paper investigates how transformers solve propositional logic problems by identifying 'planning' and 'reasoning' circuits within the network, highlighting cooperation between attention blocks for effective logical reasoning.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>A Breadth-First Catalog of Text Processing, Speech Processing and Multimodal Research in South Asian Languages</title><link>https://arxiv.org/abs/2501.00029</link><description>https://arxiv.org/abs/2501.00029&lt;br /&gt;The paper explores zero-shot strategies to improve length controllability in summarization tasks by implementing methods like length approximation and sample filtering, highlighting significant improvements without requiring model fine-tuning.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Underutilization of Syntactic Processing by Chinese Learners of English in Comprehending English Sentences, Evidenced from Adapted Garden-Path Ambiguity Experiment</title><link>https://arxiv.org/abs/2501.00030</link><description>https://arxiv.org/abs/2501.00030&lt;br /&gt;This paper develops zero-shot strategies for enhancing length-controllable text summarization in large language models (LLMs), emphasizing improvements in maintaining specified summary lengths while preserving quality.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Distilling Large Language Models for Efficient Clinical Information Extraction</title><link>https://arxiv.org/abs/2501.00031</link><description>https://arxiv.org/abs/2501.00031&lt;br /&gt;The paper investigates zero-shot strategies to improve the length-controllability of summaries generated by large language models without requiring model fine-tuning, increasing compliance with target lengths while maintaining quality.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Cross-Linguistic Examination of Machine Translation Transfer Learning</title><link>https://arxiv.org/abs/2501.00045</link><description>https://arxiv.org/abs/2501.00045&lt;br /&gt;The paper presents methods to improve the length control capabilities of large language models (LLMs) for length-controllable summarization in zero-shot settings, achieving better adherence to length targets while maintaining summary quality.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Seq2Seq Model-Based Chatbot with LSTM and Attention Mechanism for Enhanced User Interaction</title><link>https://arxiv.org/abs/2501.00049</link><description>https://arxiv.org/abs/2501.00049&lt;br /&gt;The paper proposes and evaluates methods for improving precise length control in zero-shot summarization tasks using large language models (LLMs) without requiring model adjustments.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>On Adversarial Robustness of Language Models in Transfer Learning</title><link>https://arxiv.org/abs/2501.00066</link><description>https://arxiv.org/abs/2501.00066&lt;br /&gt;This paper explores zero-shot strategies for improving length controllability in summarization tasks with large language models, proposing methods like length approximation, target adjustment, and automated revisions to enhance compliance without model adjustments.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Adversarial Negotiation Dynamics in Generative Language Models</title><link>https://arxiv.org/abs/2501.00069</link><description>https://arxiv.org/abs/2501.00069&lt;br /&gt;The research paper introduces strategies to improve length-control in zero-shot settings for large language models (LLMs), enhancing their ability to generate summaries with precise length adherence without the need for model fine-tuning.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>ICLR: In-Context Learning of Representations</title><link>https://arxiv.org/abs/2501.00070</link><description>https://arxiv.org/abs/2501.00070&lt;br /&gt;The paper proposes methods to improve the length controllability of Large Language Models (LLMs) for summarization without requiring fine-tuning, focusing on zero-shot strategies.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Position Information Emerges in Causal Transformers Without Positional Encodings via Similarity of Nearby Embeddings</title><link>https://arxiv.org/abs/2501.00073</link><description>https://arxiv.org/abs/2501.00073&lt;br /&gt;The paper explores zero-shot strategies for improving length control in large language models during summarization tasks, identifying methods that enhance length compliance while maintaining summary quality.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>CaseSumm: A Large-Scale Dataset for Long-Context Summarization from U.S. Supreme Court Opinions</title><link>https://arxiv.org/abs/2501.00097</link><description>https://arxiv.org/abs/2501.00097&lt;br /&gt;The paper explores zero-shot strategies to improve the length controllability of summarizations generated by large language models (LLMs) without fine-tuning, using methods like length approximation and automated revisions.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>A Data-Centric Approach to Detecting and Mitigating Demographic Bias in Pediatric Mental Health Text: A Case Study in Anxiety Detection</title><link>https://arxiv.org/abs/2501.00129</link><description>https://arxiv.org/abs/2501.00129&lt;br /&gt;The paper proposes zero-shot strategies to enhance the length control capabilities of large language models (LLMs) in summarization tasks, showing improvements in length compliance and summary quality without model fine-tuning.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Temporal reasoning for timeline summarisation in social media</title><link>https://arxiv.org/abs/2501.00152</link><description>https://arxiv.org/abs/2501.00152&lt;br /&gt;The paper introduces zero-shot strategies to improve length control in large language models for summarization tasks by using methods like length approximation and automated revisions, without requiring model fine-tuning.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Measuring Large Language Models Capacity to Annotate Journalistic Sourcing</title><link>https://arxiv.org/abs/2501.00164</link><description>https://arxiv.org/abs/2501.00164&lt;br /&gt;The paper presents strategies for improving length control in large language model (LLM)-based summarization, demonstrating enhanced ability to produce summaries of desired lengths in zero-shot settings without model adjustments.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>The Text Classification Pipeline: Starting Shallow going Deeper</title><link>https://arxiv.org/abs/2501.00174</link><description>https://arxiv.org/abs/2501.00174&lt;br /&gt;The paper presents methods to improve the ability of large language models (LLMs) to perform length-controllable summarization in zero-shot settings without fine-tuning, revealing key challenges and proposing strategies like length approximation and automated revisions.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>GPT-4 on Clinic Depression Assessment: An LLM-Based Pilot Study</title><link>https://arxiv.org/abs/2501.00199</link><description>https://arxiv.org/abs/2501.00199&lt;br /&gt;The paper evaluates strategies for improving length control in large language models for zero-shot text summarization, identifying effective methods to enhance controllability and maintain summary quality.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>An Empirical Evaluation of Large Language Models on Consumer Health Questions</title><link>https://arxiv.org/abs/2501.00208</link><description>https://arxiv.org/abs/2501.00208&lt;br /&gt;The paper proposes zero-shot strategies to improve length control in summarizations generated by large language models, enhancing adherence to target lengths without requiring fine-tuning.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Extracting effective solutions hidden in large language models via generated comprehensive specialists: case studies in developing electronic devices</title><link>https://arxiv.org/abs/2501.00224</link><description>https://arxiv.org/abs/2501.00224&lt;br /&gt;The paper proposes zero-shot methods to improve length controllability in large language model (LLM) summarization tasks, achieving better length compliance without model fine-tuning.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Exploring Variability in Fine-Tuned Models for Text Classification with DistilBERT</title><link>https://arxiv.org/abs/2501.00241</link><description>https://arxiv.org/abs/2501.00241&lt;br /&gt;The paper introduces a novel architecture called Superposition in Transformers to mitigate catastrophic forgetting in large language models by superimposing hidden representations of base and fine-tuned models using autoencoders and B-spline-based blending, enabling dynamic switching and preserving original tasks' performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>EQUATOR: A Deterministic Framework for Evaluating LLM Reasoning with Open-Ended Questions. # v1.0.0-beta</title><link>https://arxiv.org/abs/2501.00257</link><description>https://arxiv.org/abs/2501.00257&lt;br /&gt;This paper introduces a novel way to build a mixture of experts in Transformers by superimposing the hidden representations of base and fine-tuned models, thereby mitigating catastrophic forgetting and allowing dynamic adaptation to new tasks without degrading performance on original tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>A review of faithfulness metrics for hallucination assessment in Large Language Models</title><link>https://arxiv.org/abs/2501.00269</link><description>https://arxiv.org/abs/2501.00269&lt;br /&gt;This paper introduces a novel architecture, Superposition in Transformers, which uses autoencoders to superimpose the hidden representations of base and fine-tuned models to mitigate catastrophic forgetting while adapting large language models to new tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Echoes in AI: Quantifying Lack of Plot Diversity in LLM Outputs</title><link>https://arxiv.org/abs/2501.00273</link><description>https://arxiv.org/abs/2501.00273&lt;br /&gt;This paper introduces 'Superposition in Transformers', a novel approach using autoencoders to mitigate catastrophic forgetting in large language models by superimposing hidden representations of a base and fine-tuned model, allowing for compact domain-specific expertise without degrading original performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models</title><link>https://arxiv.org/abs/2501.00316</link><description>https://arxiv.org/abs/2501.00316&lt;br /&gt;The paper introduces a novel architecture called Superposition in Transformers which uses autoencoders to combine hidden representations of a base model and a fine-tuned model to mitigate catastrophic forgetting and retain domain-specific expertise without degrading original model performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Loss-Aware Curriculum Learning for Chinese Grammatical Error Correction</title><link>https://arxiv.org/abs/2501.00334</link><description>https://arxiv.org/abs/2501.00334&lt;br /&gt;The paper introduces a method called Superposition in Transformers that uses autoencoders to prevent catastrophic forgetting by superimposing hidden representations of a base model and a fine-tuned model.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Rethinking Layer Removal: Preserving Critical Components with Task-Aware Singular Value Decomposition</title><link>https://arxiv.org/abs/2501.00339</link><description>https://arxiv.org/abs/2501.00339&lt;br /&gt;The paper introduces Superposition in Transformers, a technique using autoencoders to mitigate catastrophic forgetting in large language models by superimposing hidden representations and allowing switching between model states.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Chunk-Distilled Language Modeling</title><link>https://arxiv.org/abs/2501.00343</link><description>https://arxiv.org/abs/2501.00343&lt;br /&gt;This paper introduces 'Superposition in Transformers', a method that uses autoencoders to superimpose hidden representations in large language models, mitigating catastrophic forgetting while allowing models to dynamically switch between base and fine-tuned states.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented Instructions</title><link>https://arxiv.org/abs/2501.00353</link><description>https://arxiv.org/abs/2501.00353&lt;br /&gt;This paper introduces a novel transformer architecture called Superposition in Transformers, which uses autoencoders to combine hidden representations of base and fine-tuned models within a shared parameter space, mitigating catastrophic forgetting and enabling dynamic expertise addition.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Trajectories of Change: Approaches for Tracking Knowledge Evolution</title><link>https://arxiv.org/abs/2501.00391</link><description>https://arxiv.org/abs/2501.00391&lt;br /&gt;The paper introduces a novel architecture called Superposition in Transformers that uses autoencoders and B-spline-based blending coefficients to mitigate catastrophic forgetting in language models by superimposing representations from a base model and a fine-tuned model within the same parameter space.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Superposition in Transformers: A Novel Way of Building Mixture of Experts</title><link>https://arxiv.org/abs/2501.00530</link><description>https://arxiv.org/abs/2501.00530&lt;br /&gt;The paper introduces a novel Transformer architecture using autoencoders to superimpose hidden representations, addressing catastrophic forgetting by allowing dynamic switching between base and fine-tuned model states within a shared parameter space.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities</title><link>https://arxiv.org/abs/2501.00571</link><description>https://arxiv.org/abs/2501.00571&lt;br /&gt;The paper introduces TEGA, a logic-aware transformer architecture designed to enhance generalizable first-order logical reasoning in transformers by improving their performance in logical entailment and querying within knowledge graphs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Setting Standards in Turkish NLP: TR-MMLU for Large Language Model Evaluation</title><link>https://arxiv.org/abs/2501.00593</link><description>https://arxiv.org/abs/2501.00593&lt;br /&gt;The paper presents TEGA, a logic-aware transformer architecture designed to enhance the first-order logical entailment capabilities of transformers, providing improved generalizability in reasoning tasks, particularly in knowledge graph querying.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>"Dialogue" vs "Dialog" in NLP and AI research: Statistics from a Confused Discourse</title><link>https://arxiv.org/abs/2501.00598</link><description>https://arxiv.org/abs/2501.00598&lt;br /&gt;The paper explores the ability of transformers to perform first-order logical entailment and proposes a logic-aware architecture, TEGA, to enhance their reasoning capabilities in out-of-distribution scenarios.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Optimizing Speech-Input Length for Speaker-Independent Depression Classification</title><link>https://arxiv.org/abs/2501.00608</link><description>https://arxiv.org/abs/2501.00608&lt;br /&gt;The paper studies the ability of transformers to perform first-order logical entailment and proposes a logic-aware architecture, TEGA, to enhance generalizability in reasoning tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Efficient Standardization of Clinical Notes using Large Language Models</title><link>https://arxiv.org/abs/2501.00644</link><description>https://arxiv.org/abs/2501.00644&lt;br /&gt;The paper explores transformers' capabilities in generalizable first-order logical entailment and proposes a new architecture, TEGA, to improve logical reasoning and query answering in semantic spaces.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Labels Generated by Large Language Model Helps Measuring People's Empathy in Vitro</title><link>https://arxiv.org/abs/2501.00691</link><description>https://arxiv.org/abs/2501.00691&lt;br /&gt;The paper introduces TEGA, a logic-aware architecture, to enhance the first-order logical entailment capabilities of transformers, addressing distribution shifts and query settings in knowledge graph query answering.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding</title><link>https://arxiv.org/abs/2501.00712</link><description>https://arxiv.org/abs/2501.00712&lt;br /&gt;This paper presents TEGA, an enhanced transformer architecture aimed at improving the generalizable first-order logical reasoning capabilities of transformers, specifically in tasks like knowledge graph query answering.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>CODEOFCONDUCT at Multilingual Counterspeech Generation: A Context-Aware Model for Robust Counterspeech Generation in Low-Resource Languages</title><link>https://arxiv.org/abs/2501.00713</link><description>https://arxiv.org/abs/2501.00713&lt;br /&gt;The paper presents TEGA, an enhanced transformer architecture designed to improve generalizable first-order logical entailment by addressing mismatches in positional encoding with logical reasoning tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>On Importance of Layer Pruning for Smaller BERT Models and Low Resource Languages</title><link>https://arxiv.org/abs/2501.00733</link><description>https://arxiv.org/abs/2501.00733&lt;br /&gt;The paper introduces TEGA, a logic-aware architecture that enhances transformers' generalizable first-order logical entailment capabilities, outperforming previous methods in knowledge graph query answering by addressing distribution shifts and mismatched design choices.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines</title><link>https://arxiv.org/abs/2501.00745</link><description>https://arxiv.org/abs/2501.00745&lt;br /&gt;The paper investigates and enhances transformers' ability for first-order logical entailment, proposing a new architecture, TEGA, to improve their reasoning capabilities in knowledge graph queries.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>DIVE: Diversified Iterative Self-Improvement</title><link>https://arxiv.org/abs/2501.00747</link><description>https://arxiv.org/abs/2501.00747&lt;br /&gt;The paper introduces TEGA, a novel transformer architecture designed to enhance generalizable first-order logical entailment by addressing mismatches in positional encoding and other design choices in existing architectures.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation</title><link>https://arxiv.org/abs/2501.00777</link><description>https://arxiv.org/abs/2501.00777&lt;br /&gt;ValuesRAG is a framework that uses Retrieval-Augmented Generation (RAG) and in-context learning to improve cultural values alignment in large language models by dynamically integrating cultural and demographic knowledge during text generation.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Decoding the Flow: CauseMotion for Emotional Causality Analysis in Long-form Conversations</title><link>https://arxiv.org/abs/2501.00778</link><description>https://arxiv.org/abs/2501.00778&lt;br /&gt;ValuesRAG is a framework that enhances cultural alignment in Large Language Models (LLMs) by incorporating Retrieval-Augmented Generation (RAG) with in-context learning to dynamically integrate cultural and demographic knowledge during text generation, reducing Western-centric biases.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Navigating Nuance: In Quest for Political Truth</title><link>https://arxiv.org/abs/2501.00782</link><description>https://arxiv.org/abs/2501.00782&lt;br /&gt;ValuesRAG is a framework that enhances cultural alignment in Large Language Models (LLMs) by employing Retrieval-Augmented Generation (RAG) to dynamically integrate cultural and demographic knowledge during text generation, using the World Values Survey dataset.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Reasoning-Oriented and Analogy-Based Methods for Locating and Editing in Zero-Shot Event-Relational Reasoning</title><link>https://arxiv.org/abs/2501.00803</link><description>https://arxiv.org/abs/2501.00803&lt;br /&gt;ValuesRAG is a framework that enhances cultural alignment in Large Language Models by integrating cultural and demographic knowledge through Retrieval-Augmented Generation and in-context learning, using the World Values Survey to mitigate Western-centric biases.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Embedding Style Beyond Topics: Analyzing Dispersion Effects Across Different Language Models</title><link>https://arxiv.org/abs/2501.00828</link><description>https://arxiv.org/abs/2501.00828&lt;br /&gt;ValuesRAG is a framework that enhances cultural alignment in large language models by using retrieval-augmented generation to dynamically integrate cultural and demographic knowledge during text generation, thus addressing Western-centric biases and improving inclusivity in AI applications.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>LLM+AL: Bridging Large Language Models and Action Languages for Complex Reasoning about Actions</title><link>https://arxiv.org/abs/2501.00830</link><description>https://arxiv.org/abs/2501.00830&lt;br /&gt;ValuesRAG is a framework that enhances cultural alignment in Large Language Models (LLMs) by applying Retrieval-Augmented Generation to dynamically integrate cultural values and demographic knowledge during text generation, overcoming Western-centric biases.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>DiffETM: Diffusion Process Enhanced Embedded Topic Model</title><link>https://arxiv.org/abs/2501.00862</link><description>https://arxiv.org/abs/2501.00862&lt;br /&gt;ValuesRAG is a framework that enhances cultural alignment in Large Language Models (LLMs) by using Retrieval-Augmented Generation (RAG) and in-context learning to integrate cultural knowledge during text generation, improving fairness and representation across different cultures.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Negative to Positive Co-learning with Aggressive Modality Dropout</title><link>https://arxiv.org/abs/2501.00865</link><description>https://arxiv.org/abs/2501.00865&lt;br /&gt;ValuesRAG is a framework that enhances cultural alignment in Large Language Models (LLMs) by using Retrieval-Augmented Generation (RAG) to integrate dynamic cultural and demographic knowledge during text generation, addressing biases and fairness issues in cross-cultural contexts.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Large Language Models Are Read/Write Policy-Makers for Simultaneous Generation</title><link>https://arxiv.org/abs/2501.00868</link><description>https://arxiv.org/abs/2501.00868&lt;br /&gt;ValuesRAG is a framework that enhances cultural alignment in Large Language Models through Retrieval-Augmented Generation and in-context learning, addressing Western-centric biases by integrating dynamic cultural and demographic knowledge during text generation.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models</title><link>https://arxiv.org/abs/2501.00874</link><description>https://arxiv.org/abs/2501.00874&lt;br /&gt;ValuesRAG is a framework that enhances cultural alignment in Large Language Models (LLMs) using Retrieval-Augmented Generation (RAG) to dynamically incorporate cultural and demographic knowledge, outperforming baseline methods in aligning with diverse cultural values.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>TrustRAG: Enhancing Robustness and Trustworthiness in RAG</title><link>https://arxiv.org/abs/2501.00879</link><description>https://arxiv.org/abs/2501.00879&lt;br /&gt;ValuesRAG is a framework that enhances cultural alignment in Large Language Models (LLMs) using Retrieval-Augmented Generation and cultural data integration to dynamically incorporate cultural values into text generation, improving fairness and inclusivity.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Representation in large language models</title><link>https://arxiv.org/abs/2501.00885</link><description>https://arxiv.org/abs/2501.00885&lt;br /&gt;ValuesRAG is a framework that enhances cultural alignment in Large Language Models (LLMs) using Retrieval-Augmented Generation (RAG) and in-context learning to integrate cultural and demographic knowledge dynamically, thereby addressing biases and improving cross-cultural representation.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Unfolding the Headline: Iterative Self-Questioning for News Retrieval and Timeline Summarization</title><link>https://arxiv.org/abs/2501.00888</link><description>https://arxiv.org/abs/2501.00888&lt;br /&gt;ValuesRAG is a framework using Retrieval-Augmented Generation (RAG) with in-context learning to dynamically integrate cultural and demographic knowledge into Large Language Models (LLMs) to enhance cultural alignment and inclusivity.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Incremental Dialogue Management: Survey, Discussion, and Implications for HRI</title><link>https://arxiv.org/abs/2501.00953</link><description>https://arxiv.org/abs/2501.00953&lt;br /&gt;ValuesRAG is a framework that employs Retrieval-Augmented Generation (RAG) and in-context learning to dynamically integrate cultural and demographic knowledge in large language models to address cultural alignment challenges and enhance inclusivity.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Exploring Information Processing in Large Language Models: Insights from Information Bottleneck Theory</title><link>https://arxiv.org/abs/2501.00999</link><description>https://arxiv.org/abs/2501.00999&lt;br /&gt;ValuesRAG is a framework that uses Retrieval-Augmented Generation (RAG) with in-context learning to dynamically integrate cultural and demographic knowledge for more culturally aligned text generation in Large Language Models.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MDSF: Context-Aware Multi-Dimensional Data Storytelling Framework based on Large language Model</title><link>https://arxiv.org/abs/2501.01014</link><description>https://arxiv.org/abs/2501.01014&lt;br /&gt;ValuesRAG is a framework that uses Retrieval-Augmented Generation (RAG) to dynamically align cultural values in Large Language Models (LLMs) by integrating cultural and demographic knowledge during text generation, aiming to address bias and misrepresentation issues in cross-cultural contexts.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model</title><link>https://arxiv.org/abs/2501.01028</link><description>https://arxiv.org/abs/2501.01028&lt;br /&gt;ValuesRAG is a framework that enhances cultural alignment in Large Language Models (LLMs) by using Retrieval-Augmented Generation (RAG) to dynamically integrate cultural and demographic knowledge during text generation.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Reasoning based on symbolic and parametric knowledge bases: a survey</title><link>https://arxiv.org/abs/2501.01030</link><description>https://arxiv.org/abs/2501.01030&lt;br /&gt;ValuesRAG is a framework that enhances cultural alignment in LLMs using Retrieval-Augmented Generation by dynamically incorporating cultural and demographic knowledge during text generation, addressing biases and misrepresentations in cross-cultural contexts.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning</title><link>https://arxiv.org/abs/2501.01031</link><description>https://arxiv.org/abs/2501.01031&lt;br /&gt;ValuesRAG is a framework that enhances cultural alignment in Large Language Models by using Retrieval-Augmented Generation and in-context learning to integrate dynamic cultural and demographic knowledge for more inclusive AI systems.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Advancing Singlish Understanding: Bridging the Gap with Datasets and Multimodal Models</title><link>https://arxiv.org/abs/2501.01034</link><description>https://arxiv.org/abs/2501.01034&lt;br /&gt;ToolComp introduces a new benchmark designed to evaluate AI models' ability to reason and execute complex, multi-step tasks using multiple tools, focusing on both final outcomes and intermediate reasoning processes.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MSWA: Refining Local Attention with Multi-ScaleWindow Attention</title><link>https://arxiv.org/abs/2501.01039</link><description>https://arxiv.org/abs/2501.01039&lt;br /&gt;ToolComp is a benchmark designed to evaluate multi-tool reasoning and process supervision in AI models, focusing on both final outcomes and the correctness of intermediate steps.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>FED: Fast and Efficient Dataset Deduplication Framework with GPU Acceleration</title><link>https://arxiv.org/abs/2501.01046</link><description>https://arxiv.org/abs/2501.01046&lt;br /&gt;ToolComp is a comprehensive benchmark designed to evaluate AI models on complex, multi-step tool-use reasoning tasks, emphasizing the importance of process supervision for improving reasoning accuracy.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Dynamic Scaling of Unit Tests for Code Reward Modeling</title><link>https://arxiv.org/abs/2501.01054</link><description>https://arxiv.org/abs/2501.01054&lt;br /&gt;ToolComp is a benchmark designed to evaluate multi-step tool-use reasoning in AI systems by emphasizing both final outcomes and intermediate reasoning steps, highlighting the importance of process supervision for improving model performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Risks of Cultural Erasure in Large Language Models</title><link>https://arxiv.org/abs/2501.01056</link><description>https://arxiv.org/abs/2501.01056&lt;br /&gt;ToolComp is a comprehensive benchmark for evaluating multi-step tool-use reasoning in AI, focusing on both final outcomes and intermediate reasoning steps, and demonstrating that process-supervised reward models outperform outcome-supervised models.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Dynamic Attention-Guided Context Decoding for Mitigating Context Faithfulness Hallucinations in Large Language Models</title><link>https://arxiv.org/abs/2501.01059</link><description>https://arxiv.org/abs/2501.01059&lt;br /&gt;ToolComp is a benchmark designed to evaluate multi-step tool-use reasoning in AI models, emphasizing the evaluation of both final outcomes and intermediate reasoning processes with a focus on process supervision.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>BeliN: A Novel Corpus for Bengali Religious News Headline Generation using Contextual Feature Fusion</title><link>https://arxiv.org/abs/2501.01069</link><description>https://arxiv.org/abs/2501.01069&lt;br /&gt;ToolComp is a benchmark designed to evaluate multi-step tool-use reasoning in AI systems, assessing both final outcomes and intermediate steps to improve model accuracy, with process supervision playing a critical role in achieving better performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>TED: Turn Emphasis with Dialogue Feature Attention for Emotion Recognition in Conversation</title><link>https://arxiv.org/abs/2501.01123</link><description>https://arxiv.org/abs/2501.01123&lt;br /&gt;ToolComp is a benchmark designed to evaluate multi-step tool-use reasoning in AI models, emphasizing the importance of process supervision in improving the accuracy and robustness of such systems.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>BlockDialect: Block-wise Fine-grained Mixed Format for Energy-Efficient LLM Inference</title><link>https://arxiv.org/abs/2501.01144</link><description>https://arxiv.org/abs/2501.01144&lt;br /&gt;ToolComp is a benchmark designed to evaluate multi-step tool-use reasoning in AI models by evaluating both intermediate reasoning steps and final outcomes, highlighting the importance of process supervision in improving complex reasoning tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Leveraging Full Dependency Parsing Graph Information For Biomedical Event Extraction</title><link>https://arxiv.org/abs/2501.01158</link><description>https://arxiv.org/abs/2501.01158&lt;br /&gt;ToolComp is a comprehensive benchmark designed to evaluate multi-step tool-use reasoning and process supervision in AI models, demonstrating that process-supervised reward models outperform outcome-supervised models in complex reasoning tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Blind Men and the Elephant: Diverse Perspectives on Gender Stereotypes in Benchmark Datasets</title><link>https://arxiv.org/abs/2501.01168</link><description>https://arxiv.org/abs/2501.01168&lt;br /&gt;ToolComp introduces a benchmark for evaluating the multi-step reasoning abilities of AI systems using multiple tools, emphasizing both final outcomes and intermediate reasoning steps to enhance tool-use reasoning effectiveness.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Data Augmentation Techniques for Chinese Disease Name Normalization</title><link>https://arxiv.org/abs/2501.01195</link><description>https://arxiv.org/abs/2501.01195&lt;br /&gt;ToolComp is a benchmark designed to evaluate complex, multi-step tool-use reasoning in AI systems, highlighting the importance of process supervision for improving models' ability to handle intricate reasoning tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Automated Self-Refinement and Self-Correction for LLM-based Product Attribute Value Extraction</title><link>https://arxiv.org/abs/2501.01237</link><description>https://arxiv.org/abs/2501.01237&lt;br /&gt;ToolComp is a benchmark designed to evaluate AI systems on complex, multi-step tool-use reasoning by assessing both final outcomes and intermediate steps through human-edited prompts and supervision labels.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Large Language Model-Enhanced Symbolic Reasoning for Knowledge Base Completion</title><link>https://arxiv.org/abs/2501.01246</link><description>https://arxiv.org/abs/2501.01246&lt;br /&gt;ToolComp is a benchmark designed to evaluate AI systems' ability to perform complex, multi-step reasoning tasks using multiple tools, emphasizing the importance of evaluating both final outcomes and intermediate steps with process supervision.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Digital Guardians: Can GPT-4, Perspective API, and Moderation API reliably detect hate speech in reader comments of German online newspapers?</title><link>https://arxiv.org/abs/2501.01256</link><description>https://arxiv.org/abs/2501.01256&lt;br /&gt;NeutraSum proposes a framework that minimizes media bias in news summaries by integrating neutrality losses to balance semantic space, achieving improved summarization performance and reduced media bias as demonstrated on the Allsides dataset.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings</title><link>https://arxiv.org/abs/2501.01257</link><description>https://arxiv.org/abs/2501.01257&lt;br /&gt;ToolComp is a benchmark designed to evaluate AI systems' ability to perform complex, multi-step reasoning tasks using multiple tools, emphasizing both outcome and intermediate reasoning evaluation.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>ProgCo: Program Helps Self-Correction of Large Language Models</title><link>https://arxiv.org/abs/2501.01264</link><description>https://arxiv.org/abs/2501.01264&lt;br /&gt;ToolComp introduces a benchmark for evaluating multi-step tool-use reasoning in AI models, highlighting the importance of process supervision in improving the generalization of complex reasoning tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Does a Large Language Model Really Speak in Human-Like Language?</title><link>https://arxiv.org/abs/2501.01273</link><description>https://arxiv.org/abs/2501.01273&lt;br /&gt;ToolComp is a benchmark designed to evaluate AI models' capability to perform complex, multi-step reasoning involving multiple tools, highlighting the importance of process supervision over outcome supervision.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>NeutraSum: A Language Model can help a Balanced Media Diet by Neutralizing News Summaries</title><link>https://arxiv.org/abs/2501.01284</link><description>https://arxiv.org/abs/2501.01284&lt;br /&gt;NeutraSum is a framework designed to generate neutral news summaries by integrating neutrality losses to minimize media bias in multi-perspective news articles.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>ToolComp: A Multi-Tool Reasoning &amp; Process Supervision Benchmark</title><link>https://arxiv.org/abs/2501.01290</link><description>https://arxiv.org/abs/2501.01290&lt;br /&gt;ToolComp is a comprehensive benchmark designed to evaluate AI systems' ability to perform complex, multi-step reasoning tasks using multiple tools, highlighting the importance of process supervision in improving model accuracy.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization</title><link>https://arxiv.org/abs/2501.01108</link><description>https://arxiv.org/abs/2501.01108&lt;br /&gt;CultureVLM introduces CultureVerse, a comprehensive multimodal benchmark to enhance and evaluate the cultural understanding capabilities of vision-language models (VLMs) across more than 100 countries by fine-tuning and improving their performance on culturally diverse datasets.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for Test Case Generation</title><link>https://arxiv.org/abs/2501.01329</link><description>https://arxiv.org/abs/2501.01329&lt;br /&gt;ReMamba enhances the Mamba architecture for better performance on long-sequence NLP tasks by incorporating selective compression and adaptation techniques, resulting in improved efficiency with minimal inference costs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct</title><link>https://arxiv.org/abs/2308.09583</link><description>https://arxiv.org/abs/2308.09583&lt;br /&gt;Security Attacks on LLM-based Code Completion Tools explores targeted attack methodologies such as jailbreaking and training data extraction on LLM-based code completion tools, highlighting security vulnerabilities and misalignment in handling code by modern LLMs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Text2Data: Low-Resource Data Generation with Textual Control</title><link>https://arxiv.org/abs/2402.10941</link><description>https://arxiv.org/abs/2402.10941&lt;br /&gt;ReMamba enhances the Mamba architecture for better long-sequence modeling in NLP tasks through selective compression and adaptation techniques, achieving efficiency improvements on benchmarks with minimal additional costs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents</title><link>https://arxiv.org/abs/2402.13184</link><description>https://arxiv.org/abs/2402.13184&lt;br /&gt;The ReMamba model improves the Mamba architecture for better long-sequence understanding in NLP tasks by using selective compression and adaptation techniques, achieving near-transoformer-level performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Synergistic Multi-Agent Framework with Trajectory Learning for Knowledge-Intensive Tasks</title><link>https://arxiv.org/abs/2407.09893</link><description>https://arxiv.org/abs/2407.09893&lt;br /&gt;ReMamba enhances the Mamba architecture's ability to handle long contexts in natural language processing tasks through selective compression and adaptation, achieving a performance nearly on par with transformer models on long-sequence benchmarks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>UPCS: Unbiased Persona Construction for Dialogue Generation</title><link>https://arxiv.org/abs/2409.05257</link><description>https://arxiv.org/abs/2409.05257&lt;br /&gt;The paper introduces CompWoB, a benchmark for evaluating language model agents (LMAs) on complex, real-world compositional web automation tasks, demonstrating significant performance challenges in sequential-task compositions despite success in simpler tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct</title><link>https://arxiv.org/abs/2409.05840</link><description>https://arxiv.org/abs/2409.05840&lt;br /&gt;The paper presents CompWoB, a benchmark for evaluating language model agents (LMAs) on sequential and compositional web automation tasks, revealing significant performance gaps in existing LMAs on compositional tasks and highlighting the need for more robust and generalizable models.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Text Clustering as Classification with LLMs</title><link>https://arxiv.org/abs/2410.00927</link><description>https://arxiv.org/abs/2410.00927&lt;br /&gt;The paper introduces the CompWoB benchmark to evaluate the performance of language model agents (LMAs) on compositional web automation tasks, revealing significant performance degradation in existing LMAs on complex task compositions compared to base tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression</title><link>https://arxiv.org/abs/2410.04139</link><description>https://arxiv.org/abs/2410.04139&lt;br /&gt;The paper presents CompWoB, a benchmark that evaluates the performance limitations of language model agents (LMAs) in sequential-task compositions, revealing significant performance drops in combinational tasks despite proficiency in base tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MaLei at the PLABA Track of TAC-2024: RoBERTa for Task 1 -- LLaMA3.1 and GPT-4o for Task 2</title><link>https://arxiv.org/abs/2411.07381</link><description>https://arxiv.org/abs/2411.07381&lt;br /&gt;The paper introduces CompWoB, a benchmark of 50 compositional web automation tasks, revealing the limitations of language model agents (LMAs) in performing real-world tasks composed of multiple components, with significant performance drops compared to base tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Enhancing LLM Reasoning with Reward-guided Tree Search</title><link>https://arxiv.org/abs/2411.11694</link><description>https://arxiv.org/abs/2411.11694&lt;br /&gt;The paper introduces CompWoB, a benchmark for evaluating language model agents on compositional web automation tasks, highlighting their limitations in task compositionality and proposing a model that improves zero-shot performance on these complex tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>A 2-step Framework for Automated Literary Translation Evaluation: Its Promises and Pitfalls</title><link>https://arxiv.org/abs/2412.01340</link><description>https://arxiv.org/abs/2412.01340&lt;br /&gt;The paper introduces CompWoB, a benchmark testing language model agents on 50 compositional web automation tasks, showing that existing language models perform well on individual tasks but struggle with task compositions, highlighting the need for more robust models for real-world applications.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>BhashaVerse : Translation Ecosystem for Indian Subcontinent Languages</title><link>https://arxiv.org/abs/2412.04351</link><description>https://arxiv.org/abs/2412.04351&lt;br /&gt;The paper assesses the performance of language model agents (LMAs) on complex compositional web automation tasks and introduces the CompWoB benchmark to highlight the challenges and limitations of LMAs in handling real-world task combinations.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework</title><link>https://arxiv.org/abs/2412.10422</link><description>https://arxiv.org/abs/2412.10422&lt;br /&gt;The paper highlights the limitations of language model agents (LMAs) in handling sequential and compositional web automation tasks, introducing the CompWoB benchmark to evaluate and improve LMA robustness and generalizability for real-world applications.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>The Role of Handling Attributive Nouns in Improving Chinese-To-English Machine Translation</title><link>https://arxiv.org/abs/2412.14323</link><description>https://arxiv.org/abs/2412.14323&lt;br /&gt;This paper introduces CompWoB, a benchmark for evaluating language model agents on realistic, compositional web automation tasks, revealing significant performance challenges compared to base tasks and emphasizing the need for robust task compositionality in real-world applications.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Quantifying Positional Biases in Text Embedding Models</title><link>https://arxiv.org/abs/2412.15241</link><description>https://arxiv.org/abs/2412.15241&lt;br /&gt;The paper introduces CompWoB, a benchmark for evaluating language model agents (LMAs) on complex compositional web automation tasks, highlighting significant performance gaps and the need for robust, generalizable LMAs for real-world applications.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Baichuan4-Finance Technical Report</title><link>https://arxiv.org/abs/2412.15270</link><description>https://arxiv.org/abs/2412.15270&lt;br /&gt;The paper presents CompWoB, a benchmark assessing the limitations of language model agents (LMAs) in handling sequential and compositional web automation tasks, highlighting their performance gaps and demonstrating the superior potential of finetuned models like HTML-T5++ in these scenarios.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Token-Budget-Aware LLM Reasoning</title><link>https://arxiv.org/abs/2412.18547</link><description>https://arxiv.org/abs/2412.18547&lt;br /&gt;The paper introduces CompWoB, a benchmark for testing language model agents (LMAs) on compositional web automation tasks, revealing significant performance drop compared to base tasks and emphasizing the need for LMAs that generalize across task combinations.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Zero-resource Speech Translation and Recognition with LLMs</title><link>https://arxiv.org/abs/2412.18566</link><description>https://arxiv.org/abs/2412.18566&lt;br /&gt;The paper introduces CompWoB, a benchmark for assessing language model agents in complex web automation tasks, revealing their limitations in generalization and task compositionality.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Extract Information from Hybrid Long Documents Leveraging LLMs: A Framework and Dataset</title><link>https://arxiv.org/abs/2412.20072</link><description>https://arxiv.org/abs/2412.20072&lt;br /&gt;The paper introduces CompWoB, a benchmark for testing language model agents' performance on compositional web automation tasks, revealing a significant drop in their success rate compared to base tasks and emphasizing the need for improved task compositionality robustness.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria</title><link>https://arxiv.org/abs/2412.21006</link><description>https://arxiv.org/abs/2412.21006&lt;br /&gt;The paper introduces a new benchmark, CompWoB, for evaluating the performance of language model agents (LMAs) on sequential-task compositions, highlighting the limitations and need for robust generalization in real-world web automation tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>An investigation of phrase break prediction in an End-to-End TTS system</title><link>https://arxiv.org/abs/2304.04157</link><description>https://arxiv.org/abs/2304.04157&lt;br /&gt;The paper introduces a new benchmark, CompWoB, to expose the limitations of language model agents (LMAs) in performing sequential-task compositions on the web, highlighting performance degradation in complex and compositional tasks compared to simpler, base tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web</title><link>https://arxiv.org/abs/2311.18751</link><description>https://arxiv.org/abs/2311.18751&lt;br /&gt;The paper introduces a benchmark called CompWoB to evaluate the performance of Language Model Agents (LMAs) on compositional web automation tasks, highlighting a significant performance drop from simple to compositional tasks, and proposes the HTML-T5++ model to improve task compositionality performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Aligning the Objective of LLM-based Program Repair</title><link>https://arxiv.org/abs/2404.08877</link><description>https://arxiv.org/abs/2404.08877&lt;br /&gt;PsychAdapter modifies the architecture of language model transformers to generate text that reflects specific personality traits, demographics, and mental health characteristics, achieving high accuracy in replicating traits like Big Five personalities and depression indicators.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Ethical-Lens: Curbing Malicious Usages of Open-Source Text-to-Image Models</title><link>https://arxiv.org/abs/2404.12104</link><description>https://arxiv.org/abs/2404.12104&lt;br /&gt;PsychAdapter introduces a modification to language model transformers to generate text that reflects specific psychological traits, personalities, and mental health conditions, enhancing applications like personalized chatbots and clinical training tools.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Refining Skewed Perceptions in Vision-Language Models through Visual Representations</title><link>https://arxiv.org/abs/2405.14030</link><description>https://arxiv.org/abs/2405.14030&lt;br /&gt;PsychAdapter is a modification to LLM transformers like GPT-2, allowing them to generate text reflecting specific personality, demographic, and mental health traits, demonstrated in applications like clinical training and personalized chatbots.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>ArguMentor: Augmenting User Experiences with Counter-Perspectives</title><link>https://arxiv.org/abs/2406.02795</link><description>https://arxiv.org/abs/2406.02795&lt;br /&gt;The paper introduces 'PsychAdapter,' a modification to language model transformers designed to generate text reflecting specified personality, demographic, and mental health traits, improving language personalization across models like GPT-2, Gemma, and Llama 3.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MLVU: Benchmarking Multi-task Long Video Understanding</title><link>https://arxiv.org/abs/2406.04264</link><description>https://arxiv.org/abs/2406.04264&lt;br /&gt;PsychAdapter is a modification to language model transformers that generates text reflecting desired personality traits and mental health characteristics, enhancing chatbots and creating tools that mimic psychological language patterns.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>SwitchLoRA: Switched Low-Rank Adaptation Can Learn Full-Rank Information</title><link>https://arxiv.org/abs/2406.06564</link><description>https://arxiv.org/abs/2406.06564&lt;br /&gt;PsychAdapter is a novel modification to transformer architecture allowing language models to generate text reflecting specific personality, demographic, and mental health traits, enhancing applications like chatbots and clinical tools.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents</title><link>https://arxiv.org/abs/2407.01887</link><description>https://arxiv.org/abs/2407.01887&lt;br /&gt;PsychAdapter is a modification to transformer architecture that enables language generation reflecting specific personality, demographic, and mental health traits, enhancing the adaptability of models like GPT-2, Gemma, and Llama 3 to better match desired psychological characteristics.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Personalized Lip Reading: Adapting to Your Unique Lip Movements with Vision and Language</title><link>https://arxiv.org/abs/2409.00986</link><description>https://arxiv.org/abs/2409.00986&lt;br /&gt;PsychAdapter introduces a lightweight modification to language model transformers that enables them to generate natural language reflecting specific personality, demographic, and mental health characteristics, and achieves high accuracy in matching intended traits like Big Five personalities and depression levels.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval</title><link>https://arxiv.org/abs/2409.10516</link><description>https://arxiv.org/abs/2409.10516&lt;br /&gt;PsychAdapter modifies language model transformers to generate text that reflects specific personality, demographic, and mental health traits, enhancing AI-generated language to better match individual psychological behaviors.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>ChemDFM-X: Towards Large Multimodal Model for Chemistry</title><link>https://arxiv.org/abs/2409.13194</link><description>https://arxiv.org/abs/2409.13194&lt;br /&gt;PsychAdapter is a modification to transformer language models enabling them to generate personalized language reflecting individual personality traits, demographics, and mental health characteristics.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension</title><link>https://arxiv.org/abs/2409.13609</link><description>https://arxiv.org/abs/2409.13609&lt;br /&gt;PsychAdapter is a modification to standard language model transformers that generates text reflecting specified personality, demographic, and mental health characteristics, enhancing the adaptability of language models to individual traits.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks</title><link>https://arxiv.org/abs/2410.05160</link><description>https://arxiv.org/abs/2410.05160&lt;br /&gt;PsychAdapter is a method that modifies LLM transformers to generate natural language reflecting specific personality, demographic, and mental health traits by using empirically derived trait-language patterns.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Reconstructive Visual Instruction Tuning</title><link>https://arxiv.org/abs/2410.09575</link><description>https://arxiv.org/abs/2410.09575&lt;br /&gt;PsychAdapter introduces a modification to language model transformers that enables them to generate text reflecting specified personality, demographic, and mental health traits, enhancing the personalization and psychological accuracy of language outputs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>FLARE: Faithful Logic-Aided Reasoning and Exploration</title><link>https://arxiv.org/abs/2410.11900</link><description>https://arxiv.org/abs/2410.11900&lt;br /&gt;PsychAdapter introduces a modification to language model transformers to generate text that reflects specific personality, demographic, and mental health traits, enabling the creation of chatbots with particular profiles and tools for clinical training and exploring psychological constructs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Can Large Audio-Language Models Truly Hear? Tackling Hallucinations with Multi-Task Assessment and Stepwise Audio Reasoning</title><link>https://arxiv.org/abs/2410.16130</link><description>https://arxiv.org/abs/2410.16130&lt;br /&gt;PsychAdapter is a modification to language model transformers that enables the generation of text reflecting specific personality, demographic, and mental health traits, offering personalized language outputs and applications in areas such as chatbots and clinical training tools.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>LLM-Forest: Ensemble Learning of LLMs with Graph-Augmented Prompts for Data Imputation</title><link>https://arxiv.org/abs/2410.21520</link><description>https://arxiv.org/abs/2410.21520&lt;br /&gt;PsychAdapter is a modification to language model transformers that generates text reflecting specific personality, demographic, and mental health characteristics, enhancing AI language generation with psychological trait patterns.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Classical and Quantum Algorithms for the Deterministic L-system Inductive Inference Problem</title><link>https://arxiv.org/abs/2411.19906</link><description>https://arxiv.org/abs/2411.19906&lt;br /&gt;PsychAdapter is a lightweight modification to language model transformers that generates text reflecting specified personality, demographic, and mental health traits without the need for prompting.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Speech Retrieval-Augmented Generation without Automatic Speech Recognition</title><link>https://arxiv.org/abs/2412.16500</link><description>https://arxiv.org/abs/2412.16500&lt;br /&gt;PsychAdapter introduces a lightweight modification to transformer-based language models to generate text reflecting specific personality traits, demographics, and mental health characteristics, enhancing applications such as personalized chatbots and clinical training tools.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Text2midi: Generating Symbolic Music from Captions</title><link>https://arxiv.org/abs/2412.16526</link><description>https://arxiv.org/abs/2412.16526&lt;br /&gt;PsychAdapter is a modification to language model transformers that enables them to generate text reflecting specific personality, demographic, and mental health traits by incorporating empirically derived trait-language patterns, enhancing their adaptability for applications such as chatbots and clinical tools.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>PsychAdapter: Adapting LLM Transformers to Reflect Traits, Personality and Mental Health</title><link>https://arxiv.org/abs/2412.16882</link><description>https://arxiv.org/abs/2412.16882&lt;br /&gt;PsychAdapter introduces a modification to transformer-based language models, allowing generated text to reflect specific personality, demographic, and mental health traits, enhancing customization in language outputs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Explainable AI for Sentiment Analysis of Human Metapneumovirus (HMPV) Using XLNet</title><link>https://arxiv.org/abs/2502.01663</link><description>https://arxiv.org/abs/2502.01663&lt;br&gt;The paper introduces CITER, a collaborative inference framework that optimizes large language model decoding by efficiently routing tokens between small and large models to balance computational costs and performance quality.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>The exception of humour: Iconicity, Phonemic Surprisal, Memory Recall, and Emotional Associations</title><link>https://arxiv.org/abs/2502.01682</link><description>https://arxiv.org/abs/2502.01682&lt;br&gt;CITER introduces a collaborative inference framework that optimizes computational efficiency in large language models by using token-level routing to allocate simpler tasks to small models while leveraging large models for complex tasks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>LLM-Powered Benchmark Factory: Reliable, Generic, and Efficient</title><link>https://arxiv.org/abs/2502.01683</link><description>https://arxiv.org/abs/2502.01683&lt;br&gt;CITER introduces a collaborative inference framework for large language models that optimizes computational efficiency by strategically routing non-critical tokens to smaller models and critical ones to larger models, balancing inference costs and output quality.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Evaluation of Large Language Models via Coupled Token Generation</title><link>https://arxiv.org/abs/2502.01754</link><description>https://arxiv.org/abs/2502.01754&lt;br&gt;The paper presents CITER, a framework for efficient large language model decoding through token-level routing, optimizing inference costs by using small models for non-critical tokens and large models for critical tokens.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Latent Lexical Projection in Large Language Models: A Novel Approach to Implicit Representation Refinement</title><link>https://arxiv.org/abs/2502.01882</link><description>https://arxiv.org/abs/2502.01882&lt;br&gt;The paper introduces CITER, a framework that reduces computational costs of large language models by using token-level routing to efficiently distribute tasks between small and large models during inference.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Boundary-Driven Table-Filling with Cross-Granularity Contrastive Learning for Aspect Sentiment Triplet Extraction</title><link>https://arxiv.org/abs/2502.01942</link><description>https://arxiv.org/abs/2502.01942&lt;br&gt;CITER is a novel framework that enhances the efficiency of large language model decoding by leveraging token-level routing to dynamically allocate computational resources between small and large models based on token importance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Gradient-Regularized Latent Space Modulation in Large Language Models for Structured Contextual Synthesis</title><link>https://arxiv.org/abs/2502.01979</link><description>https://arxiv.org/abs/2502.01979&lt;br&gt;STAIR is a framework that integrates safety alignment with introspective reasoning to enhance the safety and harmlessness of Large Language Models (LLMs) without compromising their performance, using a novel Safety-Informed Monte Carlo Tree Search and iterative preference optimization.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Can LLMs Assist Annotators in Identifying Morality Frames? -- Case Study on Vaccination Debate on Social Media</title><link>https://arxiv.org/abs/2502.01991</link><description>https://arxiv.org/abs/2502.01991&lt;br&gt;STAIR is a framework for improving the safety and alignment of Large Language Models (LLMs) by using introspective reasoning to identify safety risks and optimize responses, effectively balancing safety and performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Wavelet-based Positional Representation for Long Context</title><link>https://arxiv.org/abs/2502.02004</link><description>https://arxiv.org/abs/2502.02004&lt;br&gt;STAIR is a framework that enhances safety alignment in Large Language Models (LLMs) by employing introspective reasoning to mitigate harmful outputs while maintaining performance through a structured, step-by-step analysis and iterative preference optimization.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Reasoning Bias of Next Token Prediction Training</title><link>https://arxiv.org/abs/2502.02007</link><description>https://arxiv.org/abs/2502.02007&lt;br&gt;STAIR is a framework for improving the safety alignment of large language models by integrating introspective reasoning and iterative preference optimization, effectively reducing harmful outputs while maintaining performance against adversarial attacks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Fine-tuning Language Models for Recipe Generation: A Comparative Analysis and Benchmark Study</title><link>https://arxiv.org/abs/2502.02028</link><description>https://arxiv.org/abs/2502.02028&lt;br&gt;The STAIR framework enhances safety alignment in Large Language Models (LLMs) by incorporating introspective reasoning with safety-aware chain-of-thought analysis, effectively balancing performance and security against harmful outputs and jailbreak attacks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>M2R2: Mixture of Multi-Rate Residuals for Efficient Transformer Inference</title><link>https://arxiv.org/abs/2502.02040</link><description>https://arxiv.org/abs/2502.02040&lt;br&gt;The paper introduces STAIR, a framework for improving safety alignment in Large Language Models by combining introspective reasoning with safety-informed strategies, aiming to enhance safety while maintaining performance and resisting jailbreak attacks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Contextual Memory Reweaving in Large Language Models Using Layered Latent State Reconstruction</title><link>https://arxiv.org/abs/2502.02046</link><description>https://arxiv.org/abs/2502.02046&lt;br&gt;The paper introduces STAIR, a framework that enhances safety alignment in Large Language Models (LLMs) by integrating introspective reasoning to mitigate harmful outputs while preserving performance, using a novel Safety-Informed Monte Carlo Tree Search and preference optimization techniques.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>AmaSQuAD: A Benchmark for Amharic Extractive Question Answering</title><link>https://arxiv.org/abs/2502.02047</link><description>https://arxiv.org/abs/2502.02047&lt;br&gt;STAIR is a framework that enhances the safety alignment of Large Language Models (LLMs) by using introspective reasoning and Safety-Informed Monte Carlo Tree Search to mitigate harmful outputs while preserving helpfulness.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping</title><link>https://arxiv.org/abs/2502.02072</link><description>https://arxiv.org/abs/2502.02072&lt;br&gt;STAIR is a framework that enhances safety in Large Language Models by integrating introspective reasoning and Safety-Informed Monte Carlo Tree Search to improve safety alignment while maintaining model helpfulness and robustness against jailbreak attacks.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Rethinking stance detection: A theoretically-informed research agenda for user-level inference using language models</title><link>https://arxiv.org/abs/2502.02074</link><description>https://arxiv.org/abs/2502.02074&lt;br&gt;STAIR is a framework that enhances the safety alignment of Large Language Models (LLMs) by using introspective reasoning and iterative optimization to mitigate harmful outputs while preserving performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information</title><link>https://arxiv.org/abs/2502.02095</link><description>https://arxiv.org/abs/2502.02095&lt;br&gt;The STAIR framework enhances the safety alignment of Large Language Models (LLMs) by integrating introspective reasoning and iterative preference optimization with a focus on maintaining performance while mitigating harmful outputs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Topic Modeling in Marathi</title><link>https://arxiv.org/abs/2502.02100</link><description>https://arxiv.org/abs/2502.02100&lt;br&gt;STAIR is a novel framework for improving the safety alignment of Large Language Models (LLMs) through introspective reasoning and safety-informed Monte Carlo Tree Search, mitigating harmful outputs while preserving performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Multilingual Attribute Extraction from News Web Pages</title><link>https://arxiv.org/abs/2502.02167</link><description>https://arxiv.org/abs/2502.02167&lt;br&gt;The paper proposes STAIR, a novel framework for improving safety alignment in Large Language Models (LLMs) by integrating introspective reasoning with safety-focused chain-of-thought reasoning, aiming to mitigate harmful outputs while maintaining model helpfulness.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Mass-Editing Memory with Attention in Transformers: A cross-lingual exploration of knowledge</title><link>https://arxiv.org/abs/2502.02173</link><description>https://arxiv.org/abs/2502.02173&lt;br&gt;STAIR introduces a framework enhancing Large Language Models' safety by integrating introspective reasoning and iterative preference optimization to mitigate harmful outputs while preserving helpful performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>When Dimensionality Hurts: The Role of LLM Embedding Compression for Noisy Regression Tasks</title><link>https://arxiv.org/abs/2502.02199</link><description>https://arxiv.org/abs/2502.02199&lt;br&gt;The STAIR framework enhances safety in Large Language Models (LLMs) by employing introspective reasoning and Safety-Informed Monte Carlo Tree Search to improve the alignment of safety and performance, minimizing harmful outputs while maintaining helpfulness.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Conversation AI Dialog for Medicare powered by Finetuning and Retrieval Augmented Generation</title><link>https://arxiv.org/abs/2502.02249</link><description>https://arxiv.org/abs/2502.02249&lt;br&gt;STAIR is a framework that enhances safety alignment in Large Language Models by using introspective reasoning and iterative preference optimization to mitigate harmful outputs while preserving helpfulness.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Evalita-LLM: Benchmarking Large Language Models on Italian</title><link>https://arxiv.org/abs/2502.02289</link><description>https://arxiv.org/abs/2502.02289&lt;br&gt;STAIR is a novel framework that enhances the safety alignment of Large Language Models (LLMs) by integrating step-by-step introspective reasoning with safety awareness to reduce harmful outputs without compromising performance.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking</title><link>https://arxiv.org/abs/2502.02339</link><description>https://arxiv.org/abs/2502.02339&lt;br&gt;STAIR is a framework that enhances the safety alignment of Large Language Models (LLMs) by integrating introspective reasoning to identify and mitigate safety risks through iterative preference optimization and structured reasoning capabilities.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs</title><link>https://arxiv.org/abs/2502.02362</link><description>https://arxiv.org/abs/2502.02362&lt;br&gt;STAIR is a framework improving Large Language Models (LLMs) safety by integrating introspective reasoning and iterative preference optimization to minimize harmful outputs while preserving helpfulness.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>STAIR: Improving Safety Alignment with Introspective Reasoning</title><link>https://arxiv.org/abs/2502.02384</link><description>https://arxiv.org/abs/2502.02384&lt;br&gt;STAIR is a framework that enhances the safety and performance balance of Large Language Models (LLMs) by integrating structured introspective reasoning and iterative preference optimization for safety alignment.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>CTC-DRO: Robust Optimization for Reducing Language Disparities in Speech Recognition</title><link>https://arxiv.org/abs/2502.01777</link><description>https://arxiv.org/abs/2502.01777&lt;br&gt;The paper introduces the Soup-of-Experts architecture, which allows efficient instantiation of specialist models for various domain weights through parameter averaging without re-training.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Soup-of-Experts: Pretraining Specialist Models via Parameters Averaging</title><link>https://arxiv.org/abs/2502.01804</link><description>https://arxiv.org/abs/2502.01804&lt;br&gt;The Soup-of-Experts architecture uses a bank of expert parameters to create domain-weighted specialist models for different tasks by averaging parameter sets, allowing for efficient model specialization without re-training.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs</title><link>https://arxiv.org/abs/2502.01926</link><description>https://arxiv.org/abs/2502.01926&lt;br&gt;This paper examines the need for and methods of interpretability in NLP models, highlighting disparities in interpretability needs between different stakeholders and exploring trends over the past decade with a focus on LLMs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Vulnerability Mitigation for Safety-Aligned Language Models via Debiasing</title><link>https://arxiv.org/abs/2502.02153</link><description>https://arxiv.org/abs/2502.02153&lt;br&gt;This paper explores the trends and disparities in NLP model interpretability research, emphasizing the diverse needs and perspectives of different stakeholders, and analyzes thousands of papers using LLMs to offer insights into how interpretability paradigms are applied across various fields.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>VaiBot: Shuttle Between the Instructions and Parameters</title><link>https://arxiv.org/abs/2502.02315</link><description>https://arxiv.org/abs/2502.02315&lt;br&gt;This paper explores the necessity and methods of interpretability in NLP models, especially in the context of LLMs, and analyzes trends across research fields to highlight the diverse needs and perspectives of different stakeholders involved in NLP model usage.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>ReSpark: Leveraging Previous Data Reports as References to Generate New Reports with LLMs</title><link>https://arxiv.org/abs/2502.02329</link><description>https://arxiv.org/abs/2502.02329&lt;br&gt;The paper analyzes the trends in NLP model interpretability with the rise of LLMs, focusing on the diverse needs of stakeholders across different fields and the implications for designing methods that align with their objectives.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2502.02464</link><description>https://arxiv.org/abs/2502.02464&lt;br&gt;This paper examines the evolution of model interpretability in NLP with the rise of LLMs, highlighting the differing requirements of stakeholders and proposing future directions for developing explanations aligned with these needs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Analyzing Similarity Metrics for Data Selection for Language Model Pretraining</title><link>https://arxiv.org/abs/2502.02494</link><description>https://arxiv.org/abs/2502.02494&lt;br&gt;The paper explores trends in NLP model interpretability, focusing on the diverse needs of stakeholders and examining interpretability paradigms and their applicability across different domains in the era of LLMs.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies</title><link>https://arxiv.org/abs/2502.02533</link><description>https://arxiv.org/abs/2502.02533&lt;br&gt;This paper analyzes trends in NLP model interpretability with a focus on the diverse needs of different stakeholders and highlights disparities between developers and non-developers across various fields.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Interaction Matters: An Evaluation Framework for Interactive Dialogue Assessment on English Second Language Conversations</title><link>https://arxiv.org/abs/2407.06479</link><description>https://arxiv.org/abs/2407.06479&lt;br&gt;The paper explores the need for interpretability in NLP models, especially LLMs, by addressing stakeholders' perspectives and analyzing trends over the last decade to inform future model design and application.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>Reasoning Aware Self-Consistency: Leveraging Reasoning Paths for Efficient LLM Sampling</title><link>https://arxiv.org/abs/2408.17017</link><description>https://arxiv.org/abs/2408.17017&lt;br&gt;The paper proposes a conceptual AI framework for assessing the intelligibility of dysarthric speech across multiple languages by leveraging universal and language-specific models while addressing key challenges like data scarcity and annotation complexity.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item><item><title>LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with Effortless Adaptation</title><link>https://arxiv.org/abs/2410.13846</link><description>https://arxiv.org/abs/2410.13846&lt;br&gt;This commentary discusses the potential of artificial intelligence to improve the assessment of dysarthric speech across different languages through a framework that incorporates both universal and language-specific intelligibility models.</description><pubDate>Wed, 05 Feb 2025 06:53:31 GMT</pubDate></item></channel></rss>