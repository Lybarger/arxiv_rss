<?xml version="1.0" encoding="iso-8859-1"?>
<rss version="2.0"><channel><title>Filtered Computational Linguistics Papers</title><link>https://Lybarger.github.io/arxiv_rss/rss.xml</link><description>Filtered arXiv papers on computational linguistics</description><lastBuildDate>Sat, 19 Oct 2024 11:16:46 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>ActiveRAG: Autonomously Knowledge Assimilation and Accommodation through Retrieval-Augmented Agents</title><link>https://arxiv.org/abs/2402.13547</link><description>The paper presents ActiveRAG, a multi-agent framework designed to enhance Large Language Models (LLMs) by enabling them to actively engage with and learn from external knowledge through a knowledge assimilation agent and a thought accommodation agent. ActiveRAG improves performance on knowledge-intensive tasks by 10% over traditional RAG approaches, addressing issues related to noisy retrievals and inconsistencies in LLM responses.</description><pubDate>Sat, 19 Oct 2024 11:16:46 GMT</pubDate></item><item><title>LLoCO: Learning Long Contexts Offline</title><link>https://arxiv.org/abs/2404.07979</link><description>The paper introduces LLoCO, an innovative approach that allows large language models (LLMs) to efficiently handle long contexts by learning contexts offline through context compression and parameter-efficient finetuning with LoRA. LLoCO significantly improves long-context question-answering tasks by extending the effective context window from 4k to 128k tokens, achieving notable speed-ups and reduced costs in processing.</description><pubDate>Sat, 19 Oct 2024 11:16:46 GMT</pubDate></item></channel></rss>