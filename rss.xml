<?xml version="1.0" encoding="iso-8859-1"?>
<rss version="2.0"><channel><title>arXiv Papers</title><link>https://Lybarger.github.io/arxiv_rss/rss.xml</link><description>arXiv Papers</description><lastBuildDate>Sat, 19 Oct 2024 11:18:36 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>ActiveRAG: Autonomously Knowledge Assimilation and Accommodation through Retrieval-Augmented Agents</title><link>https://arxiv.org/abs/2402.13547</link><description>This paper presents ActiveRAG, a multi-agent framework designed to enhance the knowledge assimilation and response refinement capabilities of Large Language Models (LLMs) by allowing them to actively engage with retrieved evidence. Through experiments, ActiveRAG demonstrates a 10% performance improvement in question-answering tasks and effectively mitigates the negative impact of noisy retrieved content.</description><pubDate>Sat, 19 Oct 2024 11:18:36 GMT</pubDate></item><item><title>LLoCO: Learning Long Contexts Offline</title><link>https://arxiv.org/abs/2404.07979</link><description>The paper presents LLoCO, a novel method that enhances large language models' ability to process long contexts by using offline learning and context compression, allowing for an effective context window extension from 4k to 128k tokens. LLoCO achieves significant performance improvements in long-context question-answering tasks while reducing inference token usage and substantially increasing processing speed.</description><pubDate>Sat, 19 Oct 2024 11:18:36 GMT</pubDate></item></channel></rss>