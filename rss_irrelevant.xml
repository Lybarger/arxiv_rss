<?xml version="1.0" encoding="iso-8859-1"?>
<rss version="2.0"><channel><title>Irrelevant arXiv Papers</title><link>https://Lybarger.github.io/arxiv_rss/rss_irrelevant.xml</link><description>Irrelevant arXiv Papers</description><lastBuildDate>Thu, 24 Oct 2024 11:34:06 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Captions Speak Louder than Images (CASLIE): Generalizing Foundation Models for E-commerce from High-quality Multimodal Instruction Data</title><link>https://arxiv.org/abs/2410.17337</link><description>https://arxiv.org/abs/2410.17337&lt;br&gt;The Language Model Linguistic Personality Assessment (LMLPA) is a system that evaluates the linguistic personalities of Large Language Models (LLMs) by adapting the Big Five Inventory for assessing distinct personality traits in their outputs, contributing to improved understanding and application of LLMs in conversational interactions.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>AMUSD: Asynchronous Multi-Device Speculative Decoding for LLM Acceleration</title><link>https://arxiv.org/abs/2410.17375</link><description>https://arxiv.org/abs/2410.17375&lt;br&gt;The Language Model Linguistic Personality Assessment (LMLPA) introduces a system for evaluating the linguistic personalities of Large Language Models (LLMs) through an adapted personality assessment questionnaire and AI rating methods, highlighting the distinct personality traits reflected in LLM outputs.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities</title><link>https://arxiv.org/abs/2410.17385</link><description>https://arxiv.org/abs/2410.17385&lt;br&gt;The Language Model Linguistic Personality Assessment (LMLPA) is introduced as a system to quantitatively evaluate the personality traits of Large Language Models (LLMs) using an adapted version of the Big Five Inventory, contributing to better understanding and integration of LLMs in various applications.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Scalable Influence and Fact Tracing for Large Language Model Pretraining</title><link>https://arxiv.org/abs/2410.17413</link><description>https://arxiv.org/abs/2410.17413&lt;br&gt;The LMLPA (Language Model Linguistic Personality Assessment) introduces a system for quantifying the personality traits of Large Language Models (LLMs) based on their linguistic outputs, using an adapted version of the Big Five Inventory to measure distinct personality traits and contributing to the fields of Human-Computer Interaction and Human-Centered AI.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Evaluating AI-Generated Essays with GRE Analytical Writing Assessment</title><link>https://arxiv.org/abs/2410.17439</link><description>https://arxiv.org/abs/2410.17439&lt;br&gt;The paper introduces the Language Model Linguistic Personality Assessment (LMLPA), a system that quantitatively evaluates the distinct personality traits of Large Language Models (LLMs) by adapting the Big Five Inventory for their operational capabilities, facilitating a better understanding of their language generation capabilities in conversational interactions.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>In Context Learning and Reasoning for Symbolic Regression with Large Language Models</title><link>https://arxiv.org/abs/2410.17448</link><description>https://arxiv.org/abs/2410.17448&lt;br&gt;The Language Model Linguistic Personality Assessment (LMLPA) is a system designed to evaluate the distinct personality traits of Large Language Models (LLMs) based on their linguistic outputs, adapting traditional psychometric assessments to effectively quantify these traits in an open-ended format.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination</title><link>https://arxiv.org/abs/2410.17477</link><description>https://arxiv.org/abs/2410.17477&lt;br&gt;The Language Model Linguistic Personality Assessment (LMLPA) introduces a system for quantitatively evaluating the linguistic personality traits of Large Language Models (LLMs) based on their language outputs, adapting the Big Five Inventory for AI capabilities and providing insights into Human-Computer Interaction.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Is artificial intelligence still intelligence? LLMs generalize to novel adjective-noun pairs, but don't mimic the full human distribution</title><link>https://arxiv.org/abs/2410.17482</link><description>https://arxiv.org/abs/2410.17482&lt;br&gt;LMLPA introduces a novel system for evaluating the linguistic personalities of Large Language Models (LLMs) by adapting the Big Five Inventory for AI, allowing for the quantification of distinct personality traits reflected in their language generation outputs.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>VoiceTextBlender: Augmenting Large Language Models with Speech Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning</title><link>https://arxiv.org/abs/2410.17485</link><description>https://arxiv.org/abs/2410.17485&lt;br&gt;The Language Model Linguistic Personality Assessment (LMLPA) introduces a quantitative system for evaluating the linguistic personalities of Large Language Models (LLMs), adapting the Big Five Inventory to assess distinct personality traits based on LLM-generated outputs, which can enhance understanding in fields such as Human-Computer Interaction and Human-Centered AI.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Large Language Models Still Exhibit Bias in Long Text</title><link>https://arxiv.org/abs/2410.17519</link><description>https://arxiv.org/abs/2410.17519&lt;br&gt;The Language Model Linguistic Personality Assessment (LMLPA) introduces a system for quantitatively assessing the linguistic personalities of Large Language Models (LLMs) by adapting the Big Five Inventory into an open-ended questionnaire format, enabling evaluation of distinct personality traits reflected in LLM outputs and enhancing understanding of their conversational capabilities.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact</title><link>https://arxiv.org/abs/2410.17532</link><description>https://arxiv.org/abs/2410.17532&lt;br&gt;The Language Model Linguistic Personality Assessment (LMLPA) introduces a system to evaluate the linguistic personalities of Large Language Models (LLMs) by adapting the Big Five Inventory questionnaire to quantify distinct personality traits in their outputs, enhancing understanding of LLM conversational interactions.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>ProtoLens: Advancing Prototype Learning for Fine-Grained Interpretability in Text Classification</title><link>https://arxiv.org/abs/2410.17546</link><description>https://arxiv.org/abs/2410.17546&lt;br&gt;The LMLPA is a system that quantitatively assesses the linguistic personalities of Large Language Models (LLMs) by adapting the Big Five Inventory for evaluating their language generation capabilities, thereby enhancing understanding of their personality traits in conversational interactions.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>ESpeW: Robust Copyright Protection for LLM-based EaaS via Embedding-Specific Watermark</title><link>https://arxiv.org/abs/2410.17552</link><description>https://arxiv.org/abs/2410.17552&lt;br&gt;LMLPA is a system that quantitatively assesses the linguistic personalities of Large Language Models (LLMs) by adapting the Big Five Inventory for personality measurement, enabling a clearer understanding of LLMs' language generation capabilities through a novel framework that incorporates AI assessment methods.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Graphusion: A RAG Framework for Knowledge Graph Construction with a Global Perspective</title><link>https://arxiv.org/abs/2410.17600</link><description>https://arxiv.org/abs/2410.17600&lt;br&gt;LMLPA introduces a system for assessing the linguistic personality traits of Large Language Models (LLMs) using a modified Big Five Inventory to evaluate their language generation capabilities in conversational interactions, contributing to fields like Human-Computer Interaction and Human-Centered AI.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>LMLPA: Language Model Linguistic Personality Assessment</title><link>https://arxiv.org/abs/2410.17632</link><description>https://arxiv.org/abs/2410.17632&lt;br&gt;The paper presents the Language Model Linguistic Personality Assessment (LMLPA), a system designed to evaluate the linguistic personalities of Large Language Models (LLMs) by adapting the Big Five Inventory to quantitatively assess distinct personality traits in their outputs, thereby enhancing understanding in Human-Computer Interaction and Human-Centered AI.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents</title><link>https://arxiv.org/abs/2410.17657</link><description>https://arxiv.org/abs/2410.17657&lt;br&gt;The paper analyzes historical texts from the German Bundestag using a time series variant of the topic model LDA to investigate the impact of significant events on the evolution of political discourse in Germany since the formation of the Federal Republic in 1949.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Quantifying the Risks of Tool-assisted Rephrasing to Linguistic Diversity</title><link>https://arxiv.org/abs/2410.17670</link><description>https://arxiv.org/abs/2410.17670&lt;br&gt;The paper discusses an analysis of changes in the German political discourse from 1871 to the present using a time series variant of the topic model LDA applied to digitized transcripts of plenary sessions of the German Bundestag, highlighting key events that influenced the evolution of political discussions.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Towards a Similarity-adjusted Surprisal Theory</title><link>https://arxiv.org/abs/2410.17676</link><description>https://arxiv.org/abs/2410.17676&lt;br&gt;The paper analyzes the evolution of political discourse in Germany from historical events using a time series variant of the topic model LDA, examining changes in word frequency and key discussion points in the Bundestag sessions logged since 1949.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>An Adaptive Framework for Generating Systematic Explanatory Answer in Online Q&amp;A Platforms</title><link>https://arxiv.org/abs/2410.17694</link><description>https://arxiv.org/abs/2410.17694&lt;br&gt;The paper analyzes the evolution of political discourse in Germany from 1871 to the present using a time series variant of the topic model LDA on digitized records of Bundestag plenary sessions to identify significant events and changes in discussion points.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Beware of Calibration Data for Pruning Large Language Models</title><link>https://arxiv.org/abs/2410.17711</link><description>https://arxiv.org/abs/2410.17711&lt;br&gt;The paper explores changes in German political discourse by analyzing digitized texts from Bundestag plenary sessions using a time series variant of LDA topic modeling to identify significant events and shifts in discussion topics over time.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>CogSteer: Cognition-Inspired Selective Layer Intervention for Efficient Semantic Steering in Large Language Models</title><link>https://arxiv.org/abs/2410.17714</link><description>https://arxiv.org/abs/2410.17714&lt;br&gt;The paper 'Zeitenwenden' analyzes digitized transcripts of German Bundestag plenary sessions using a time series variant of the topic model LDA to identify significant events and changes in political discourse over time.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Dialectal and Low Resource Machine Translation for Aromanian</title><link>https://arxiv.org/abs/2410.17728</link><description>https://arxiv.org/abs/2410.17728&lt;br&gt;The paper analyzes changes in German political discourse over time through a time series variant of the topic model LDA, using digitized records of plenary sessions from the Bundestag to identify events that have significantly affected political discussions.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>MojoBench: Language Modeling and Benchmarks for Mojo</title><link>https://arxiv.org/abs/2410.17736</link><description>https://arxiv.org/abs/2410.17736&lt;br&gt;The paper presents an analysis of the evolution of political discourse in Germany using a time series variant of the LDA topic model to detect changes in word frequency and key discussion points since the establishment of the Federal Republic of Germany in 1949.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Local Contrastive Editing of Gender Stereotypes</title><link>https://arxiv.org/abs/2410.17739</link><description>https://arxiv.org/abs/2410.17739&lt;br&gt;The paper investigates the evolution of German political discourse from 1871 to the present by analyzing digitized transcripts of Bundestag sessions using a time series variant of topic modeling to identify significant changes in key discussion points over time.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Latent Structures of Intertextuality in French Fiction</title><link>https://arxiv.org/abs/2410.17759</link><description>https://arxiv.org/abs/2410.17759&lt;br&gt;The paper analyzes digitized transcripts of German Bundestag plenary sessions using a time series variant of the topic model LDA to detect changes in political discourse and identify key discussion points influenced by historical events since 1949.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination</title><link>https://arxiv.org/abs/2410.17783</link><description>https://arxiv.org/abs/2410.17783&lt;br&gt;The paper analyzes historical German political discourse from 1949 to the present by employing a time series variant of the topic model LDA to detect changes in key discussion points and word frequency in response to significant political events.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation</title><link>https://arxiv.org/abs/2410.17799</link><description>https://arxiv.org/abs/2410.17799&lt;br&gt;The paper presents a time series analysis of political discourse in Germany using a topic model to investigate how key discussion points and word frequencies have changed over time in the German Bundestag, reflecting shifts from monarchy to democracy and other political transformations.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Understanding When Tree of Thoughts Succeeds: Larger Models Excel in Generation, Not Discrimination</title><link>https://arxiv.org/abs/2410.17820</link><description>https://arxiv.org/abs/2410.17820&lt;br&gt;The paper analyzes the evolution of German political discourse since 1871 by employing a time series variant of topic modeling on digitized plenary session texts of the Bundestag to detect significant changes in political topics and key discussion points over time.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Understanding Layer Significance in LLM Alignment</title><link>https://arxiv.org/abs/2410.17875</link><description>https://arxiv.org/abs/2410.17875&lt;br&gt;The paper analyzes the evolution of the German political discourse since the establishment of the Federal Republic of Germany in 1949 by employing a time series variant of the LDA topic model to detect significant changes in political topics and key discussion points over time, using digitized records of plenary sessions from the Bundestag.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>SpeakGer: A meta-data enriched speech corpus of German state and federal parliaments</title><link>https://arxiv.org/abs/2410.17886</link><description>https://arxiv.org/abs/2410.17886&lt;br&gt;The paper analyzes historical texts from the German Bundestag using a time series variant of the LDA topic model to detect changes in political discourse and key discussion points over time in response to significant events.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Scaling Diffusion Language Models via Adaptation from Autoregressive Models</title><link>https://arxiv.org/abs/2410.17891</link><description>https://arxiv.org/abs/2410.17891&lt;br&gt;The paper analyzes digitized plenary session texts from the German Bundestag since 1949 using a time series variant of topic modeling to detect changes in political discourse and the impact of historical events on political topics over time.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Value Residual Learning For Alleviating Attention Concentration In Transformers</title><link>https://arxiv.org/abs/2410.17897</link><description>https://arxiv.org/abs/2410.17897&lt;br&gt;The paper analyzes changes in the German political discourse from 1871 to the present by employing a time series variant of the topic model LDA to examine digitized texts from every plenary session of the German Bundestag, focusing on how key political topics have evolved over time.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>ELAICHI: Enhancing Low-resource TTS by Addressing Infrequent and Low-frequency Character Bigrams</title><link>https://arxiv.org/abs/2410.17901</link><description>https://arxiv.org/abs/2410.17901&lt;br&gt;The paper presents an analysis of the German political discourse from 1871 to the present, leveraging a time series variant of the topic model LDA to identify how significant historical events have influenced changes in political discussions as reflected in the digitized texts of the German Bundestag.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains</title><link>https://arxiv.org/abs/2410.17952</link><description>https://arxiv.org/abs/2410.17952&lt;br&gt;The paper investigates changes in the German political discourse from 1871 to the present by analyzing digitized plenary session texts of the Bundestag using a time series version of the LDA topic model to identify lasting effects of historical events on political discussions.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Zeitenwenden: Detecting changes in the German political discourse</title><link>https://arxiv.org/abs/2410.17960</link><description>https://arxiv.org/abs/2410.17960&lt;br&gt;The paper analyzes historical political discourse in Germany from 1871 to the present using a time series variant of topic modeling to detect changes in discussions and key topics influenced by significant political events.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Dependency Graph Parsing as Sequence Labeling</title><link>https://arxiv.org/abs/2410.17972</link><description>https://arxiv.org/abs/2410.17972&lt;br&gt;This paper presents a time-aware approach, involving a CPI+DMC strategy, for the early detection of signs of anorexia, which emphasizes the integration of temporal metrics during the training process to enhance precision and speed in risk detection tasks.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning</title><link>https://arxiv.org/abs/2410.18035</link><description>https://arxiv.org/abs/2410.18035&lt;br&gt;The study presents a time-aware approach for the early detection of anorexia, focusing on both precision and speed through a CPI+DMC methodology and the integration of temporal metrics during the learning process.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Literature Meets Data: A Synergistic Approach to Hypothesis Generation</title><link>https://arxiv.org/abs/2410.17309</link><description>https://arxiv.org/abs/2410.17309&lt;br&gt;This paper presents a time-aware approach for early detection of anorexia that emphasizes both precision and speed, utilizing a CPI+DMC method and integrating temporal factors during the learning process to improve risk detection metrics in the eRisk 2024 competition.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents</title><link>https://arxiv.org/abs/2410.17401</link><description>https://arxiv.org/abs/2410.17401&lt;br&gt;The paper presents a time-aware method for the early detection of anorexia, utilizing a CPI+DMC approach and integrating time during the learning process to enhance precision and speed for risk detection on the web.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers</title><link>https://arxiv.org/abs/2410.17492</link><description>https://arxiv.org/abs/2410.17492&lt;br&gt;This paper presents a time-aware approach for early detection of anorexia in the context of the eRisk 2024 competition, where the research group uses a CPI+DMC methodology to optimize both precision and speed in identifying risks online, successfully integrating temporal considerations into their model training and evaluation processes.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>ExpertFlow: Optimized Expert Activation and Token Allocation for Efficient Mixture-of-Experts Inference</title><link>https://arxiv.org/abs/2410.17954</link><description>https://arxiv.org/abs/2410.17954&lt;br&gt;This study proposes a time-aware approach for the early detection of anorexia that integrates precision and speed as a combined objective during the learning process, achieving significant results in the eRisk 2024 competition.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration</title><link>https://arxiv.org/abs/2410.18032</link><description>https://arxiv.org/abs/2410.18032&lt;br&gt;The paper presents a unified framework called Fast and Slow Generating (FS-GEN) to analyze collaborative decoding between Large Language Models (LLMs) and Small Language Models (SLMs), offering insights into their distinct roles in improving inference speed and mitigating issues like hallucinations in language generation tasks.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>CLEAR: Character Unlearning in Textual and Visual Modalities</title><link>https://arxiv.org/abs/2410.18057</link><description>https://arxiv.org/abs/2410.18057&lt;br&gt;The paper presents a unified framework, Fast and Slow Generating (FS-GEN), analyzing collaborative decoding between large and small language models to address inference latency and hallucination issues, demonstrating that minimal interactions are necessary for effective collaboration while highlighting the distinct roles of 'slow' and 'fast' models in this process.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts</title><link>https://arxiv.org/abs/2410.18071</link><description>https://arxiv.org/abs/2410.18071&lt;br&gt;The paper presents a unified framework called Fast and Slow Generating (FS-GEN) for collaborative decoding between Large Language Models (LLMs) and Small Language Models (SLMs), analyzing their respective roles in generating responses with a focus on optimizing performance and minimizing hallucinations through effective collaboration.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>ALTA: Compiler-Based Analysis of Transformers</title><link>https://arxiv.org/abs/2410.18077</link><description>https://arxiv.org/abs/2410.18077&lt;br&gt;This paper introduces the Fast and Slow Generating (FS-GEN) framework to analyze collaborative decoding strategies between large and small language models, categorizing them into Systems 1 and 2 based on their speed and cognitive approach, and providing insights into their interactions and effectiveness under uncertainty.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>GPT-SW3: An Autoregressive Language Model for the Nordic Languages</title><link>https://arxiv.org/abs/2305.12987</link><description>https://arxiv.org/abs/2305.12987&lt;br&gt;The paper introduces the Fast and Slow Generating (FS-GEN) framework, which analyzes the collaborative decoding of large and small language models, identifying their roles and interactions under the dual-process cognitive theory while exploring the conditions that maximize their effective collaboration.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Let Me Teach You: Pedagogical Foundations of Feedback for Language Models</title><link>https://arxiv.org/abs/2307.00279</link><description>https://arxiv.org/abs/2307.00279&lt;br&gt;The paper presents a unified framework, Fast and Slow Generating (FS-GEN), that analyzes collaborative decoding strategies between large and small language models, shedding light on their interaction dynamics and conditions for effectiveness based on dual-process cognitive theory.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications</title><link>https://arxiv.org/abs/2311.05876</link><description>https://arxiv.org/abs/2311.05876&lt;br&gt;This paper introduces the Fast and Slow Generating (FS-GEN) framework to analyze and optimize collaborative decoding between Large Language Models (LLMs) and Small Language Models (SLMs), providing insights into their differential knowledge capabilities and the conditions for effective collaboration.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>The Causal Influence of Grammatical Gender on Distributional Semantics</title><link>https://arxiv.org/abs/2311.18567</link><description>https://arxiv.org/abs/2311.18567&lt;br&gt;The paper proposes a unified framework called Fast and Slow Generating (FS-GEN) that explores the collaborative decoding between large and small language models to mitigate challenges such as inference latency and hallucinations, drawing inspiration from dual-process cognitive theory to categorize model interactions and analyze their effectiveness across various methodologies.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>TravelPlanner: A Benchmark for Real-World Planning with Language Agents</title><link>https://arxiv.org/abs/2402.01622</link><description>https://arxiv.org/abs/2402.01622&lt;br&gt;The paper presents a unified framework called Fast and Slow Generating (FS-GEN) to analyze and optimize collaborative decoding between large and small language models, highlighting their respective roles and the conditions that enhance their cooperation in reducing issues like inference latency and hallucinations.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Reconfidencing LLMs from the Grouping Loss Perspective</title><link>https://arxiv.org/abs/2402.04957</link><description>https://arxiv.org/abs/2402.04957&lt;br&gt;The paper introduces the Fast and Slow Generating (FS-GEN) framework to explore collaborative decoding between Large Language Models (LLMs) and small language models (SLMs), inspired by dual-process cognitive theory, and emphasizes how varying levels of collaboration can effectively reduce inference latency and enhance performance while managing the complexities of model interactions.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers</title><link>https://arxiv.org/abs/2402.10601</link><description>https://arxiv.org/abs/2402.10601&lt;br&gt;The paper presents a unified framework called Fast and Slow Generating (FS-GEN) for understanding collaborative decoding between Large Language Models (LLMs) and small language models (SLMs), highlighting the dynamics of their interaction, particularly in managing uncertainty in token prediction.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Reinforcement Learning with Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation</title><link>https://arxiv.org/abs/2402.14146</link><description>https://arxiv.org/abs/2402.14146&lt;br&gt;This paper presents the Fast and Slow Generating (FS-GEN) framework, analyzing the collaborative decoding between Large Language Models (LLMs) and Small Language Models (SLMs) inspired by dual-process cognitive theory, and reveals effective conditions for their interaction to optimize performance amidst challenges like inference latency and hallucination generation.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions</title><link>https://arxiv.org/abs/2402.15055</link><description>https://arxiv.org/abs/2402.15055&lt;br&gt;This paper presents the Fast and Slow Generating (FS-GEN) framework for collaborative decoding between large and small language models, inspired by dual-process cognitive theory, to mitigate issues such as inference latency and hallucinations by analyzing their differential knowledge capabilities.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2404.03622</link><description>https://arxiv.org/abs/2404.03622&lt;br&gt;This paper presents the Fast and Slow Generating (FS-GEN) framework for collaborative decoding between large and small language models, highlighting their interactions and effectiveness in overcoming challenges like inference latency and hallucinations by categorizing them based on dual-process cognitive theory.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>A Bi-consolidating Model for Joint Relational Triple Extraction</title><link>https://arxiv.org/abs/2404.03881</link><description>https://arxiv.org/abs/2404.03881&lt;br&gt;This paper introduces the Fast and Slow Generating (FS-GEN) framework, which analyzes collaborative decoding strategies between large and small language models to address challenges like inference latency and hallucinations, revealing that only a small fraction of interactions are needed for effective collaboration.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Annotator-Centric Active Learning for Subjective NLP Tasks</title><link>https://arxiv.org/abs/2404.15720</link><description>https://arxiv.org/abs/2404.15720&lt;br&gt;The paper proposes the Fast and Slow Generating (FS-GEN) framework for collaborative decoding between large and small language models to address challenges in inference latency and hallucination generation, drawing on dual-process cognitive theory and exploring the effectiveness of such collaborations under various conditions.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Does Generative AI speak Nigerian-Pidgin?: Issues about Representativeness and Bias for Multilingualism in LLMs</title><link>https://arxiv.org/abs/2404.19442</link><description>https://arxiv.org/abs/2404.19442&lt;br&gt;The paper introduces a unified framework called Fast and Slow Generating (FS-GEN) that analyzes collaborative decoding between large language models (LLMs) and small language models (SLMs), informed by dual-process cognitive theory, to optimize performance by delineating their distinct roles in generating responses.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Token-wise Influential Training Data Retrieval for Large Language Models</title><link>https://arxiv.org/abs/2405.11724</link><description>https://arxiv.org/abs/2405.11724&lt;br&gt;The paper introduces the Fast and Slow Generating (FS-GEN) framework, which analyzes collaborative decoding between large and small language models to address challenges like high inference latency and hallucinations, highlighting the effective interaction needed between the two systems.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs</title><link>https://arxiv.org/abs/2406.10216</link><description>https://arxiv.org/abs/2406.10216&lt;br&gt;This paper presents the Fast and Slow Generating (FS-GEN) framework, which explores the collaborative decoding between large and small language models to mitigate issues like inference latency and hallucinations by drawing on dual-process cognitive theory to classify their interactions as either methodical or intuitive.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Fast and Slow Generating: An Empirical Study on Large and Small Language Models Collaborative Decoding</title><link>https://arxiv.org/abs/2406.12295</link><description>https://arxiv.org/abs/2406.12295&lt;br&gt;The paper proposes a unified framework called Fast and Slow Generating (FS-GEN) to analyze the collaborative decoding between large and small language models, inspired by dual-process cognitive theory, wherein LLMs are seen as System 2 (slow and deliberate) and independent small language models as System 1 (fast and intuitive), assessing their interactions and the conditions for effective collaboration.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs</title><link>https://arxiv.org/abs/2406.14282</link><description>https://arxiv.org/abs/2406.14282&lt;br&gt;The paper discusses the unique security challenges posed by Generative AI as it becomes more prevalent across various industries and outlines potential research directions to manage these risks.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics</title><link>https://arxiv.org/abs/2406.14703</link><description>https://arxiv.org/abs/2406.14703&lt;br&gt;This paper explores the security challenges posed by Generative AI as its use expands across various industries, suggesting potential research directions for mitigating these risks.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>AlleNoise: large-scale text classification benchmark dataset with real-world label noise</title><link>https://arxiv.org/abs/2407.10992</link><description>https://arxiv.org/abs/2407.10992&lt;br&gt;This paper discusses the unique security challenges associated with the rise of Generative AI in various industries and suggests potential research avenues to mitigate these risks.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>NutriBench: A Dataset for Evaluating Large Language Models in Carbohydrate Estimation from Meal Descriptions</title><link>https://arxiv.org/abs/2407.12843</link><description>https://arxiv.org/abs/2407.12843&lt;br&gt;This paper addresses the unique security challenges posed by Generative AI and outlines potential research directions for managing these risks in various industries.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Task Prompt Vectors: Effective Initialization through Multi-Task Soft-Prompt Transfer</title><link>https://arxiv.org/abs/2408.01119</link><description>https://arxiv.org/abs/2408.01119&lt;br&gt;The paper explores the security challenges associated with the rise of Generative AI across various industries and suggests potential research directions for addressing these risks.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Can Language Models Induce Grammatical Knowledge from Indirect Evidence?</title><link>https://arxiv.org/abs/2410.06022</link><description>https://arxiv.org/abs/2410.06022&lt;br&gt;This paper explores the unique security challenges introduced by Generative AI across various industries and suggests potential research directions to manage these risks.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems</title><link>https://arxiv.org/abs/2410.13334</link><description>https://arxiv.org/abs/2410.13334&lt;br&gt;This paper explores the security challenges associated with Generative AI and suggests potential research directions to mitigate these risks as its usage expands across various industries.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>On the Diversity of Synthetic Data and its Impact on Training Large Language Models</title><link>https://arxiv.org/abs/2410.15226</link><description>https://arxiv.org/abs/2410.15226&lt;br&gt;This paper discusses the security challenges presented by Generative AI across various industries and suggests potential research directions for addressing these risks.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Do Large Language Models Have an English Accent? Evaluating and Improving the Naturalness of Multilingual LLMs</title><link>https://arxiv.org/abs/2410.15956</link><description>https://arxiv.org/abs/2410.15956&lt;br&gt;This paper discusses the unique security challenges associated with Generative AI and proposes potential research directions to address these risks.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S</title><link>https://arxiv.org/abs/2410.16451</link><description>https://arxiv.org/abs/2410.16451&lt;br&gt;This paper explores the unique security challenges presented by Generative AI across various industries and suggests potential research directions to manage these risks.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Generative AI Security: Challenges and Countermeasures</title><link>https://arxiv.org/abs/2402.12617</link><description>https://arxiv.org/abs/2402.12617&lt;br&gt;The paper discusses the unique security challenges posed by Generative AI as it becomes more prevalent across industries and outlines potential research directions for addressing these risks.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Learning to Poison Large Language Models During Instruction Tuning</title><link>https://arxiv.org/abs/2402.13459</link><description>https://arxiv.org/abs/2402.13459&lt;br&gt;CPE-Pro is a structure-sensitive supervised deep learning model designed to accurately represent and evaluate the origins of protein structures, distinguishing between experimentally resolved and computationally predicted structures while enhancing protein feature learning through a newly developed 'structure-sequence'.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>From Keywords to Structured Summaries: Streamlining Scholarly Information Access</title><link>https://arxiv.org/abs/2402.14622</link><description>https://arxiv.org/abs/2402.14622&lt;br&gt;CPE-Pro is a deep learning model designed to accurately evaluate and differentiate the origins of protein structures by learning structural information and inter-structural differences, thus enhancing the reliability of protein structure predictions.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Few-Shot Adversarial Prompt Learning on Vision-Language Models</title><link>https://arxiv.org/abs/2403.14774</link><description>https://arxiv.org/abs/2403.14774&lt;br&gt;CPE-Pro is a structure-sensitive deep learning model designed to represent and evaluate the origins of protein structures, leveraging structural information to enhance discrimination between experimentally resolved and computationally predicted structures.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach</title><link>https://arxiv.org/abs/2404.15993</link><description>https://arxiv.org/abs/2404.15993&lt;br&gt;CPE-Pro is a structure-sensitive supervised deep learning model designed to represent and evaluate the origin of protein structures, enhancing the traceability and reliability of protein structure predictions by utilizing structural sequence information.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Characterizing the Accuracy -- Efficiency Trade-off of Low-rank Decomposition in Language Models</title><link>https://arxiv.org/abs/2405.06626</link><description>https://arxiv.org/abs/2405.06626&lt;br&gt;CPE-Pro is a structure-sensitive deep learning method developed for accurately representing and evaluating the origin of protein structures, enhancing the distinction between experimentally resolved and computationally predicted structures through a supervised approach.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language</title><link>https://arxiv.org/abs/2405.12856</link><description>https://arxiv.org/abs/2405.12856&lt;br&gt;CPE-Pro is a structure-sensitive deep learning model designed to evaluate and represent protein structures, distinguishing between experimentally resolved and computationally predicted structures to enhance the understanding of their functions and interactions.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning</title><link>https://arxiv.org/abs/2406.05804</link><description>https://arxiv.org/abs/2406.05804&lt;br&gt;CPE-Pro introduces a structure-sensitive deep learning model for representing and evaluating protein structures, enhancing the discrimination of their origins and improving representation through the use of structural sequences.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>STAR: SocioTechnical Approach to Red Teaming Language Models</title><link>https://arxiv.org/abs/2406.11757</link><description>https://arxiv.org/abs/2406.11757&lt;br&gt;CPE-Pro is a structure-sensitive deep learning model designed to accurately represent and evaluate the origins of protein structures, improving upon existing methods by capturing inter-structural differences and incorporating structural information into its training.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>CDQuant: Greedy Coordinate Descent for Accurate LLM Quantization</title><link>https://arxiv.org/abs/2406.17542</link><description>https://arxiv.org/abs/2406.17542&lt;br&gt;CPE-Pro is a structure-sensitive deep learning model designed to accurately represent and evaluate the origin of protein structures, distinguishing between experimentally resolved and computationally predicted structures by learning structural information and inter-structural differences.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>I've Got 99 Problems But FLOPS Ain't One</title><link>https://arxiv.org/abs/2407.12819</link><description>https://arxiv.org/abs/2407.12819&lt;br&gt;CPE-Pro is a structure-sensitive deep learning model designed to accurately represent and discriminate the origins of protein structures, enhancing structural representations by utilizing 'structure-sequences' for improved learning of protein features.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning</title><link>https://arxiv.org/abs/2407.15762</link><description>https://arxiv.org/abs/2407.15762&lt;br&gt;CPE-Pro is a structure-sensitive supervised deep learning model designed to evaluate and represent protein structures by discriminating their origins, improving the reliability of protein structure predictions and enhancing structural representations through 'structure-sequences'.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning</title><link>https://arxiv.org/abs/2409.17270</link><description>https://arxiv.org/abs/2409.17270&lt;br&gt;CPE-Pro is a structure-sensitive deep learning model designed to represent and evaluate the origin of protein structures, utilizing structure-sequences to enhance the learning of informative protein features compared to traditional language models focused on amino acid sequences.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>FLAG: Financial Long Document Classification via AMR-based GNN</title><link>https://arxiv.org/abs/2410.02024</link><description>https://arxiv.org/abs/2410.02024&lt;br&gt;CPE-Pro is a deep learning method designed to accurately evaluate and represent protein structures, distinguishing between experimentally resolved and computationally predicted ones through a structure-sensitive approach that enhances the learning of informative features from protein structures.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>TSDS: Data Selection for Task-Specific Model Finetuning</title><link>https://arxiv.org/abs/2410.11303</link><description>https://arxiv.org/abs/2410.11303&lt;br&gt;CPE-Pro is a structure-sensitive deep learning model designed to accurately represent and evaluate the origins of protein structures, distinguishing between experimentally resolved and computationally predicted structures, thereby enhancing reliability in biological studies.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>LocoMotion: Learning Motion-Focused Video-Language Representations</title><link>https://arxiv.org/abs/2410.12018</link><description>https://arxiv.org/abs/2410.12018&lt;br&gt;CPE-Pro is a structure-sensitive deep learning model designed to accurately represent and evaluate the origins of protein structures, enhancing the differentiation between experimentally resolved and computationally predicted structures and capturing inter-structural differences for improved traceability.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>BrainTransformers: SNN-LLM</title><link>https://arxiv.org/abs/2410.14687</link><description>https://arxiv.org/abs/2410.14687&lt;br&gt;CPE-Pro is a structure-sensitive deep learning model designed to accurately represent and evaluate the origin of protein structures, distinguishing between experimentally resolved and computationally predicted structures through enhanced structural information encoding.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>How to Evaluate Reward Models for RLHF</title><link>https://arxiv.org/abs/2410.14872</link><description>https://arxiv.org/abs/2410.14872&lt;br&gt;CPE-Pro is a structure-sensitive deep learning model designed to represent and evaluate the origin of protein structures, enhancing the accuracy of differentiating between experimentally resolved and computationally predicted structures using a new 'structure-sequence' approach.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Do Large Language Models Truly Grasp Mathematics? An Empirical Exploration</title><link>https://arxiv.org/abs/2410.14979</link><description>https://arxiv.org/abs/2410.14979&lt;br&gt;CPE-Pro presents a structure-sensitive deep learning model for evaluating the origins of protein structures, enhancing the representation of structural information and improving the traceability of experimentally resolved versus computationally predicted structures.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>OpenMU: Your Swiss Army Knife for Music Understanding</title><link>https://arxiv.org/abs/2410.15573</link><description>https://arxiv.org/abs/2410.15573&lt;br&gt;CPE-Pro is a structure-sensitive deep learning model designed to accurately represent and evaluate the origins of protein structures, enhancing the traceability between experimentally resolved and computationally predicted structures.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>CPE-Pro: A Structure-Sensitive Deep Learning Method for Protein Representation and Origin Evaluation</title><link>https://arxiv.org/abs/2410.15592</link><description>https://arxiv.org/abs/2410.15592&lt;br&gt;CPE-Pro introduces a structure-sensitive deep learning model for evaluating and representing protein structures, enabling accurate discrimination between experimentally resolved and computationally predicted structures while enhancing protein feature learning through 'structure-sequences.'</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>LLMScan: Causal Scan for LLM Misbehavior Detection</title><link>https://arxiv.org/abs/2410.16638</link><description>https://arxiv.org/abs/2410.16638&lt;br&gt;This paper presents Predictive-Decoding, a novel method that enhances the planning accuracy of Large Language Models (LLMs) by applying Model Predictive Control to enable non-myopic reasoning and reduce early errors in task performance across various domains such as math and coding.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item><item><title>Non-myopic Generation of Language Model for Reasoning and Planning</title><link>https://arxiv.org/abs/2410.17195</link><description>https://arxiv.org/abs/2410.17195&lt;br&gt;The paper presents Predictive-Decoding, a method that enhances the planning accuracy of Large Language Models by using Model Predictive Control to mitigate early errors and promote non-myopic reasoning in various tasks.</description><pubDate>Thu, 24 Oct 2024 11:34:06 GMT</pubDate></item></channel></rss>