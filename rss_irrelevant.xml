<?xml version="1.0" encoding="iso-8859-1"?>
<rss version="2.0"><channel><title>Irrelevant arXiv Papers</title><link>https://Lybarger.github.io/arxiv_rss/rss_irrelevant.xml</link><description>Irrelevant arXiv Papers</description><lastBuildDate>Mon, 21 Oct 2024 10:10:55 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Boosting LLM Translation Skills without General Ability Loss via Rationale Distillation</title><link>https://arxiv.org/abs/2410.13944</link><description>https://arxiv.org/abs/2410.13944&lt;br /&gt;RaDis (Rationale Distillation) is a novel approach that improves machine translation skills of Large Language Models (LLMs) by using self-generated rationales to prevent catastrophic forgetting, thereby maintaining their general abilities and ensuring safety in training.</description><pubDate>Mon, 21 Oct 2024 10:10:55 GMT</pubDate></item><item><title>From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization</title><link>https://arxiv.org/abs/2410.13961</link><description>https://arxiv.org/abs/2410.13961&lt;br /&gt;This paper investigates hallucinations in large language models (LLMs) during multi-document summarization, revealing that up to 75% of generated content may be fabricated, particularly towards the end of summaries, and highlights the limitations of current methods to reduce these inaccuracies.</description><pubDate>Mon, 21 Oct 2024 10:10:55 GMT</pubDate></item><item><title>Detecting AI-Generated Texts in Cross-Domains</title><link>https://arxiv.org/abs/2410.13966</link><description>https://arxiv.org/abs/2410.13966&lt;br /&gt;This paper presents RoBERTa-Ranker, a modified ranking classifier that improves the detection of AI-generated texts across multiple domains by fine-tuning with minimal labeled data, outperforming existing tools like DetectGPT and GPTZero.</description><pubDate>Mon, 21 Oct 2024 10:10:55 GMT</pubDate></item><item><title>Are LLMs Models of Distributional Semantics? A Case Study on Quantifiers</title><link>https://arxiv.org/abs/2410.13984</link><description>https://arxiv.org/abs/2410.13984&lt;br&gt;This paper explores the alignment between large language models (LLMs) and human judgments in understanding vague and exact quantifiers, suggesting a need to re-evaluate the assumptions of distributional semantics models.</description><pubDate>Mon, 21 Oct 2024 10:10:55 GMT</pubDate></item></channel></rss>