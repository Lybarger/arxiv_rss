<?xml version="1.0" encoding="iso-8859-1"?>
<rss version="2.0"><channel><title>Irrelevant arXiv Papers</title><link>https://Lybarger.github.io/arxiv_rss/rss_irrelevant.xml</link><description>Irrelevant arXiv Papers</description><lastBuildDate>Sat, 19 Oct 2024 11:59:45 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>LLoCO: Learning Long Contexts Offline</title><link>https://arxiv.org/abs/2404.07979</link><description>This paper presents LLoCO, a method designed to improve Large Language Models' (LLMs) ability to process long contexts by utilizing context compression and parameter-efficient finetuning, allowing for up to 128k tokens to be handled efficiently. LLoCO shows significant improvements in both inference speed and accuracy for long-context question-answering tasks, outperforming in-context learning while reducing the computational output.</description><pubDate>Sat, 19 Oct 2024 11:59:45 GMT</pubDate></item></channel></rss>