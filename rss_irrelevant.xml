<?xml version="1.0" encoding="iso-8859-1"?>
<rss version="2.0"><channel><title>Irrelevant arXiv Papers</title><link>https://Lybarger.github.io/arxiv_rss/rss_irrelevant.xml</link><description>Irrelevant arXiv Papers</description><lastBuildDate>Tue, 22 Oct 2024 10:17:07 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Boosting LLM Translation Skills without General Ability Loss via Rationale Distillation</title><link>https://arxiv.org/abs/2410.13944</link><description>https://arxiv.org/abs/2410.13944&lt;br /&gt;The paper presents RaDis (Rationale Distillation), a novel method that enhances the machine translation skills of Large Language Models (LLMs) while preserving their overall general capabilities by replaying self-generated rationales to prevent forgetting during training.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization</title><link>https://arxiv.org/abs/2410.13961</link><description>https://arxiv.org/abs/2410.13961&lt;br /&gt;This study investigates how hallucinations occur in large language models (LLMs) during multi-document summarization, revealing that up to 75% of content generated can be fabricated, especially in the latter parts of summaries, and emphasizes the need for better mitigation strategies against such errors.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Detecting AI-Generated Texts in Cross-Domains</title><link>https://arxiv.org/abs/2410.13966</link><description>https://arxiv.org/abs/2410.13966&lt;br /&gt;The paper presents RoBERTa-Ranker, a modified ranking classifier that enhances the detection of AI-generated texts across different domains by leveraging fine-tuning techniques with limited labeled data, outperforming existing tools like DetectGPT and GPTZero.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Are LLMs Models of Distributional Semantics? A Case Study on Quantifiers</title><link>https://arxiv.org/abs/2410.13984</link><description>https://arxiv.org/abs/2410.13984&lt;br /&gt;This paper evaluates the performance of Large Language Models (LLMs) in capturing distributional semantics, particularly focusing on their ability to understand vague and exact quantifiers, and finds that LLMs align more closely with human judgments on exact quantifiers than previously expected.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>LLMs are Biased Teachers: Evaluating LLM Bias in Personalized Education</title><link>https://arxiv.org/abs/2410.14012</link><description>https://arxiv.org/abs/2410.14012&lt;br /&gt;This paper evaluates biases in large language models (LLMs) used in personalized education, highlighting significant disparities in how educational content is generated for different demographic groups, and proposes two metrics to analyze these biases.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Measuring and Modifying the Readability of English Texts with GPT-4</title><link>https://arxiv.org/abs/2410.14028</link><description>https://arxiv.org/abs/2410.14028&lt;br /&gt;This paper investigates the ability of Large Language Models (LLMs) like GPT-4 to assess and modify the readability of English texts, demonstrating high correlation with human judgments and indicating potential for making texts more accessible, despite some unexplained variance in responses.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Efficient Retrieval of Temporal Event Sequences from Textual Descriptions</title><link>https://arxiv.org/abs/2410.14043</link><description>https://arxiv.org/abs/2410.14043&lt;br /&gt;TPP-LLM-Embedding is a unified model that enhances the retrieval of temporal event sequences from textual descriptions by integrating large language models with temporal point processes, enabling efficient embedding and improved performance over baseline models.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Learning Metadata-Agnostic Representations for Text-to-SQL In-Context Example Selection</title><link>https://arxiv.org/abs/2410.14049</link><description>https://arxiv.org/abs/2410.14049&lt;br /&gt;MARLO is a technique that enables more effective selection of in-context examples for Text-to-SQL tasks by learning metadata-agnostic representations, aligning natural language questions and SQL queries in a shared embedding space to enhance execution accuracy.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Learning Multimodal Cues of Children's Uncertainty</title><link>https://arxiv.org/abs/2410.14050</link><description>https://arxiv.org/abs/2410.14050&lt;br /&gt;This paper introduces a novel dataset for studying nonverbal cues of children's uncertainty and presents a multimodal machine learning model that predicts uncertainty from video clips, highlighting the importance of understanding uncertainty in human-AI collaboration.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs</title><link>https://arxiv.org/abs/2410.14057</link><description>https://arxiv.org/abs/2410.14057&lt;br /&gt;This paper presents XC-Translate, a benchmark for cross-cultural machine translation focusing on culturally-nuanced entity names, and proposes KG-MT, a novel method that integrates multilingual knowledge graphs into translation models using a dense retrieval mechanism, achieving significant performance improvements over existing models.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Be My Donor. Transfer the NLP Datasets Between the Languages Using LLM</title><link>https://arxiv.org/abs/2410.14074</link><description>https://arxiv.org/abs/2410.14074&lt;br /&gt;The paper explores how Large Language Models (LLMs) can be utilized to transfer datasets and annotations between languages, specifically translating the DEFT corpus from English to Russian, to enhance resource availability and expedite data annotation efforts.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Lightweight Multi Aspect Controlled Text Generation Solution For Large Language Models</title><link>https://arxiv.org/abs/2410.14144</link><description>https://arxiv.org/abs/2410.14144&lt;br /&gt;This paper presents a lightweight data augmentation pipeline for Multi-Aspect Controllable Text Generation (MCTG) in Large Language Models (LLMs), addressing biases and correlations in traditional datasets, leading to improved performance and accuracy.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent</title><link>https://arxiv.org/abs/2410.14152</link><description>https://arxiv.org/abs/2410.14152&lt;br /&gt;SRAP-Agent is a framework that utilizes Large Language Models (LLMs) to enhance the simulation and optimization of public scarce resource allocation policies, addressing limitations of traditional methods by incorporating real-world dynamics into economic simulations.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models</title><link>https://arxiv.org/abs/2410.14155</link><description>https://arxiv.org/abs/2410.14155&lt;br /&gt;This paper presents a novel metric called Causal Faithfulness, which measures the consistency of causal attributions in Natural Language Explanations generated by Large Language Models (LLMs), proposing that models with alignment tuning yield more faithful explanations than those tested by traditional methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning</title><link>https://arxiv.org/abs/2410.14157</link><description>https://arxiv.org/abs/2410.14157&lt;br /&gt;The paper presents a novel approach using discrete diffusion models for complex reasoning and planning tasks, which demonstrates significant performance improvements over traditional autoregressive language models in handling challenging subgoals.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Automated Genre-Aware Article Scoring and Feedback Using Large Language Models</title><link>https://arxiv.org/abs/2410.14165</link><description>https://arxiv.org/abs/2410.14165&lt;br /&gt;The paper presents an advanced article scoring system that combines the BERT model and Chat-GPT to provide genre-aware evaluations and personalized feedback on written work, outperforming traditional methods in quality assessments.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with Simple Word-based Counting Problems</title><link>https://arxiv.org/abs/2410.14166</link><description>https://arxiv.org/abs/2410.14166&lt;br /&gt;This paper examines the limitations of Large Language Models (LLMs) in performing simple word-based counting tasks, explores reasons behind their deficiencies, and advocates for leveraging reasoning capabilities to improve performance on such tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems</title><link>https://arxiv.org/abs/2410.14179</link><description>https://arxiv.org/abs/2410.14179&lt;br /&gt;MultiChartQA introduces a benchmark for evaluating Multimodal Large Language Models (MLLMs) on complex multi-chart problems, focusing on various reasoning tasks that reflect real-world applications and highlighting performance gaps compared to human abilities.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>XForecast: Evaluating Natural Language Explanations for Time Series Forecasting</title><link>https://arxiv.org/abs/2410.14180</link><description>https://arxiv.org/abs/2410.14180&lt;br /&gt;XForecast introduces new performance metrics for evaluating natural language explanations in time series forecasting, focusing on their accessibility to laypeople and the correlation of these metrics with human judgments, while also assessing the explanatory capabilities of large language models (LLMs).</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs</title><link>https://arxiv.org/abs/2410.14182</link><description>https://arxiv.org/abs/2410.14182&lt;br /&gt;LabSafety Bench introduces a benchmarking framework to evaluate the reliability of large language models (LLMs) in providing safety guidance for laboratory settings, highlighting their critical errors despite outperforming human participants.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Speciesism in Natural Language Processing Research</title><link>https://arxiv.org/abs/2410.14194</link><description>https://arxiv.org/abs/2410.14194&lt;br /&gt;This study investigates speciesism, or discrimination against nonhuman animals, in Natural Language Processing (NLP) research, revealing its presence among researchers, in datasets, and within NLP models, and discusses potential strategies for mitigation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Supervised Chain of Thought</title><link>https://arxiv.org/abs/2410.14198</link><description>https://arxiv.org/abs/2410.14198&lt;br /&gt;The paper 'Supervised Chain of Thought' demonstrates that task-specific supervision is crucial for optimizing the prompt use in Large Language Models (LLMs) to enhance reasoning performance, overcoming the limitations of a 'one-prompt-for-all' approach.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay Scoring with Rationale Generated by LLMs</title><link>https://arxiv.org/abs/2410.14202</link><description>https://arxiv.org/abs/2410.14202&lt;br /&gt;The paper introduces Rationale-based Multiple Trait Scoring (RMTS), a novel method for automated essay scoring that combines large language models with trait-specific rationale generation to improve the accuracy and reliability of multi-trait essay evaluation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning</title><link>https://arxiv.org/abs/2410.14208</link><description>https://arxiv.org/abs/2410.14208&lt;br /&gt;Montessori-Instruct is a data synthesis framework that improves the generation of training data for language models by aligning synthetic data with the learning preferences of student models, leading to significant improvements in their performance over traditional synthesis methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Few-Shot Joint Multimodal Entity-Relation Extraction via Knowledge-Enhanced Cross-modal Prompt Model</title><link>https://arxiv.org/abs/2410.14225</link><description>https://arxiv.org/abs/2410.14225&lt;br /&gt;The paper presents the Knowledge-Enhanced Cross-modal Prompt Model (KECPM) for Few-Shot Joint Multimodal Entity-Relation Extraction, which generates supplementary background knowledge to improve performance on multimodal data involving text-image pairs, surpassing existing methods in evaluation metrics.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Unveiling Large Language Models Generated Texts: A Multi-Level Fine-Grained Detection Framework</title><link>https://arxiv.org/abs/2410.14231</link><description>https://arxiv.org/abs/2410.14231&lt;br /&gt;The Multi-level Fine-grained Detection (MFD) framework proposed in this research enhances the detection of Large Language Model (LLM)-generated texts by integrating multiple levels of analysis, improving robustness against evasion techniques, and providing a mechanism to address concerns over authorship and originality.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Towards Robust Knowledge Representations in Multilingual LLMs for Equivalence and Inheritance based Consistent Reasoning</title><link>https://arxiv.org/abs/2410.14235</link><description>https://arxiv.org/abs/2410.14235&lt;br /&gt;The paper investigates the limitations of Large Language Models (LLMs) in performing consistent reasoning across languages, introducing new tasks and benchmarks to evaluate their understanding of equivalence and inheritance relationships, and proposing 'Compositional Representations' to enhance multilingual reasoning consistency.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Novel Method to Metigate Demographic and Expert Bias in ICD Coding with Causal Inference</title><link>https://arxiv.org/abs/2410.14236</link><description>https://arxiv.org/abs/2410.14236&lt;br /&gt;The paper presents DECI, a causal inference-based method designed to mitigate demographic and expert biases in ICD coding, demonstrating improved accuracy and reduced spurious correlations compared to existing models.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models</title><link>https://arxiv.org/abs/2410.14248</link><description>https://arxiv.org/abs/2410.14248&lt;br /&gt;This research paper investigates selection bias in Multiple-Choice Question Answering (MCQA) for Video Language Models (VLMs), proposing a calibration technique called BOLD to enhance performance by reducing the impact of bias and improving genuine understanding in evaluations.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>MoDification: Mixture of Depths Made Easy</title><link>https://arxiv.org/abs/2410.14268</link><description>https://arxiv.org/abs/2410.14268&lt;br /&gt;MoDification presents a method for optimizing large language models (LLMs) by leveraging a mixture of depths to significantly enhance efficiency in terms of latency and memory usage, especially for long-context tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>REEF: Representation Encoding Fingerprints for Large Language Models</title><link>https://arxiv.org/abs/2410.14273</link><description>https://arxiv.org/abs/2410.14273&lt;br /&gt;REEF is a training-free method for identifying relationships between Large Language Models by comparing their feature representations via centered kernel alignment similarity, aiming to protect intellectual property without impairing model capabilities.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>EcomEdit: An Automated E-commerce Knowledge Editing Framework for Enhanced Product and Purchase Intention Understanding</title><link>https://arxiv.org/abs/2410.14276</link><description>https://arxiv.org/abs/2410.14276&lt;br /&gt;ECOMEDIT is an automated framework that applies Knowledge Editing to enhance Large Language Models' understanding of product features and customer purchase intentions in the e-commerce sector by enabling automatic conflict detection and improving semantic coverage.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>LoGU: Long-form Generation with Uncertainty Expressions</title><link>https://arxiv.org/abs/2410.14309</link><description>https://arxiv.org/abs/2410.14309&lt;br /&gt;LoGU introduces the task of Long-form Generation with Uncertainty, addressing challenges in allowing Large Language Models (LLMs) to express uncertainty accurately while generating longer responses, improving accuracy and reducing hallucinations through a specialized data collection and training approach.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Efficiently Computing Susceptibility to Context in Language Models</title><link>https://arxiv.org/abs/2410.14361</link><description>https://arxiv.org/abs/2410.14361&lt;br /&gt;The paper introduces Fisher susceptibility, an efficient method for estimating the sensitivity of language models to context changes in user queries, demonstrating its effectiveness compared to traditional Monte Carlo methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>How Do Multilingual Models Remember? Investigating Multilingual Factual Recall Mechanisms</title><link>https://arxiv.org/abs/2410.14387</link><description>https://arxiv.org/abs/2410.14387&lt;br /&gt;This paper investigates the mechanisms of factual recall in multilingual Large Language Models (LLMs) and examines the extent to which findings from English monolingual models apply to multilingual contexts, identifying both language-independent and language-dependent mechanisms.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Analyzing Context Utilization of LLMs in Document-Level Translation</title><link>https://arxiv.org/abs/2410.14391</link><description>https://arxiv.org/abs/2410.14391&lt;br /&gt;This paper examines the context utilization of large language models (LLMs) in document-level translation, revealing that while LLMs show enhanced overall translation capabilities, their performance on pronoun translation does not consistently reflect these improvements, emphasizing the importance of context-aware fine-tuning.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Generative AI, Pragmatics, and Authenticity in Second Language Learning</title><link>https://arxiv.org/abs/2410.14395</link><description>https://arxiv.org/abs/2410.14395&lt;br /&gt;This paper discusses the limitations and challenges of using generative AI in second language learning, highlighting issues of cultural bias and the lack of pragmatics and authenticity in AI-generated language compared to human interaction.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic Reasoning</title><link>https://arxiv.org/abs/2410.14399</link><description>https://arxiv.org/abs/2410.14399&lt;br /&gt;SylloBio-NLI introduces a framework for evaluating Large Language Models on biomedical syllogistic reasoning, revealing challenges in zero-shot performance and improvements with few-shot prompting, while highlighting dependencies on model architecture and pre-training for biomedical applications.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation</title><link>https://arxiv.org/abs/2410.14425</link><description>https://arxiv.org/abs/2410.14425&lt;br /&gt;The paper presents W2SDefense, a weak-to-strong unlearning algorithm that uses feature alignment knowledge distillation to help large language models (LLMs) effectively unlearn backdoor attacks while maintaining their performance on downstream tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference</title><link>https://arxiv.org/abs/2410.14442</link><description>https://arxiv.org/abs/2410.14442&lt;br /&gt;This paper presents a systematic study on cross-layer key-value (KV) sharing techniques for enhancing the efficiency of large language models (LLMs) during inference, demonstrating that reduced KV cache sizes can maintain competitive performance while improving throughput in various configurations.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of Language Models</title><link>https://arxiv.org/abs/2410.14480</link><description>https://arxiv.org/abs/2410.14480&lt;br /&gt;This paper presents a novel hybrid evaluation method for large language models (LLMs) that combines entropy from covariance matrices and the Matrix Nuclear Norm, offering a comprehensive, efficient evaluation framework adaptable to various objectives.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>SignAttention: On the Interpretability of Transformer Models for Sign Language Translation</title><link>https://arxiv.org/abs/2410.14506</link><description>https://arxiv.org/abs/2410.14506&lt;br /&gt;This paper provides an interpretability analysis of a Transformer model for Sign Language Translation, revealing how the model aligns visual inputs with corresponding glosses and text, highlighting the dynamics of attention mechanisms throughout the translation process.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Tell me what I need to know: Exploring LLM-based (Personalized) Abstractive Multi-Source Meeting Summarization</title><link>https://arxiv.org/abs/2410.14545</link><description>https://arxiv.org/abs/2410.14545&lt;br /&gt;This paper presents a multi-source meeting summarization approach using large language models that enhances summary relevance and informativeness by integrating supplementary materials and personalizing outputs based on participant characteristics.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Large Language Models Are Overparameterized Text Encoders</title><link>https://arxiv.org/abs/2410.14578</link><description>https://arxiv.org/abs/2410.14578&lt;br /&gt;This paper demonstrates that large language models (LLMs) can be effectively pruned to reduce memory and inference time with minimal performance impact, leading to the conclusion that LLMs are overparameterized for text embedding tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Dialetto, ma Quanto Dialetto? Transcribing and Evaluating Dialects on a Continuum</title><link>https://arxiv.org/abs/2410.14589</link><description>https://arxiv.org/abs/2410.14589&lt;br /&gt;This research investigates within-dialect variation in Italian dialects by measuring speech-to-text performance and revealing geographical disparities that correlate with linguistic similarity, while suggesting that existing models bias performance towards dialects similar to standard varieties.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and Tool Knowledge Bases</title><link>https://arxiv.org/abs/2410.14594</link><description>https://arxiv.org/abs/2410.14594&lt;br /&gt;Toolshed introduces a tool knowledge base and Advanced RAG-Tool Fusion techniques to enhance the performance and retrieval accuracy of tool-equipped agents by optimizing tool selection without requiring model fine-tuning.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>You Shall Know a Tool by the Traces it Leaves: The Predictability of Sentiment Analysis Tools</title><link>https://arxiv.org/abs/2410.14626</link><description>https://arxiv.org/abs/2410.14626&lt;br /&gt;This paper demonstrates that different sentiment analysis tools yield inconsistent results across various datasets and languages, revealing an algorithmic bias that can be predicted from their outcomes, and emphasizes the necessity for improved evaluation standards in natural language processing (NLP).</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>GenEOL: Harnessing the Generative Power of LLMs for Training-Free Sentence Embeddings</title><link>https://arxiv.org/abs/2410.14635</link><description>https://arxiv.org/abs/2410.14635&lt;br /&gt;GenEOL introduces a novel method for obtaining training-free sentence embeddings by leveraging the generative capabilities of large language models (LLMs) to create diverse transformations of sentences, significantly improving performance in semantic text similarity and related tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs</title><link>https://arxiv.org/abs/2410.14641</link><description>https://arxiv.org/abs/2410.14641&lt;br /&gt;The research presents LongPiBench, a benchmark to evaluate positional bias in large language models (LLMs) when processing multiple relevant information pieces, revealing significant biases related to their spacing and the challenges posed by the 'lost in the middle' phenomenon.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie Character-Aware Discourse Graph</title><link>https://arxiv.org/abs/2410.14666</link><description>https://arxiv.org/abs/2410.14666&lt;br /&gt;DiscoGraMS introduces a novel discourse graph representation for movie screenplays that enhances summarization by capturing complex character interactions and contextual nuances, while addressing limitations of current transformer models.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps</title><link>https://arxiv.org/abs/2410.14668</link><description>https://arxiv.org/abs/2410.14668&lt;br /&gt;MiCEval introduces a framework for evaluating the quality of reasoning chains in Multimodal Chain of Thought (MCoT) among multimodal large language models (MLLMs) by assessing both image descriptions and the correctness of reasoning steps.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment</title><link>https://arxiv.org/abs/2410.14676</link><description>https://arxiv.org/abs/2410.14676&lt;br /&gt;SudoLM introduces a framework that enables Large Language Models (LLMs) to learn access control over parametric knowledge using authorization alignment, allowing qualified users to access specific knowledge while restricting it from non-qualified users.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts</title><link>https://arxiv.org/abs/2410.14677</link><description>https://arxiv.org/abs/2410.14677&lt;br /&gt;This paper surveys the quality of datasets used for training AI detectors of machine-generated texts, questioning the reliability of these detectors and advocating for improved methods in evaluating and enhancing the datasets involved.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Stars, Stripes, and Silicon: Unravelling the ChatGPT's All-American, Monochrome, Cis-centric Bias</title><link>https://arxiv.org/abs/2410.13868</link><description>https://arxiv.org/abs/2410.13868&lt;br /&gt;The paper explores the biases, toxicity, and unreliability of large language models like ChatGPT, attributing these issues to the quality and diversity of training data rather than model architectures, and calls for interdisciplinary collaboration to address these challenges and mitigate societal harm.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Observing the Southern US Culture of Honor Using Large-Scale Social Media Analysis</title><link>https://arxiv.org/abs/2410.13887</link><description>https://arxiv.org/abs/2410.13887&lt;br /&gt;This study examines the 'culture of honor' in the Southern US by analyzing social media interactions, finding that individuals from this region are more likely to retaliate to insults, and utilizes GPT-3.5 for geolocation and insult detection.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>NSmark: Null Space Based Black-box Watermarking Defense Framework for Pre-trained Language Models</title><link>https://arxiv.org/abs/2410.13907</link><description>https://arxiv.org/abs/2410.13907&lt;br /&gt;NSmark is a black-box watermarking defense framework designed for pre-trained language models that resists Linear Functionality Equivalence Attacks by leveraging the invariant properties of the output matrix's null space, ensuring the protection of intellectual property in language models.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Automatically Interpreting Millions of Features in Large Language Models</title><link>https://arxiv.org/abs/2410.13928</link><description>https://arxiv.org/abs/2410.13928&lt;br /&gt;This paper presents an automated pipeline developed to generate and evaluate natural language explanations for features from sparse autoencoders in large language models, employing new techniques to assess explanation quality and enhancing the interpretability of neural network activations.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Identifying High Consideration E-Commerce Search Queries</title><link>https://arxiv.org/abs/2410.13951</link><description>https://arxiv.org/abs/2410.13951&lt;br /&gt;The paper introduces an Engagement-based Query Ranking (EQR) approach to identify High Consideration e-commerce search queries, enabling better user experiences and improved engagement through targeted search features.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Debiasing Large Vision-Language Models by Ablating Protected Attribute Representations</title><link>https://arxiv.org/abs/2410.13976</link><description>https://arxiv.org/abs/2410.13976&lt;br /&gt;This research introduces a debiasing framework for Large Vision-Language Models (LVLMs) that aims to reduce societal biases in text generation by ablating biased attribute representations without requiring additional training.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Graph Neural Flows for Unveiling Systemic Interactions Among Irregularly Sampled Time Series</title><link>https://arxiv.org/abs/2410.14030</link><description>https://arxiv.org/abs/2410.14030&lt;br /&gt;This paper presents a graph-based model called graph neural flows that effectively captures systemic interactions among irregularly sampled time series by learning conditional dependencies and improving prediction accuracy compared to traditional methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models</title><link>https://arxiv.org/abs/2410.14059</link><description>https://arxiv.org/abs/2410.14059&lt;br /&gt;The UCFE benchmark is a user-centric framework for evaluating large language models' ability to manage complex financial tasks, combining human expert assessments with dynamic interactions to simulate real-world scenarios, and demonstrating alignment with user preferences.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers</title><link>https://arxiv.org/abs/2410.14072</link><description>https://arxiv.org/abs/2410.14072&lt;br /&gt;The paper introduces 'Victor', a method to enhance the efficiency of vision-language models by summarizing visual tokens into a compact set of register tokens, significantly improving computational efficiency while maintaining model performance.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>ViConsFormer: Constituting Meaningful Phrases of Scene Texts using Transformer-based Method in Vietnamese Text-based Visual Question Answering</title><link>https://arxiv.org/abs/2410.14132</link><description>https://arxiv.org/abs/2410.14132&lt;br /&gt;ViConsFormer is a novel transformer-based method for Vietnamese Text-based Visual Question Answering (VQA) that effectively utilizes the meaning extracted from scene texts in images to improve answer accuracy, achieving state-of-the-art results on two large-scale datasets.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Coherence-Driven Multimodal Safety Dialogue with Active Learning for Embodied Agents</title><link>https://arxiv.org/abs/2410.14141</link><description>https://arxiv.org/abs/2410.14141&lt;br /&gt;M-CoDAL is a multimodal dialogue system for embodied agents that enhances safety communication in critical situations by interpreting visual cues and leveraging discourse coherence relations, evaluated through a user study with safety scenarios.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment</title><link>https://arxiv.org/abs/2410.14148</link><description>https://arxiv.org/abs/2410.14148&lt;br /&gt;This paper introduces FiSAO, a self-alignment method that uses a model's own visual encoder as a fine-grained verifier to enhance alignment between vision and language in Vision-Language Large Models (VLLMs), addressing misalignment issues without requiring additional data.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Utilizing Large Language Models for Event Deconstruction to Enhance Multimodal Aspect-Based Sentiment Analysis</title><link>https://arxiv.org/abs/2410.14150</link><description>https://arxiv.org/abs/2410.14150&lt;br /&gt;This paper presents a novel framework, MABSA-RL, that utilizes Large Language Models (LLMs) for event decomposition to improve Multimodal Aspect-Based Sentiment Analysis (MABSA) by simplifying the complexity of analysis through reinforcement learning optimization.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation</title><link>https://arxiv.org/abs/2410.14251</link><description>https://arxiv.org/abs/2410.14251&lt;br /&gt;The paper presents MATRIX, a multi-agent simulation framework that generates diverse text-based scenarios for post-training data synthesis, enabling LLMs to better follow human instructions, and demonstrates its effectiveness through superior performance on various benchmarks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Good Parenting is all you need -- Multi-agentic LLM Hallucination Mitigation</title><link>https://arxiv.org/abs/2410.14262</link><description>https://arxiv.org/abs/2410.14262&lt;br /&gt;This study demonstrates that multi-agent setups using advanced Large Language Models (LLMs) can effectively detect and correct hallucinations in AI-generated content, achieving high accuracy in identifying inaccuracies and revising outputs.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Fine-Tuning Pre-trained Language Models for Robust Causal Representation Learning</title><link>https://arxiv.org/abs/2410.14375</link><description>https://arxiv.org/abs/2410.14375&lt;br /&gt;The research investigates how fine-tuning pre-trained language models can enhance their generalizability in single-domain scenarios by utilizing causal mechanisms to mitigate the effect of spurious features, demonstrating superior performance in both synthetic and real-world settings.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts</title><link>https://arxiv.org/abs/2410.14574</link><description>https://arxiv.org/abs/2410.14574&lt;br /&gt;MomentumSMoE introduces a new framework for Sparse Mixture of Experts by integrating momentum to enhance stability and robustness during training, demonstrating improved performance across various practical tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Optimizing Attention with Mirror Descent: Generalized Max-Margin Token Selection</title><link>https://arxiv.org/abs/2410.14581</link><description>https://arxiv.org/abs/2410.14581&lt;br /&gt;This paper explores the optimization dynamics and convergence properties of mirror descent algorithms for softmax attention mechanisms, demonstrating their effectiveness in token selection and generalization in classification tasks compared to traditional gradient descent methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Do LLMs estimate uncertainty well in instruction-following?</title><link>https://arxiv.org/abs/2410.14582</link><description>https://arxiv.org/abs/2410.14582&lt;br /&gt;This paper systematically evaluates the ability of large language models (LLMs) to estimate uncertainty while following instructions and identifies challenges in existing benchmarks, aiming to improve the reliability of LLMs in high-stakes applications.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>CELI: Controller-Embedded Language Model Interactions</title><link>https://arxiv.org/abs/2410.14627</link><description>https://arxiv.org/abs/2410.14627&lt;br /&gt;CELI introduces a framework that embeds control logic within language model prompts to enhance complex task execution and dynamic adaptation, leading to significant performance improvements in code generation and content creation tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples</title><link>https://arxiv.org/abs/2410.14669</link><description>https://arxiv.org/abs/2410.14669&lt;br /&gt;NaturalBench is a new benchmark for evaluating vision-language models (VLMs) using 10,000 human-verified visual-question-answering samples that highlight the models' struggles with natural images and complex reasoning, exposing significant biases and performance gaps compared to human understanding.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Improving Word Translation via Two-Stage Contrastive Learning</title><link>https://arxiv.org/abs/2203.08307</link><description>https://arxiv.org/abs/2203.08307&lt;br /&gt;This paper introduces a two-stage contrastive learning framework for improving bilingual lexicon induction (BLI), enhancing word translation by refining cross-lingual linear maps and fine-tuning mBERT to achieve substantial performance gains across multiple language pairs.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Improving Bilingual Lexicon Induction with Cross-Encoder Reranking</title><link>https://arxiv.org/abs/2210.16953</link><description>https://arxiv.org/abs/2210.16953&lt;br /&gt;The paper presents BLICEr, a novel semi-supervised post-hoc reranking method that enhances bilingual lexicon induction by combining cross-lingual word embeddings with semantic similarity scores from a fine-tuned multilingual pre-trained language model.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech Recognition</title><link>https://arxiv.org/abs/2302.08102</link><description>https://arxiv.org/abs/2302.08102&lt;br /&gt;The paper presents a method for improving speaker-adaptive Visual Speech Recognition (VSR) by using prompt tuning of Deep Neural Networks (DNNs), allowing the model to better adapt to unseen speakers with minimal adaptation data.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>I run as fast as a rabbit, can you? A Multilingual Simile Dialogue Dataset</title><link>https://arxiv.org/abs/2306.05672</link><description>https://arxiv.org/abs/2306.05672&lt;br /&gt;The paper introduces the Multilingual Simile Dialogue (MSD) dataset, the largest manually annotated simile data that facilitates the study of complex simile phenomena in dialogues, featuring tasks for simile recognition, interpretation, generation, and dialogue generation using English and Chinese data.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models</title><link>https://arxiv.org/abs/2306.15087</link><description>https://arxiv.org/abs/2306.15087&lt;br /&gt;WinoQueer is a benchmark created to assess and quantify the anti-LGBTQ+ bias present in large language models (LLMs), utilizing community input to generate the benchmark and demonstrating that bias can be mitigated by fine-tuning with data produced by community members.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Entity Matching using Large Language Models</title><link>https://arxiv.org/abs/2310.11244</link><description>https://arxiv.org/abs/2310.11244&lt;br /&gt;This paper explores the use of generative large language models (LLMs) for entity matching, demonstrating their reduced dependency on task-specific training data and increased robustness compared to traditional pre-trained language models (PLMs).</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>"We Demand Justice!": Towards Social Context Grounding of Political Texts</title><link>https://arxiv.org/abs/2311.09106</link><description>https://arxiv.org/abs/2311.09106&lt;br /&gt;This paper explores the social context necessary for understanding ambiguous political discourse on social media, proposing datasets to benchmark large pre-trained models and structured models in their capacity for contextual and pragmatic language understanding.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models</title><link>https://arxiv.org/abs/2402.13606</link><description>https://arxiv.org/abs/2402.13606&lt;br /&gt;This paper presents a comprehensive study of multilingual confidence estimation in Large Language Models (LLMs), highlighting the performance disparities across languages and introducing a native-tone prompting strategy to enhance reliability and accuracy on language-specific tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Framing in the Presence of Supporting Data: A Case Study in U.S. Economic News</title><link>https://arxiv.org/abs/2402.14224</link><description>https://arxiv.org/abs/2402.14224&lt;br /&gt;The paper presents a computational framework for analyzing editorial choices in U.S. economic news by framing prediction with objective measures derived from economic indicators, allowing for evaluation of how publications select and frame economic information from 2015 to 2023.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis</title><link>https://arxiv.org/abs/2403.01976</link><description>https://arxiv.org/abs/2403.01976&lt;br /&gt;SciAssess is a benchmarking framework that evaluates the proficiency of Large Language Models (LLMs) in analyzing scientific literature across various domains, focusing on their Memorization, Comprehension, and Analysis &amp; Reasoning capabilities.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>MaiBaam Annotation Guidelines</title><link>https://arxiv.org/abs/2403.05902</link><description>https://arxiv.org/abs/2403.05902&lt;br /&gt;The MaiBaam Annotation Guidelines document outlines the processes and principles for annotating the Bavarian corpus with part-of-speech tags and syntactic dependencies, contributing to the Universal Dependencies project.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>One size doesn't fit all: Predicting the Number of Examples for In-Context Learning</title><link>https://arxiv.org/abs/2403.06402</link><description>https://arxiv.org/abs/2403.06402&lt;br /&gt;The study introduces a dynamic approach to In-Context Learning (ICL) by predicting the optimal number of examples to use for each data instance in few-shot inference with Large Language Models (LLMs), leading to improved performance compared to standard ICL methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Efficiently Quantifying and Mitigating Ripple Effects in Model Editing</title><link>https://arxiv.org/abs/2403.07825</link><description>https://arxiv.org/abs/2403.07825&lt;br /&gt;This paper presents Graphical Impact Evaluation (GIE) and Selective Impact Revision (SIR) as methodologies to quantify and mitigate the ripple effects in model editing for Large Language Models, which can adversely affect model performance and editing tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Train &amp; Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases</title><link>https://arxiv.org/abs/2403.13901</link><description>https://arxiv.org/abs/2403.13901&lt;br /&gt;The paper introduces TwisterLister, a pipeline for generating English tongue twisters using phonologically informed methods, which utilizes a phonologically constrained vocabulary and large language models to create the largest annotated dataset of tongue twisters while maintaining semantic consistency and grammatical correctness.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Multi-Conditional Ranking with Large Language Models</title><link>https://arxiv.org/abs/2404.00211</link><description>https://arxiv.org/abs/2404.00211&lt;br /&gt;This paper introduces MCRank, a benchmark for evaluating large language models (LLMs) in the task of multi-conditional ranking, and presents a decomposed reasoning method (EXSIR) that significantly improves LLM performance on complex ranking tasks involving diverse conditions.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge</title><link>https://arxiv.org/abs/2404.06833</link><description>https://arxiv.org/abs/2404.06833&lt;br /&gt;This research investigates the cultural biases present in Large Language Models (LLMs) concerning food-related knowledge, introducing the FmLAMA dataset to analyze LLM performance in both monolingual and multilingual settings, highlighting the influence of cultural context on LLMs' ability to access and accurately represent food-related cultural facts.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>What's under the hood: Investigating Automatic Metrics on Meeting Summarization</title><link>https://arxiv.org/abs/2404.11124</link><description>https://arxiv.org/abs/2404.11124&lt;br /&gt;This paper investigates the effectiveness of automatic metrics for evaluating meeting summarization, identifying their shortcomings in capturing meeting-specific errors and correlating these metrics with human evaluations across a detailed error taxonomy, revealing that existing metrics often mask important summarization errors.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding</title><link>https://arxiv.org/abs/2404.16710</link><description>https://arxiv.org/abs/2404.16710&lt;br /&gt;LayerSkip is an end-to-end solution designed to accelerate inference in large language models (LLMs) by employing layer dropout during training and a self-speculative decoding approach that allows for early exit inference, resulting in significant speedups without additional model complexity.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Everything is Editable: Extend Knowledge Editing to Unstructured Data in Large Language Models</title><link>https://arxiv.org/abs/2405.15349</link><description>https://arxiv.org/abs/2405.15349&lt;br /&gt;The paper proposes UnKE, a novel method for editing unstructured knowledge in large language models by introducing non-local block key-value storage and cause-driven optimization to improve representation and context preservation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations</title><link>https://arxiv.org/abs/2405.19740</link><description>https://arxiv.org/abs/2405.19740&lt;br /&gt;PertEval is a toolkit designed to assess the knowledge capacity of large language models (LLMs) through knowledge-invariant perturbations, revealing significant overestimations in LLM performance on traditional benchmarks and providing insights for advancing their knowledge mastery.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Fundamental Trade-off in Aligned Language Models and its Relation to Sampling Adaptors</title><link>https://arxiv.org/abs/2406.10203</link><description>https://arxiv.org/abs/2406.10203&lt;br /&gt;This paper explores the trade-off between quality and probability in aligned language models, particularly focusing on how sampling adaptors can influence the balance between average reward and log-likelihood when samples are drawn from a model aligned to human preferences through reinforcement learning.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation</title><link>https://arxiv.org/abs/2406.11580</link><description>https://arxiv.org/abs/2406.11580&lt;br /&gt;Error Span Annotation (ESA) is a new human evaluation protocol for machine translation that balances the efficiency of Direct Assessment with the detailed error classification of Multidimensional Quality Metrics, offering quicker and more cost-effective evaluations without needing expert involvement.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>LLM Critics Help Catch Bugs in Mathematics: Towards a Better Mathematical Verifier with Natural Language Feedback</title><link>https://arxiv.org/abs/2406.14024</link><description>https://arxiv.org/abs/2406.14024&lt;br /&gt;The paper presents Math-Minos, a natural language feedback-enhanced mathematical verifier that improves assessment accuracy of solutions by using step-wise natural language feedback instead of binary classification labels.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>MACAROON: Training Vision-Language Models To Be Your Engaged Partners</title><link>https://arxiv.org/abs/2406.14137</link><description>https://arxiv.org/abs/2406.14137&lt;br /&gt;MACAROON introduces a framework that enhances the proactive engagement capabilities of large vision-language models (LVLMs) by enabling them to autonomously generate contrastive response pairs to improve their interaction with ambiguous or personalizable questions.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Evaluating Contextualized Representations of (Spanish) Ambiguous Words: A New Lexical Resource and Empirical Analysis</title><link>https://arxiv.org/abs/2406.14678</link><description>https://arxiv.org/abs/2406.14678&lt;br /&gt;This paper evaluates how different BERT-based language models represent ambiguous Spanish nouns in context, providing a novel dataset for understanding their capacity to reflect human semantic judgments, and analyzing the influence of model architecture and size on performance.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>"Vorbe\c{s}ti Rom\^ane\c{s}te?" A Recipe to Train Powerful Romanian LLMs with English Instructions</title><link>https://arxiv.org/abs/2406.18266</link><description>https://arxiv.org/abs/2406.18266&lt;br /&gt;This paper presents the development and training of Romanian Large Language Models (RoLLMs) using a large collection of translated texts and benchmarks to achieve high performance and state-of-the-art results, aimed at improving LLM capabilities for Romanian and supporting research on low-resourced languages.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Crossroads of Continents: Automated Artifact Extraction for Cultural Adaptation with Large Multimodal Models</title><link>https://arxiv.org/abs/2407.02067</link><description>https://arxiv.org/abs/2407.02067&lt;br /&gt;The paper explores cultural understanding in Large Multimodal Models (LMMs) through a study involving a dataset of images and cultural artifact extraction, revealing disparities in cultural understanding and emphasizing the importance of culture-aware systems.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for data pruning in LLM Training</title><link>https://arxiv.org/abs/2408.05541</link><description>https://arxiv.org/abs/2408.05541&lt;br /&gt;P3 is an adaptive framework for fine-tuning Large Language Models (LLMs) that optimizes data pruning through policy-driven difficulty measurement, pace-adaptive selection, and diversity promotion, enhancing model performance on reasoning tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding</title><link>https://arxiv.org/abs/2409.03258</link><description>https://arxiv.org/abs/2409.03258&lt;br /&gt;GraphInsight is a framework designed to enhance Large Language Models' comprehension of graph structures by addressing positional biases in memory performance and leveraging an external knowledge base for improved understanding and multi-step reasoning in graph tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>With Ears to See and Eyes to Hear: Sound Symbolism Experiments with Multimodal Large Language Models</title><link>https://arxiv.org/abs/2409.14917</link><description>https://arxiv.org/abs/2409.14917&lt;br /&gt;The study investigates the ability of Vision Language Models (VLMs) and Large Language Models (LLMs) to understand sound symbolism through experiments that explore the connection between sounds and concepts by using visual and textual information.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>English offensive text detection using CNN based Bi-GRU model</title><link>https://arxiv.org/abs/2409.15652</link><description>https://arxiv.org/abs/2409.15652&lt;br /&gt;This paper presents a novel Bi-GRU-CNN model for automating the detection of offensive text on social media platforms, improving the classification of inappropriate content compared to existing models.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Modular-based Strategy for Mitigating Gradient Conflicts in Simultaneous Speech Translation</title><link>https://arxiv.org/abs/2409.15911</link><description>https://arxiv.org/abs/2409.15911&lt;br /&gt;The study introduces a Modular Gradient Conflict Mitigation (MGCM) strategy that detects and resolves optimization conflicts in Simultaneous Speech Translation (SimulST), leading to improved performance and significantly reduced GPU memory consumption.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding for Large Language Models</title><link>https://arxiv.org/abs/2409.19700</link><description>https://arxiv.org/abs/2409.19700&lt;br /&gt;The paper introduces 2D-TPE, a Two-Dimensional Positional Encoding method designed to enhance the ability of large language models (LLMs) in understanding table structures by preserving spatial relationships and improving performance on tabular data tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>ExACT: Teaching AI Agents to Explore with Reflective-MCTS and Exploratory Learning</title><link>https://arxiv.org/abs/2410.02052</link><description>https://arxiv.org/abs/2410.02052&lt;br /&gt;ExACT presents a novel approach that combines Reflective Monte Carlo Tree Search and Exploratory Learning to enhance the decision-making capabilities of autonomous agents, particularly in complex multistep tasks, achieving significant performance improvements in various applications.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Contextual Document Embeddings</title><link>https://arxiv.org/abs/2410.02525</link><description>https://arxiv.org/abs/2410.02525&lt;br /&gt;This paper introduces two methods for creating contextualized document embeddings that enhance neural retrieval by considering both the target document and its neighboring documents, leading to improved performance over traditional biencoders.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Hyper-multi-step: The Truth Behind Difficult Long-context Tasks</title><link>https://arxiv.org/abs/2410.04422</link><description>https://arxiv.org/abs/2410.04422&lt;br /&gt;The paper investigates the challenges faced by Long-context Language Models (LCLMs) in completing difficult long-context tasks, identifying that the challenges primarily arise from 'multi-matching retrieval' and 'logic-based retrieval' requirements that exceed LCLMs' capabilities due to their hyper-multi-step nature.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>$\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization</title><link>https://arxiv.org/abs/2410.04717</link><description>https://arxiv.org/abs/2410.04717&lt;br /&gt;The paper investigates how the diversity of training instructions across various semantic domains affects the generalization abilities of large language models (LLMs) and provides guidelines for effective dataset collection for instruction-tuning.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing</title><link>https://arxiv.org/abs/2410.06331</link><description>https://arxiv.org/abs/2410.06331&lt;br /&gt;The paper presents IFMET, a novel locate-then-edit approach for knowledge editing in Large Language Models (LLMs), which enhances multi-hop factual recall by modifying both shallow and deep MLP layers to overcome limitations of previous methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Advocating Character Error Rate for Multilingual ASR Evaluation</title><link>https://arxiv.org/abs/2410.07400</link><description>https://arxiv.org/abs/2410.07400&lt;br /&gt;The paper advocates for the use of character error rate (CER) as the primary evaluation metric for multilingual automatic speech recognition (ASR) systems, highlighting its advantages over the traditional word error rate (WER) in accounting for the complexities of different languages.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>GUS-Net: Social Bias Classification in Text with Generalizations, Unfairness, and Stereotypes</title><link>https://arxiv.org/abs/2410.08388</link><description>https://arxiv.org/abs/2410.08388&lt;br /&gt;GUS-Net is a novel approach for detecting social biases in text, focusing on generalizations, unfairness, and stereotypes, utilizing generative AI and automated agents to create a synthetic dataset for improved multi-label token classification.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective</title><link>https://arxiv.org/abs/2410.10291</link><description>https://arxiv.org/abs/2410.10291&lt;br /&gt;The paper introduces a novel metric and benchmark for evaluating the causal relationship between semantic variations in linguistic inputs and outputs in text-to-image synthesis, highlighting the limitations of existing evaluation methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>FAME: Towards Factual Multi-Task Model Editing</title><link>https://arxiv.org/abs/2410.10859</link><description>https://arxiv.org/abs/2410.10859&lt;br /&gt;FAME introduces a factual, comprehensive, and multi-task dataset for model editing in large language models, along with SKEME, a novel editing method that employs a caching mechanism to improve synchronization with real-world information and enhance LLM capabilities in practical applications.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>MlingConf: A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models</title><link>https://arxiv.org/abs/2410.12478</link><description>https://arxiv.org/abs/2410.12478&lt;br /&gt;MlingConf presents a comprehensive study of Multilingual Confidence Estimation on Large Language Models, revealing insights on language dominance and proposing a native-tone prompting strategy to enhance the reliability and accuracy of LLMs in different languages.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Comparative Analysis of Extrinsic Factors for NER in French</title><link>https://arxiv.org/abs/2410.12750</link><description>https://arxiv.org/abs/2410.12750&lt;br /&gt;This paper explores various extrinsic factors including model structure, corpus annotation schemes, and data augmentation techniques to enhance the performance of Named Entity Recognition (NER) models for the French language, demonstrating significant improvements in F1 scores with limited data.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>UniAutoML: A Human-Centered Framework for Unified Discriminative and Generative AutoML with Large Language Models</title><link>https://arxiv.org/abs/2410.12841</link><description>https://arxiv.org/abs/2410.12841&lt;br /&gt;UniAutoML is a human-centered AutoML framework that integrates Large Language Models to unify discriminative and generative tasks with a conversational user interface, enhancing user engagement, transparency, and control in the AutoML process.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced Explanation Evaluation Framework</title><link>https://arxiv.org/abs/2410.12855</link><description>https://arxiv.org/abs/2410.12855&lt;br /&gt;JAILJUDGE introduces a comprehensive benchmark for evaluating Large Language Model defenses against jailbreak attacks by providing a diverse set of risk scenarios and utilizing a Multi-Agent framework for explainable, fine-grained scoring.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>On Debiasing Text Embeddings Through Context Injection</title><link>https://arxiv.org/abs/2410.12874</link><description>https://arxiv.org/abs/2410.12874&lt;br /&gt;This paper reviews 19 embedding models to analyze their biases and the effectiveness of context injection for debiasing, revealing that higher-performing models are more prone to biases but also better at incorporating context, and it proposes a new algorithm for improved retrieval that addresses bias issues.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs</title><link>https://arxiv.org/abs/2410.13276</link><description>https://arxiv.org/abs/2410.13276&lt;br /&gt;SeerAttention introduces a novel adaptive attention mechanism for Large Language Models (LLMs) that learns attention sparsity dynamically, significantly improving efficiency and scalability for long-context language tasks while balancing accuracy and speedup.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>BANTH: A Multi-label Hate Speech Detection Dataset for Transliterated Bangla</title><link>https://arxiv.org/abs/2410.13281</link><description>https://arxiv.org/abs/2410.13281&lt;br /&gt;BANTH introduces the first multi-label hate speech detection dataset for transliterated Bangla, consisting of 37.3k samples sourced from YouTube comments, and establishes novel transformer encoder-based baselines that achieve state-of-the-art performance while addressing classification challenges in low-resource languages.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>BenTo: Benchmark Task Reduction with In-Context Transferability</title><link>https://arxiv.org/abs/2410.13804</link><description>https://arxiv.org/abs/2410.13804&lt;br /&gt;BenTo introduces a method for efficiently reducing the number of tasks used to benchmark large language models (LLMs) by utilizing task transferability and relevance to identify a representative subset, achieving significant task reduction while maintaining evaluation quality.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off</title><link>https://arxiv.org/abs/2403.04808</link><description>https://arxiv.org/abs/2403.04808&lt;br /&gt;WaterMax introduces a novel watermarking scheme for Large Language Models that achieves high watermark detectability while maintaining the quality of the generated text, overcoming the traditional quality-robustness trade-off in watermarking techniques.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Movie101v2: Improved Movie Narration Benchmark</title><link>https://arxiv.org/abs/2404.13370</link><description>https://arxiv.org/abs/2404.13370&lt;br /&gt;Movie101v2 introduces a large-scale, bilingual dataset and establishes a benchmark for automatic movie narration to support visually impaired audiences, focusing on generating video-aligned plot descriptions across multiple shots.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>On the Use of Large Language Models to Generate Capability Ontologies</title><link>https://arxiv.org/abs/2404.17524</link><description>https://arxiv.org/abs/2404.17524&lt;br /&gt;This paper investigates how Large Language Models (LLMs) can assist in the generation of capability ontologies by creating machine-interpretable models from natural language and evaluating the quality of generated ontologies through various experiments and checks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs</title><link>https://arxiv.org/abs/2406.02958</link><description>https://arxiv.org/abs/2406.02958&lt;br /&gt;PrE-Text is a method for generating differentially private synthetic textual data that allows small models to outperform traditional on-device training while requiring significantly less communication and computation, and also enhances the performance of large language models on private data.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Toward a Method to Generate Capability Ontologies from Natural Language Descriptions</title><link>https://arxiv.org/abs/2406.07962</link><description>https://arxiv.org/abs/2406.07962&lt;br /&gt;This paper presents an automated method for generating capability ontologies from natural language descriptions using Large Language Models (LLMs), which includes a looped verification process to ensure correctness and reduce manual effort.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction</title><link>https://arxiv.org/abs/2406.12950</link><description>https://arxiv.org/abs/2406.12950&lt;br /&gt;MolecularGPT introduces a fine-tuned large language model (LLM) for few-shot molecular property prediction, showcasing its ability to adapt to new tasks with minimal examples and outperforming traditional methods and standard LLMs in various evaluation metrics.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Amphista: Bi-directional Multi-head Decoding for Accelerating LLM Inference</title><link>https://arxiv.org/abs/2406.13170</link><description>https://arxiv.org/abs/2406.13170&lt;br /&gt;Amphista is a speculative decoding framework for Large Language Models (LLMs) that enhances inference speed through bi-directional multi-head decoding, achieving significant speedup while maintaining the quality of generated content.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>LatentExplainer: Explaining Latent Representations in Deep Generative Models with Multi-modal Foundation Models</title><link>https://arxiv.org/abs/2406.14862</link><description>https://arxiv.org/abs/2406.14862&lt;br /&gt;LatentExplainer is a framework that generates semantically meaningful explanations of latent variables in deep generative models by perturbing these variables and utilizing multi-modal large language models to produce human-understandable interpretations.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>From Introspection to Best Practices: Principled Analysis of Demonstrations in Multimodal In-Context Learning</title><link>https://arxiv.org/abs/2407.00902</link><description>https://arxiv.org/abs/2407.00902&lt;br /&gt;The paper presents a principled analysis of multimodal in-context learning (ICL) in Large Language Models (LLMs), exploring how different modalities affect task performance and recommending strategies for effective demonstrations to enhance ICL across various tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Dating ancient manuscripts using radiocarbon and AI-based writing style analysis</title><link>https://arxiv.org/abs/2407.12013</link><description>https://arxiv.org/abs/2407.12013&lt;br /&gt;The study presents Enoch, an AI-based model leveraging Bayesian ridge regression and handwriting style analysis to predict the dates of ancient manuscripts, particularly the Dead Sea Scrolls, achieving improved granularity and contributing to the chronology of historical texts.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Improving Retrieval in Sponsored Search by Leveraging Query Context Signals</title><link>https://arxiv.org/abs/2407.14346</link><description>https://arxiv.org/abs/2407.14346&lt;br /&gt;This paper presents an approach to enhance retrieval in Sponsored Search by utilizing query context signals from web search results and large language models to improve understanding of user intent, leading to better performance in retrieving relevant keywords for short, ambiguous queries.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Tighter Complexity Analysis of SparseGPT</title><link>https://arxiv.org/abs/2408.12151</link><description>https://arxiv.org/abs/2408.12151&lt;br /&gt;This paper presents an improved analysis of SparseGPT's running time, reducing it from $O(d^{3})$ to $O(d^{2.53})$ by examining the lazy update behavior in iterative maintenance problems.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding</title><link>https://arxiv.org/abs/2408.15545</link><description>https://arxiv.org/abs/2408.15545&lt;br /&gt;SciLitLLM is a framework that enhances Large Language Models (LLMs) for scientific literature understanding through continual pre-training and supervised fine-tuning, addressing challenges in knowledge infusion and instructional generation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Supervised Fine-Tuning Achieve Rapid Task Adaption Via Alternating Attention Head Activation Patterns</title><link>https://arxiv.org/abs/2409.15820</link><description>https://arxiv.org/abs/2409.15820&lt;br /&gt;The paper explores how supervised fine-tuning (SFT) can enhance the rapid adaptation of Large Language Models (LLMs) to complex tasks by analyzing attention head activation patterns, revealing that selective activation of task-specific heads and parameter changes can improve task performance.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models</title><link>https://arxiv.org/abs/2410.00131</link><description>https://arxiv.org/abs/2410.00131&lt;br /&gt;FibecFed is a Fisher Information-based framework that enhances the efficiency of Federated Learning for fine-tuning Large Language Models by implementing adaptive sampling and sparse parameter updates to reduce communication costs and improve training speed.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>System 2 thinking in OpenAI's o1-preview model: Near-perfect performance on a mathematics exam</title><link>https://arxiv.org/abs/2410.07114</link><description>https://arxiv.org/abs/2410.07114&lt;br /&gt;The study evaluates OpenAI's o1-preview model, which demonstrates near-perfect performance on a mathematics exam, indicating its capability for System 2-like reasoning, while also identifying variability in outputs and the importance of a self-consistency approach for improving accuracy.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment</title><link>https://arxiv.org/abs/2410.09421</link><description>https://arxiv.org/abs/2410.09421&lt;br /&gt;VLFeedback introduces a large-scale dataset for AI feedback aimed at aligning large vision-language models (LVLMs), demonstrating improved performance in helpfulness, visual faithfulness, and safety, while addressing challenges such as hallucinations and vulnerability to adversarial attacks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>BlackDAN: A Black-Box Multi-Objective Approach for Effective and Contextual Jailbreaking of Large Language Models</title><link>https://arxiv.org/abs/2410.09804</link><description>https://arxiv.org/abs/2410.09804&lt;br /&gt;BlackDAN is a multi-objective optimization framework designed to enhance the effectiveness and contextual relevance of jailbreak attacks on large language models (LLMs) while minimizing their detectability, utilizing Multiobjective Evolutionary Algorithms for prompt generation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Learning Linear Attention in Polynomial Time</title><link>https://arxiv.org/abs/2410.10101</link><description>https://arxiv.org/abs/2410.10101&lt;br /&gt;This study demonstrates that single-layer Transformers with linear attention can be learned in polynomial time and provides insights into their theoretical expressivity and practical applications, including associative memories and finite automata.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>QUIS: Question-guided Insights Generation for Automated Exploratory Data Analysis</title><link>https://arxiv.org/abs/2410.10270</link><description>https://arxiv.org/abs/2410.10270&lt;br /&gt;QUIS is a fully automated exploratory data analysis system that generates and refines questions to drive insight generation from large datasets, eliminating the need for human intervention and allowing for adaptability to new data.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Liger Kernel: Efficient Triton Kernels for LLM Training</title><link>https://arxiv.org/abs/2410.10989</link><description>https://arxiv.org/abs/2410.10989&lt;br /&gt;Liger Kernel introduces a set of optimized Triton kernels for efficient training of Large Language Models (LLMs), achieving significant improvements in training throughput and GPU memory usage.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Scaling laws for post-training quantized large language models</title><link>https://arxiv.org/abs/2410.12119</link><description>https://arxiv.org/abs/2410.12119&lt;br /&gt;This research paper investigates the predictability of post-training performance in quantized large language models (LLMs) by identifying scaling factors related to the local loss landscape, providing a statistical model for estimating their performance after compression.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>An Evolved Universal Transformer Memory</title><link>https://arxiv.org/abs/2410.13166</link><description>https://arxiv.org/abs/2410.13166&lt;br /&gt;The paper introduces Neural Attention Memory Models (NAMMs), which enhance the performance and efficiency of transformers by evolving memory management to focus on the most relevant information for attention layers, achieving significant improvements in long-context tasks and demonstrating general applicability across various transformer architectures.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>MoR: Mixture of Ranks for Low-Rank Adaptation Tuning</title><link>https://arxiv.org/abs/2410.13408</link><description>https://arxiv.org/abs/2410.13408&lt;br /&gt;MoR introduces a new framework for Low-Rank Adaptation that efficiently captures task-specific information and integrates multi-rank data to improve performance while reducing parameter usage in comparison to traditional methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Harnessing Webpage UIs for Text-Rich Visual Understanding</title><link>https://arxiv.org/abs/2410.13824</link><description>https://arxiv.org/abs/2410.13824&lt;br /&gt;The paper presents MultiUI, a dataset comprising 7.3 million samples from 1 million websites, which synthesizes multimodal instructions from webpage UIs to train models for improved text-rich visual understanding across various tasks, demonstrating significant performance enhancements in web UI and non-UI applications.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles</title><link>https://arxiv.org/abs/2410.14042</link><description>https://arxiv.org/abs/2410.14042&lt;br /&gt;Style-Compress is a framework that enables a smaller language model to effectively compress prompts for larger models by utilizing task-specific styles, improving efficiency and performance across multiple tasks while significantly reducing computational costs.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion</title><link>https://arxiv.org/abs/2410.14405</link><description>https://arxiv.org/abs/2410.14405&lt;br /&gt;The paper investigates the distinct behaviors of language models (LMs) in processing factual information through a model-specific framework called PrISM, revealing variations in model reliability across different prediction scenarios for fact completion.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas</title><link>https://arxiv.org/abs/2410.14255</link><description>https://arxiv.org/abs/2410.14255&lt;br /&gt;Nova presents an iterative planning and search methodology that enhances the novelty and diversity of ideas generated by large language models (LLMs) by effectively retrieving and integrating external knowledge, resulting in a significant increase in the generation of unique and high-quality research ideas.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data</title><link>https://arxiv.org/abs/2406.15053</link><description>https://arxiv.org/abs/2406.15053&lt;br /&gt;The paper investigates the agreement between human evaluators and Large Language Models (LLMs) on the assessment of multilingual models across various Indic languages, revealing differences in evaluation performance and biases.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>RiTeK: A Dataset for Large Language Models Complex Reasoning over Textual Knowledge Graphs</title><link>https://arxiv.org/abs/2410.13987</link><description>https://arxiv.org/abs/2410.13987&lt;br /&gt;The paper presents a two-stage contrastive learning framework for improving bilingual lexicon induction (BLI), successfully refining cross-lingual mappings and enhancing word translation capabilities of mBERT through fine-tuning.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Generating Signed Language Instructions in Large-Scale Dialogue Systems</title><link>https://arxiv.org/abs/2410.14026</link><description>https://arxiv.org/abs/2410.14026&lt;br /&gt;This paper introduces the concept of entity-level unlearning for large language models, highlighting the limitations of current instance-level unlearning methods and presenting a comprehensive evaluation of unlearning algorithms in the context of removing entity-related knowledge.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A two-stage transliteration approach to improve performance of a multilingual ASR</title><link>https://arxiv.org/abs/2410.14709</link><description>https://arxiv.org/abs/2410.14709&lt;br /&gt;CAP introduces a novel framework for detecting data contamination in large language models (LLMs) through performance consistency measurement, offering a practical solution applicable across various benchmarks and model types.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Eliciting Uncertainty in Chain-of-Thought to Mitigate Bias against Forecasting Harmful User Behaviors</title><link>https://arxiv.org/abs/2410.14744</link><description>https://arxiv.org/abs/2410.14744&lt;br /&gt;This paper investigates the effects of scaling inference compute by analyzing the coverage improvements through repeated sampling in large language models (LLMs) across two domains, proposing an enumeration baseline for more accurate performance measurement.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Controllable Discovery of Intents: Incremental Deep Clustering Using Semi-Supervised Contrastive Learning</title><link>https://arxiv.org/abs/2410.14755</link><description>https://arxiv.org/abs/2410.14755&lt;br /&gt;This study assesses the rhetorical styles of various large language models (LLMs) by comparing their outputs to human-written texts, revealing systematic differences in grammatical and rhetorical features that persist across different model sizes and tuning strategies.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Enabling Scalable Evaluation of Bias Patterns in Medical LLMs</title><link>https://arxiv.org/abs/2410.14763</link><description>https://arxiv.org/abs/2410.14763&lt;br /&gt;RepoGraph is a plugin module designed to enhance large language models' performance in AI software engineering by providing a repository-level code structure, enabling better context understanding and interaction with code repositories.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Cross-Document Event-Keyed Summarization</title><link>https://arxiv.org/abs/2410.14795</link><description>https://arxiv.org/abs/2410.14795&lt;br /&gt;ChitroJera introduces a large-scale visual question answering dataset specifically tailored for the Bangla language, addressing the lack of culturally relevant resources and demonstrating improved performance with novel dual-encoder models and large language models.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Isolated Causal Effects of Natural Language</title><link>https://arxiv.org/abs/2410.14812</link><description>https://arxiv.org/abs/2410.14812&lt;br /&gt;The proposed Generalized Probabilistic Attention Mechanism (GPAM) enhances the Transformer architecture by addressing rank-collapse and gradient vanishing issues while allowing for negative attention scores, thereby improving performance across various natural language processing tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Adapting Multilingual LLMs to Low-Resource Languages using Continued Pre-training and Synthetic Corpus</title><link>https://arxiv.org/abs/2410.14815</link><description>https://arxiv.org/abs/2410.14815&lt;br /&gt;This paper introduces the Counterfactual Augmented Calibration Network (FACTUAL) to mitigate biases in Large Language Models (LLMs) during stance detection tasks, demonstrating its effectiveness through experimental results that show an enhancement in calibration and generalizability.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Complexity-Based Theory of Compositionality</title><link>https://arxiv.org/abs/2410.14817</link><description>https://arxiv.org/abs/2410.14817&lt;br /&gt;This paper investigates the inner workings of Mixture-of-Experts (MoE) architecture in large language models, highlighting features such as the behavior of neurons as experts, router selection of experts, and the increasing diversity of experts across layers, aiming to provide insights for future improvements in MoE designs.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>SPRIG: Improving Large Language Model Performance by System Prompt Optimization</title><link>https://arxiv.org/abs/2410.14826</link><description>https://arxiv.org/abs/2410.14826&lt;br /&gt;The CLC-UKET dataset is developed to benchmark case outcome prediction in the UK Employment Tribunal by utilizing a large language model for automatic annotation of approximately 19,000 cases and their legal metadata, aiming to improve access to justice through empirical comparison of model performance in predicting case outcomes.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>DFlow: Diverse Dialogue Flow Simulation with Large Language Models</title><link>https://arxiv.org/abs/2410.14853</link><description>https://arxiv.org/abs/2410.14853&lt;br /&gt;This paper investigates how well contextualized representations in neural autoregressive language models encode long-range context, finding that different architectures and training configurations significantly impact their ability to capture complex sequences.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Which LLMs are Difficult to Detect? A Detailed Analysis of Potential Factors Contributing to Difficulties in LLM Text Detection</title><link>https://arxiv.org/abs/2410.14875</link><description>https://arxiv.org/abs/2410.14875&lt;br /&gt;RACCooN is a versatile video editing framework that employs a two-stage process to transform videos into structured natural language descriptions and back to modified videos, allowing for flexible adaptations without complex human annotations.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items</title><link>https://arxiv.org/abs/2410.14897</link><description>https://arxiv.org/abs/2410.14897&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to evaluate systems developed to attack voice anonymization techniques, with specific datasets and metrics for participant assessment, culminating in a presentation of results at ICASSP 2025.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>SemiHVision: Enhancing Medical Multimodal Models with a Semi-Human Annotated Dataset and Fine-Tuned Instruction Generation</title><link>https://arxiv.org/abs/2410.14948</link><description>https://arxiv.org/abs/2410.14948&lt;br /&gt;MathGAP is a framework designed to evaluate Large Language Models (LLMs) on arithmetic problems with arbitrarily complex proofs, revealing significant performance drops in models as proof complexity increases, particularly in complex, nonlinear structures.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Agent Skill Acquisition for Large Language Models via CycleQD</title><link>https://arxiv.org/abs/2410.14735</link><description>https://arxiv.org/abs/2410.14735&lt;br /&gt;This study investigates the impact of soft-domain transfer and named entity information on deception detection by analyzing multiple datasets and utilizing transfer learning with fine-tuned BERT models to improve classifier accuracy.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation</title><link>https://arxiv.org/abs/2410.14745</link><description>https://arxiv.org/abs/2410.14745&lt;br /&gt;This study investigates the effects of soft-domain transfer and named entity information on deception detection, demonstrating that leveraging multiple datasets through transfer learning can significantly improve classifier accuracy in identifying deceitful online communications.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Accounting for Sycophancy in Language Model Uncertainty Estimation</title><link>https://arxiv.org/abs/2410.14746</link><description>https://arxiv.org/abs/2410.14746&lt;br /&gt;This paper explores the use of transfer learning from various text-only domains and the incorporation of named entity information to enhance the performance of deception detection classifiers based on fine-tuned BERT models, resulting in significant accuracy improvements.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Effects of Soft-Domain Transfer and Named Entity Information on Deception Detection</title><link>https://arxiv.org/abs/2410.14814</link><description>https://arxiv.org/abs/2410.14814&lt;br /&gt;This paper investigates the impact of soft-domain transfer learning and the incorporation of named entity information on improving deception detection accuracy across multiple text-only domains using fine-tuned BERT models.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>ChronoFact: Timeline-based Temporal Fact Verification</title><link>https://arxiv.org/abs/2410.14964</link><description>https://arxiv.org/abs/2410.14964&lt;br /&gt;CAP introduces a novel framework for detecting data contamination in large language models (LLMs) through Performance Consistency Ratio (PCR), enabling differentiation between fine-tuning and contamination while being applicable across various benchmarks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Subversive Characters and Stereotyping Readers: Characterizing Queer Relationalities with Dialogue-Based Relation Extraction</title><link>https://arxiv.org/abs/2410.14978</link><description>https://arxiv.org/abs/2410.14978&lt;br /&gt;CAP introduces a novel framework for detecting data contamination in large language models via Performance Consistency Ratio, effectively distinguishing between fine-tuning and contamination across various benchmarks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>CAP: Data Contamination Detection via Consistency Amplification</title><link>https://arxiv.org/abs/2410.15005</link><description>https://arxiv.org/abs/2410.15005&lt;br /&gt;The CAP framework introduces a novel method for detecting data contamination in large language models by measuring the Performance Consistency Ratio (PCR) to differentiate between fine-tuning and contamination, enhancing reliability in LLM evaluations across various benchmarks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>On the Diversity of Synthetic Data and its Impact on Training Large Language Models</title><link>https://arxiv.org/abs/2410.15226</link><description>https://arxiv.org/abs/2410.15226&lt;br /&gt;This paper proposes a baseline for evaluating the impact of repeated sampling on inference coverage in large language models (LLMs), revealing that for certain models, a prevalence-based baseline outperforms repeated sampling in tasks like mathematical reasoning and factual knowledge.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Lossless KV Cache Compression to 2%</title><link>https://arxiv.org/abs/2410.15252</link><description>https://arxiv.org/abs/2410.15252&lt;br /&gt;This paper investigates the effect of repeated sampling on inference scaling in large language models (LLMs), determining that a baseline method of answer enumeration can outperform or match the effectiveness of repeated model sampling in enhancing problem-solving coverage.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Back to School: Translation Using Grammar Books</title><link>https://arxiv.org/abs/2410.15263</link><description>https://arxiv.org/abs/2410.15263&lt;br /&gt;The paper investigates the effects of repeated sampling for inference scaling in large language models (LLMs), proposing a baseline that enumerates answers based on their prevalence in training data, which outperforms some sampling strategies in improving coverage for various tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>BRIEF: Bridging Retrieval and Inference for Multi-hop Reasoning via Compression</title><link>https://arxiv.org/abs/2410.15277</link><description>https://arxiv.org/abs/2410.15277&lt;br /&gt;This paper investigates the impact of inference scaling and repeated sampling in large language models (LLMs), proposing a new baseline that outperforms traditional repeated sampling in certain contexts and highlighting the importance of answer distributions in evaluating model performance.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Training Language Models to Critique With Multi-agent Feedback</title><link>https://arxiv.org/abs/2410.15287</link><description>https://arxiv.org/abs/2410.15287&lt;br /&gt;The paper investigates the effects of scaling inference compute in large language models (LLMs) through repeated sampling and proposes a baseline method that accounts for answer distribution, revealing that this baseline often outperforms or matches the coverage improvement achieved by repeated model sampling in various domains.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Redefining Proactivity for Information Seeking Dialogue</title><link>https://arxiv.org/abs/2410.15297</link><description>https://arxiv.org/abs/2410.15297&lt;br /&gt;This paper investigates the effects of repeated sampling in large language models (LLMs) on inference scaling and coverage, revealing that a simple baseline approach can outperform or match the performance of repeated sampling in certain contexts, emphasizing the importance of benchmarking against prevalent answer distributions.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Does ChatGPT Have a Poetic Style?</title><link>https://arxiv.org/abs/2410.15299</link><description>https://arxiv.org/abs/2410.15299&lt;br /&gt;This paper investigates the effectiveness of repeated sampling in large language models for improving inference coverage, comparing it against a baseline that enumerates common answers from the training set, revealing that the baseline can outperform or match the performance of sampling strategies in certain scenarios.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>LlamaLens: Specialized Multilingual LLM for Analyzing News and Social Media Content</title><link>https://arxiv.org/abs/2410.15308</link><description>https://arxiv.org/abs/2410.15308&lt;br /&gt;This paper investigates the effectiveness of scaling inference compute in large language models (LLMs) through repeated sampling, proposing a baseline that enumerates common answers and demonstrating its potential to outperform some sampling strategies in specific domains.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>KTCR: Improving Implicit Hate Detection with Knowledge Transfer driven Concept Refinement</title><link>https://arxiv.org/abs/2410.15314</link><description>https://arxiv.org/abs/2410.15314&lt;br /&gt;This paper investigates the effects of repeated sampling in large language models (LLMs) on inference scaling and coverage, proposing a baseline approach that enumerates answers based on their prevalence in the training set, revealing that this approach can outperform or match the performance of repeated sampling strategies for certain models.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Causality for Large Language Models</title><link>https://arxiv.org/abs/2410.15319</link><description>https://arxiv.org/abs/2410.15319&lt;br /&gt;This paper examines the effectiveness of baselines for increasing coverage in large language model (LLM) inference by contrasting repeated sampling with a new baseline that leverages the prevalence of answers in training datasets, revealing variable performance across different models in mathematical reasoning and factual knowledge domains.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Survey of Uncertainty Estimation in LLMs: Theory Meets Practice</title><link>https://arxiv.org/abs/2410.15326</link><description>https://arxiv.org/abs/2410.15326&lt;br /&gt;The paper investigates the effects of repeated sampling for inference in large language models (LLMs), arguing that improvements in coverage can be attributed to skewed answer distributions in evaluation benchmarks and proposing a baseline that uses answer prevalence for a fairer comparison against model sampling.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>BERTtime Stories: Investigating the Role of Synthetic Story Data in Language pre-training</title><link>https://arxiv.org/abs/2410.15365</link><description>https://arxiv.org/abs/2410.15365&lt;br /&gt;This paper introduces a baseline for evaluating the efficacy of repeated sampling in large language models (LLMs) by comparing it against a method that enumerates answers based on their prevalence in the training set, revealing that for certain LLMs, this baseline can outperform repeated sampling in coverage on standard evaluation benchmarks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>CalibraEval: Calibrating Prediction Distribution to Mitigate Selection Bias in LLMs-as-Judges</title><link>https://arxiv.org/abs/2410.15393</link><description>https://arxiv.org/abs/2410.15393&lt;br /&gt;This paper investigates the efficacy of inference scaling in large language models (LLMs) by comparing repeated sampling strategies with a baseline that enumerates answers according to their prevalence in the training set, revealing insights into how these strategies impact coverage in mathematical reasoning and factual knowledge domains.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Comprehensive Evaluation of Cognitive Biases in LLMs</title><link>https://arxiv.org/abs/2410.15413</link><description>https://arxiv.org/abs/2410.15413&lt;br /&gt;This paper examines the effects of scaling inference compute in large language models, positing that traditional evaluation benchmarks may skew results, and introduces a new baseline that enumerates answers based on training set prevalence to enhance the understanding of coverage improvements from repeated sampling methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Evaluating Consistencies in LLM responses through a Semantic Clustering of Question Answering</title><link>https://arxiv.org/abs/2410.15440</link><description>https://arxiv.org/abs/2410.15440&lt;br /&gt;The paper investigates the effects of repeated sampling in large language models (LLMs) on inference scaling, proposing a baseline that enumerates answer prevalence in the training set to compare against the benefits of repeated model sampling in terms of coverage and problem-solving ability.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>CROPE: Evaluating In-Context Adaptation of Vision and Language Models to Culture-Specific Concepts</title><link>https://arxiv.org/abs/2410.15453</link><description>https://arxiv.org/abs/2410.15453&lt;br /&gt;This paper investigates the effectiveness of scaling inference in large language models (LLMs) through repeated sampling, proposing a baseline based on the prevalence of answers in training data, and showing that this baseline may outperform traditional sampling methods in certain domains, thereby allowing for more accurate assessments of inference improvement.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>MedLogic-AQA: Enhancing Medical Question Answering with Abstractive Models Focusing on Logical Structures</title><link>https://arxiv.org/abs/2410.15463</link><description>https://arxiv.org/abs/2410.15463&lt;br /&gt;The paper investigates the effectiveness of scaling inference in large language models (LLMs) through repeated sampling, revealing that a newly defined baseline based on answer prevalence can surpass or match the performance of sampling methods, thus providing a clearer understanding of their true impact on coverage of problem-solving across various domains.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Novel Interpretability Metric for Explaining Bias in Language Models: Applications on Multilingual Models from Southeast Asia</title><link>https://arxiv.org/abs/2410.15464</link><description>https://arxiv.org/abs/2410.15464&lt;br /&gt;The paper proposes a new baseline for evaluating the effectiveness of repeated sampling in large language models (LLMs), suggesting that this strategy may not consistently outperform a simpler approach that enumerates answers based on their prevalence in training data.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Keep Guessing? When Considering Inference Scaling, Mind the Baselines</title><link>https://arxiv.org/abs/2410.15466</link><description>https://arxiv.org/abs/2410.15466&lt;br /&gt;This paper investigates the effect of scaling inference compute through repeated sampling in large language models, proposing a baseline answer enumeration method that challenges the performance of repeated model sampling in improving problem coverage across domains like mathematical reasoning and factual knowledge.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Hey GPT, Can You be More Racist? Analysis from Crowdsourced Attempts to Elicit Biased Content from Generative AI</title><link>https://arxiv.org/abs/2410.15467</link><description>https://arxiv.org/abs/2410.15467&lt;br /&gt;This paper investigates security vulnerabilities in large language models (LLMs) related to chemistry, introducing a novel SMILES-prompting technique that can bypass safety mechanisms and provide instructions for synthesizing hazardous substances, thus highlighting the need for improved safeguards.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>"What is the value of {templates}?" Rethinking Document Information Extraction Datasets for LLMs</title><link>https://arxiv.org/abs/2410.15484</link><description>https://arxiv.org/abs/2410.15484&lt;br /&gt;This paper investigates the vulnerabilities of large language models (LLMs) in chemistry, focusing on how certain prompting techniques, particularly SMILES-prompting, can exploit these models to provide instructions for synthesizing hazardous substances, thereby revealing the need for improved safety mechanisms.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>RoMemes: A multimodal meme corpus for the Romanian language</title><link>https://arxiv.org/abs/2410.15497</link><description>https://arxiv.org/abs/2410.15497&lt;br /&gt;This paper investigates the security vulnerabilities of large language models (LLMs) in chemistry, introducing a novel attack method called SMILES-prompting, which uses chemical notation to bypass safety mechanisms and provide potentially dangerous synthesis instructions, underscoring the need for improved safeguards in LLM applications.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Reverse Question Answering: Can an LLM Write a Question so Hard (or Bad) that it Can't Answer?</title><link>https://arxiv.org/abs/2410.15512</link><description>https://arxiv.org/abs/2410.15512&lt;br /&gt;This paper introduces SMILES-prompting, a novel attack method that utilizes the Simplified Molecular-Input Line-Entry System (SMILES) to exploit security vulnerabilities in large language models (LLMs) specific to the field of chemistry, demonstrating the effectiveness of such attacks in bypassing existing safety mechanisms.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>SceneGraMMi: Scene Graph-boosted Hybrid-fusion for Multi-Modal Misinformation Veracity Prediction</title><link>https://arxiv.org/abs/2410.15517</link><description>https://arxiv.org/abs/2410.15517&lt;br /&gt;This paper investigates the security vulnerabilities of large language models (LLMs) in the chemistry domain, introducing a new attack technique called SMILES-prompting, which exploits the Simplified Molecular-Input Line-Entry System to bypass existing safety mechanisms and potentially provide instructions for synthesizing hazardous substances.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>M-RewardBench: Evaluating Reward Models in Multilingual Settings</title><link>https://arxiv.org/abs/2410.15522</link><description>https://arxiv.org/abs/2410.15522&lt;br /&gt;The paper examines security vulnerabilities in large language models (LLMs) related to chemical synthesis, introducing a novel attack method called SMILES-prompting that effectively bypasses current safety mechanisms, underscoring the need for improved safeguards against misuse.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Do RAG Systems Cover What Matters? Evaluating and Optimizing Responses with Sub-Question Coverage</title><link>https://arxiv.org/abs/2410.15531</link><description>https://arxiv.org/abs/2410.15531&lt;br /&gt;This paper examines the security vulnerabilities of large language models (LLMs) in the field of chemistry, introducing a novel attack method called SMILES-prompting that effectively bypasses safety mechanisms to provide instructions for synthesizing hazardous substances, thereby emphasizing the need for improved safeguards.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Grammatical Error Correction for Low-Resource Languages: The Case of Zarma</title><link>https://arxiv.org/abs/2410.15539</link><description>https://arxiv.org/abs/2410.15539&lt;br /&gt;The paper investigates security vulnerabilities within large language models (LLMs) regarding their potential to provide dangerous information in chemical synthesis, introducing a novel attack technique called SMILES-prompting that successfully bypasses existing safety mechanisms.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>WHoW: A Cross-domain Approach for Analysing Conversation Moderation</title><link>https://arxiv.org/abs/2410.15551</link><description>https://arxiv.org/abs/2410.15551&lt;br /&gt;The paper investigates the security vulnerabilities of large language models (LLMs) in chemistry, introducing SMILES-prompting as a novel attack method that can bypass safety mechanisms to potentially provide instructions for synthesizing hazardous substances, thus highlighting the need for enhanced safeguards in LLM applications.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Multi-IF: Benchmarking LLMs on Multi-Turn and Multilingual Instructions Following</title><link>https://arxiv.org/abs/2410.15553</link><description>https://arxiv.org/abs/2410.15553&lt;br /&gt;The paper investigates security vulnerabilities in large language models (LLMs) within the field of chemistry, particularly focusing on the risks of LLMs providing instructions for synthesizing hazardous substances, and introduces a novel attack technique called SMILES-prompting that effectively bypasses safety mechanisms.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Stacking Small Language Models for Generalizability</title><link>https://arxiv.org/abs/2410.15570</link><description>https://arxiv.org/abs/2410.15570&lt;br /&gt;This paper investigates the security vulnerabilities of large language models (LLMs) in chemistry, particularly their ability to provide dangerous synthesis instructions, and introduces a novel attack method called SMILES-prompting that can bypass existing safety mechanisms.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Leveraging Retrieval-Augmented Generation for Culturally Inclusive Hakka Chatbots: Design Insights and User Perceptions</title><link>https://arxiv.org/abs/2410.15572</link><description>https://arxiv.org/abs/2410.15572&lt;br /&gt;This paper investigates the security vulnerabilities of large language models (LLMs) in the field of chemistry, particularly focusing on a novel attack method called SMILES-prompting, which uses the Simplified Molecular-Input Line-Entry System to bypass safety mechanisms and potentially facilitate harmful chemical synthesis.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Survey of Conversational Search</title><link>https://arxiv.org/abs/2410.15576</link><description>https://arxiv.org/abs/2410.15576&lt;br /&gt;This paper investigates the security vulnerabilities of large language models (LLMs) in chemistry, specifically focusing on prompt injection attack techniques and introducing a novel method called SMILES-prompting that effectively bypasses safety mechanisms designed to prevent dangerous chemical synthesis instructions.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Interventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding</title><link>https://arxiv.org/abs/2410.15609</link><description>https://arxiv.org/abs/2410.15609&lt;br /&gt;This paper investigates security vulnerabilities in large language models (LLMs) regarding their ability to provide potentially hazardous chemical synthesis instructions and introduces a novel attack method called SMILES-prompting, which bypasses existing safety mechanisms.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Guardians of Discourse: Evaluating LLMs on Multilingual Offensive Language Detection</title><link>https://arxiv.org/abs/2410.15623</link><description>https://arxiv.org/abs/2410.15623&lt;br /&gt;This paper investigates the vulnerabilities of large language models (LLMs) in providing instructions for synthesizing hazardous substances and introduces a novel attack method called SMILES-prompting, demonstrating its effectiveness in bypassing safety mechanisms.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Can Large Language Models Invent Algorithms to Improve Themselves?</title><link>https://arxiv.org/abs/2410.15639</link><description>https://arxiv.org/abs/2410.15639&lt;br /&gt;SMILES-Prompting introduces a novel attack technique that exploits the vulnerabilities of large language models in the field of chemistry, demonstrating how they can be manipulated to provide dangerous instructions for synthesizing hazardous substances, emphasizing the need for stronger safety measures.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>SMILES-Prompting: A Novel Approach to LLM Jailbreak Attacks in Chemical Synthesis</title><link>https://arxiv.org/abs/2410.15641</link><description>https://arxiv.org/abs/2410.15641&lt;br /&gt;The paper introduces SMILES-Prompting, a novel attack technique that leverages chemical notation to exploit vulnerabilities in large language models (LLMs) for providing instructions on hazardous substance synthesis, thus underscoring the need for better safety measures in LLM applications in chemistry.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>CausalGraph2LLM: Evaluating LLMs for Causal Queries</title><link>https://arxiv.org/abs/2410.15939</link><description>https://arxiv.org/abs/2410.15939&lt;br /&gt;This paper investigates the grammatical and rhetorical style variations between outputs from Large Language Models (LLMs) and human-written texts, revealing systematic differences across various models and highlighting the struggles of LLMs to fully emulate human writing styles.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Findings of the Third Shared Task on Multilingual Coreference Resolution</title><link>https://arxiv.org/abs/2410.15949</link><description>https://arxiv.org/abs/2410.15949&lt;br /&gt;This paper analyzes variations in grammatical and rhetorical styles between human-written text and that generated by Large Language Models (LLMs), revealing systematic differences that persist across model sizes and imply that LLMs struggle to replicate human writing styles despite their advanced capabilities.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Do Large Language Models Have an English Accent? Evaluating and Improving the Naturalness of Multilingual LLMs</title><link>https://arxiv.org/abs/2410.15956</link><description>https://arxiv.org/abs/2410.15956&lt;br /&gt;This study investigates the rhetorical and grammatical style differences between human-written texts and those generated by various Large Language Models (LLMs), revealing systematic variations even as LLMs improve in producing grammatically correct outputs.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Systematic Exploration of Dialogue Summarization Approaches for Reproducibility, Comparative Assessment, and Methodological Innovations for Advancing Natural Language Processing in Abstractive Summarization</title><link>https://arxiv.org/abs/2410.15962</link><description>https://arxiv.org/abs/2410.15962&lt;br /&gt;This study analyzes the grammatical and rhetorical styles of Large Language Models (LLMs) compared to human writing, revealing systematic differences even among advanced models like Llama 3 and GPT-4o, particularly in instruction-tuned variants, and suggesting that LLMs still struggle to fully emulate human styles.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Self-Explained Keywords Empower Large Language Models for Code Generation</title><link>https://arxiv.org/abs/2410.15966</link><description>https://arxiv.org/abs/2410.15966&lt;br /&gt;This study investigates the grammatical and rhetorical styles of output generated by various Large Language Models (LLMs), revealing systematic differences from human-written text despite the advanced capabilities of LLMs.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Policy-driven Knowledge Selection and Response Generation for Document-grounded Dialogue</title><link>https://arxiv.org/abs/2410.15970</link><description>https://arxiv.org/abs/2410.15970&lt;br /&gt;The paper analyzes the differences in grammatical and rhetorical styles between human-written texts and outputs generated by various large language models (LLMs), revealing systematic variations that indicate LLMs do not fully emulate human writing styles despite their advanced capabilities.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Large Language Models for Cross-lingual Emotion Detection</title><link>https://arxiv.org/abs/2410.15974</link><description>https://arxiv.org/abs/2410.15974&lt;br /&gt;This paper investigates the differences in grammatical and rhetorical styles between human-written texts and those generated by various Large Language Models (LLMs), revealing systematic variations that persist across different model sizes and specific tuning techniques.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Augmenting Legal Decision Support Systems with LLM-based NLI for Analyzing Social Media Evidence</title><link>https://arxiv.org/abs/2410.15990</link><description>https://arxiv.org/abs/2410.15990&lt;br /&gt;This paper investigates the differences in grammatical and rhetorical styles between human-written texts and those generated by various large language models (LLMs), revealing systematic variations that persist across model sizes and types, and highlighting the challenges LLMs face in mimicking human writing styles.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>1024m at SMM4H 2024: Tasks 3, 5 &amp; 6 -- Ensembles of Transformers and Large Language Models for Medical Text Classification</title><link>https://arxiv.org/abs/2410.15998</link><description>https://arxiv.org/abs/2410.15998&lt;br /&gt;This paper investigates the grammatical and rhetorical differences between human-written texts and those generated by Large Language Models (LLMs), revealing systematic variations that persist across model sizes and types, thereby highlighting challenges LLMs face in fully mimicking human writing styles.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering</title><link>https://arxiv.org/abs/2410.15999</link><description>https://arxiv.org/abs/2410.15999&lt;br /&gt;This study investigates the grammatical and rhetorical styles of Large Language Models (LLMs) compared to human writing, revealing systematic differences in their outputs, particularly in instruction-tuned models, despite their ability to produce grammatical text.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Exploring Continual Fine-Tuning for Enhancing Language Ability in Large Language Model</title><link>https://arxiv.org/abs/2410.16006</link><description>https://arxiv.org/abs/2410.16006&lt;br /&gt;This paper investigates the grammatical and rhetorical styles of Large Language Models (LLMs), revealing systematic differences between LLM and human-written texts, especially in the context of instruction-tuned models compared to base models.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>CA*: Addressing Evaluation Pitfalls in Computation-Aware Latency for Simultaneous Speech Translation</title><link>https://arxiv.org/abs/2410.16011</link><description>https://arxiv.org/abs/2410.16011&lt;br /&gt;This paper analyzes the differences in grammatical and rhetorical styles between human-written texts and those generated by various large language models (LLMs), revealing systematic variations that indicate LLMs often cannot fully emulate human writing despite their advanced capabilities.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>ComPO: Community Preferences for Language Model Personalization</title><link>https://arxiv.org/abs/2410.16027</link><description>https://arxiv.org/abs/2410.16027&lt;br /&gt;This paper investigates grammatical and rhetorical style differences between human-written and Large Language Model (LLM)-produced texts, revealing that LLMs exhibit systematic variations in style despite their advanced output capabilities.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling</title><link>https://arxiv.org/abs/2410.16033</link><description>https://arxiv.org/abs/2410.16033&lt;br /&gt;This paper examines the differences in grammatical and rhetorical styles between outputs generated by large language models (LLMs) and human-written text, revealing systematic variations that persist across different models despite their advanced capabilities.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Large Language Models Know What To Say But Not When To Speak</title><link>https://arxiv.org/abs/2410.16044</link><description>https://arxiv.org/abs/2410.16044&lt;br /&gt;This paper investigates the differences in grammatical and rhetorical styles between human-written texts and those generated by large language models (LLMs), revealing systematic variations that persist across different model sizes and types.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Surprise! Uniform Information Density Isn't the Whole Story: Predicting Surprisal Contours in Long-form Discourse</title><link>https://arxiv.org/abs/2410.16062</link><description>https://arxiv.org/abs/2410.16062&lt;br /&gt;This study analyzes grammatical and rhetorical style differences between human-written texts and those produced by various Large Language Models (LLMs), revealing systematic variations in their outputs despite advancements in linguistic capabilities.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Rolling the DICE on Idiomaticity: How LLMs Fail to Grasp Context</title><link>https://arxiv.org/abs/2410.16069</link><description>https://arxiv.org/abs/2410.16069&lt;br /&gt;This paper investigates the variations in grammatical and rhetorical styles between human-written text and outputs generated by different Large Language Models (LLMs), revealing systematic differences that persist across model sizes and tuning methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Fine-Tuning LLMs for Reliable Medical Question-Answering Services</title><link>https://arxiv.org/abs/2410.16088</link><description>https://arxiv.org/abs/2410.16088&lt;br /&gt;This paper explores the variation in grammatical and rhetorical styles between outputs from large language models (LLMs) and human writing, finding systematic differences that persist even as models advance.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Analysing the Residual Stream of Language Models Under Knowledge Conflicts</title><link>https://arxiv.org/abs/2410.16090</link><description>https://arxiv.org/abs/2410.16090&lt;br /&gt;This paper examines whether large language models (LLMs) write like humans by analyzing grammatical and rhetorical style differences between human-written and LLM-generated texts, revealing systematic variations in their outputs despite advances in technology.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Do LLMs write like humans? Variation in grammatical and rhetorical styles</title><link>https://arxiv.org/abs/2410.16107</link><description>https://arxiv.org/abs/2410.16107&lt;br /&gt;This study investigates the grammatical and rhetorical style differences between large language models (LLMs) and human writers, revealing systematic distinctions that persist across different model sizes and tuning, indicating that LLMs struggle to fully emulate human writing styles.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Psycholinguistic Evaluation of Language Models' Sensitivity to Argument Roles</title><link>https://arxiv.org/abs/2410.16139</link><description>https://arxiv.org/abs/2410.16139&lt;br /&gt;RepoGraph is a plug-in module designed to enhance AI software engineering by managing a repository-level structure, which significantly improves performance in code generation tasks for large language models (LLMs) in the context of modern software engineering.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs</title><link>https://arxiv.org/abs/2410.16144</link><description>https://arxiv.org/abs/2410.16144&lt;br /&gt;RepoGraph is a plug-in module designed to enhance AI software engineering by providing repository-level code understanding, significantly improving the performance of code generation and management systems.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages</title><link>https://arxiv.org/abs/2410.16153</link><description>https://arxiv.org/abs/2410.16153&lt;br /&gt;RepoGraph is a novel plug-in module that enhances AI software engineering by providing repository-level understanding and navigation, significantly improving code generation performance across various methods and benchmarks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns</title><link>https://arxiv.org/abs/2410.16155</link><description>https://arxiv.org/abs/2410.16155&lt;br /&gt;RepoGraph is a plug-in module that enhances AI software engineering by providing repository-level code graph management, significantly improving the performance of code generation methods and enabling better context understanding for modern software engineering tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Limpeh ga li gong: Challenges in Singlish Annotations</title><link>https://arxiv.org/abs/2410.16156</link><description>https://arxiv.org/abs/2410.16156&lt;br /&gt;RepoGraph is a plug-in module designed to enhance AI software engineering by providing repository-level code understanding, significantly improving performance across various coding methods on benchmarks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>From Tokens to Materials: Leveraging Language Models for Scientific Discovery</title><link>https://arxiv.org/abs/2410.16165</link><description>https://arxiv.org/abs/2410.16165&lt;br /&gt;RepoGraph is a plug-in module that enhances AI software engineering by managing repository-level structures, improving code generation tasks and overall performance across various AI software engineering frameworks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Exploring Pretraining via Active Forgetting for Improving Cross Lingual Transfer for Decoder Language Models</title><link>https://arxiv.org/abs/2410.16168</link><description>https://arxiv.org/abs/2410.16168&lt;br /&gt;RepoGraph is a plug-in module that enhances AI software engineering by providing a repository-level code graph for better context understanding, thereby improving the performance of various coding methods and achieving state-of-the-art results in software engineering benchmarks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>MagicPIG: LSH Sampling for Efficient LLM Generation</title><link>https://arxiv.org/abs/2410.16179</link><description>https://arxiv.org/abs/2410.16179&lt;br /&gt;RepoGraph is a plug-in module designed to enhance AI software engineering by providing repository-level code management and navigation capabilities, significantly boosting the performance of various coding frameworks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style</title><link>https://arxiv.org/abs/2410.16184</link><description>https://arxiv.org/abs/2410.16184&lt;br /&gt;RepoGraph is a plug-in module designed to enhance AI software engineering by providing repository-level code understanding and guidance, significantly improving the performance of code generation models on software engineering tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Contamination Report for Multilingual Benchmarks</title><link>https://arxiv.org/abs/2410.16186</link><description>https://arxiv.org/abs/2410.16186&lt;br /&gt;RepoGraph is a plug-in module designed to enhance AI software engineering by managing repository-level code graphs, significantly improving the performance of coding systems in a comprehensive evaluation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Information for Conversation Generation: Proposals Utilising Knowledge Graphs</title><link>https://arxiv.org/abs/2410.16196</link><description>https://arxiv.org/abs/2410.16196&lt;br /&gt;RepoGraph is a plug-in module developed to enhance AI software engineering by providing repository-level code understanding and navigation, significantly improving the performance of various code generation methods and achieving state-of-the-art results.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Pre-training Distillation for Large Language Models: A Design Space Exploration</title><link>https://arxiv.org/abs/2410.16215</link><description>https://arxiv.org/abs/2410.16215&lt;br /&gt;RepoGraph is a plug-in module designed to enhance AI software engineering by providing repository-level code understanding, significantly improving performance in code generation tasks and establishing a new state-of-the-art in various benchmarks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>On Creating an English-Thai Code-switched Machine Translation in Medical Domain</title><link>https://arxiv.org/abs/2410.16221</link><description>https://arxiv.org/abs/2410.16221&lt;br /&gt;RepoGraph is a plug-in module designed to enhance AI software engineering by providing repository-level code understanding and navigation, significantly improving performance across existing coding methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Building A Coding Assistant via the Retrieval-Augmented Language Model</title><link>https://arxiv.org/abs/2410.16229</link><description>https://arxiv.org/abs/2410.16229&lt;br /&gt;RepoGraph is a plug-in module designed to enhance AI software engineering by providing repository-level code understanding, significantly improving the performance of large language models on complex coding tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Sketch2Code: Evaluating Vision-Language Models for Interactive Web Design Prototyping</title><link>https://arxiv.org/abs/2410.16232</link><description>https://arxiv.org/abs/2410.16232&lt;br /&gt;RepoGraph is a plug-in module that enhances code generation in large language models (LLMs) by providing repository-level structure and management, significantly improving performance on modern AI software engineering tasks compared to traditional coding methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>ToW: Thoughts of Words Improve Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2410.16235</link><description>https://arxiv.org/abs/2410.16235&lt;br /&gt;RepoGraph is a plug-in module designed to enhance AI software engineering by providing repository-level code understanding and navigation, significantly improving performance across multiple coding frameworks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Analyzing Context Contributions in LLM-based Machine Translation</title><link>https://arxiv.org/abs/2410.16246</link><description>https://arxiv.org/abs/2410.16246&lt;br /&gt;RepoGraph is a plug-in module designed to enhance AI software engineering by providing repository-level code understanding and navigation, significantly improving the performance of various coding methods in AI applications.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Can Knowledge Editing Really Correct Hallucinations?</title><link>https://arxiv.org/abs/2410.16251</link><description>https://arxiv.org/abs/2410.16251&lt;br /&gt;RepoGraph is a plug-in module that enhances AI software engineering by providing a repository-level code graph structure for better context management and interaction, significantly improving performance across various coding methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution</title><link>https://arxiv.org/abs/2410.16256</link><description>https://arxiv.org/abs/2410.16256&lt;br /&gt;RepoGraph is a plug-in module designed to enhance AI software engineering by providing repository-level code structure and guidance, significantly improving performance in managing and interacting with code repositories, and achieving state-of-the-art results in the SWE-bench framework.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph</title><link>https://arxiv.org/abs/2410.14684</link><description>https://arxiv.org/abs/2410.14684&lt;br /&gt;RepoGraph is a plug-in module designed to enhance AI software engineering by providing repository-level structure and management, thereby improving the performance of code generation tasks in LLMs.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>How to Evaluate Reward Models for RLHF</title><link>https://arxiv.org/abs/2410.14872</link><description>https://arxiv.org/abs/2410.14872&lt;br /&gt;ChitroJera is a new visual question answering dataset designed for Bangla, comprising over 15,000 samples aimed at addressing the lack of culturally relevant benchmarks in this low-resource language, and demonstrating the effectiveness of dual-encoder models and large language models in VQA tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Towards Safer Heuristics With XPlain</title><link>https://arxiv.org/abs/2410.15086</link><description>https://arxiv.org/abs/2410.15086&lt;br /&gt;The paper introduces a Generalized Probabilistic Attention Mechanism (GPAM) within Transformer architectures, which effectively addresses issues of rank-collapse and gradient vanishing associated with traditional attention mechanisms, thereby enhancing performance in natural language processing tasks like language modeling and machine translation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>On Designing Effective RL Reward at Training Time for LLM Reasoning</title><link>https://arxiv.org/abs/2410.15115</link><description>https://arxiv.org/abs/2410.15115&lt;br /&gt;The paper presents the Generalized Probabilistic Attention Mechanism (GPAM), a novel attention mechanism for Transformers that mitigates issues of rank-collapse and gradient vanishing while demonstrating improved performance in natural language processing tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Evaluation Of P300 Speller Performance Using Large Language Models Along With Cross-Subject Training</title><link>https://arxiv.org/abs/2410.15161</link><description>https://arxiv.org/abs/2410.15161&lt;br /&gt;This paper introduces the generalized probabilistic attention mechanism (GPAM) in Transformers, which effectively resolves the issues of rank-collapse and gradient vanishing in conventional attention mechanisms while demonstrating superior performance in NLP tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Explaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective for Molecular Property Prediction</title><link>https://arxiv.org/abs/2410.15165</link><description>https://arxiv.org/abs/2410.15165&lt;br /&gt;This paper introduces a generalized probabilistic attention mechanism (GPAM) for Transformers, which addresses the rank-collapse and gradient vanishing issues in conventional attention mechanisms by allowing negative attention scores while preserving a fixed sum, demonstrating its effectiveness through theoretical and empirical validation in natural language processing tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>The Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse</title><link>https://arxiv.org/abs/2410.15182</link><description>https://arxiv.org/abs/2410.15182&lt;br /&gt;The paper introduces the generalized probabilistic attention mechanism (GPAM) as a novel attention mechanism for Transformers, which addresses the issues of rank-collapse and gradient vanishing while demonstrating superior performance in natural language processing tasks compared to conventional attention mechanisms.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Chasing Random: Instruction Selection Strategies Fail to Generalize</title><link>https://arxiv.org/abs/2410.15225</link><description>https://arxiv.org/abs/2410.15225&lt;br /&gt;This paper introduces a Generalized Probabilistic Attention Mechanism (GPAM) within the Transformer architecture that effectively mitigates the issues of rank-collapse and gradient vanishing, improving performance in natural language processing tasks such as language modeling and neural machine translation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep Secret or Forget Knowledge?</title><link>https://arxiv.org/abs/2410.15267</link><description>https://arxiv.org/abs/2410.15267&lt;br /&gt;This paper introduces a novel generalized probabilistic attention mechanism (GPAM) within the Transformer architecture, which addresses the rank-collapse and gradient vanishing issues of conventional attention mechanisms, and demonstrates its superiority in various natural language processing tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>TAGExplainer: Narrating Graph Explanations for Text-Attributed Graph Learning Models</title><link>https://arxiv.org/abs/2410.15268</link><description>https://arxiv.org/abs/2410.15268&lt;br /&gt;This paper introduces the Generalized Probabilistic Attention Mechanism (GPAM) for Transformers, which addresses rank-collapse and gradient vanishing issues in conventional attention mechanisms by allowing negative attention scores while maintaining a fixed total sum, and demonstrates its effectiveness in natural language processing tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Large Language Models for Autonomous Driving (LLM4AD): Concept, Benchmark, Simulation, and Real-Vehicle Experiment</title><link>https://arxiv.org/abs/2410.15281</link><description>https://arxiv.org/abs/2410.15281&lt;br /&gt;The paper presents a Generalized Probabilistic Attention Mechanism (GPAM) in Transformers, which effectively addresses rank-collapse and gradient vanishing issues associated with conventional attention mechanisms, enhancing performance in natural language processing tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Who is Undercover? Guiding LLMs to Explore Multi-Perspective Team Tactic in the Game</title><link>https://arxiv.org/abs/2410.15311</link><description>https://arxiv.org/abs/2410.15311&lt;br /&gt;This paper introduces a generalized probabilistic attention mechanism (GPAM) in Transformers that effectively addresses issues of rank-collapse and gradient vanishing while enhancing performance in natural language processing tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>EPIC: Efficient Position-Independent Context Caching for Serving Large Language Models</title><link>https://arxiv.org/abs/2410.15332</link><description>https://arxiv.org/abs/2410.15332&lt;br /&gt;This paper introduces a generalized probabilistic attention mechanism (GPAM) that addresses the issues of rank-collapse and gradient vanishing in traditional transformer attention mechanisms, enhancing performance in natural language processing tasks like language modeling and neural machine translation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>CompAct: Compressed Activations for Memory-Efficient LLM Training</title><link>https://arxiv.org/abs/2410.15352</link><description>https://arxiv.org/abs/2410.15352&lt;br /&gt;The paper introduces a novel generalized probabilistic attention mechanism (GPAM) for Transformers, which addresses the issues of rank-collapse and gradient vanishing in conventional attention mechanisms, thereby demonstrating superior performance in natural language processing tasks such as language modeling and translation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Faster-GCG: Efficient Discrete Optimization Jailbreak Attacks against Aligned Large Language Models</title><link>https://arxiv.org/abs/2410.15362</link><description>https://arxiv.org/abs/2410.15362&lt;br /&gt;The paper introduces a Generalized Probabilistic Attention Mechanism (GPAM) for Transformers, which addresses the challenges of rank-collapse and gradient vanishing in conventional attention mechanisms while enhancing performance in natural language processing tasks such as language modeling and neural machine translation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>IPO: Interpretable Prompt Optimization for Vision-Language Models</title><link>https://arxiv.org/abs/2410.15397</link><description>https://arxiv.org/abs/2410.15397&lt;br /&gt;The paper introduces a novel generalized probabilistic attention mechanism (GPAM) that addresses the rank-collapse and gradient vanishing issues in Transformer architecture, showcasing its effectiveness through theoretical analysis and empirical validation in natural language processing tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training</title><link>https://arxiv.org/abs/2410.15460</link><description>https://arxiv.org/abs/2410.15460&lt;br /&gt;The paper introduces a novel generalized probabilistic attention mechanism (GPAM) for Transformers, addressing rank-collapse and gradient vanishing issues in conventional attention mechanisms while providing empirical evidence of its superiority in natural language processing tasks such as language modeling and neural machine translation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Mitigating Forgetting in LLM Supervised Fine-Tuning and Preference Learning</title><link>https://arxiv.org/abs/2410.15483</link><description>https://arxiv.org/abs/2410.15483&lt;br /&gt;This paper introduces the Generalized Probabilistic Attention Mechanism (GPAM) in Transformers, which effectively tackles the issues of rank-collapse and gradient vanishing inherent in conventional attention mechanisms, demonstrating improved performance in natural language processing tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Exploring Curriculum Learning for Vision-Language Tasks: A Study on Small-Scale Multimodal Training</title><link>https://arxiv.org/abs/2410.15509</link><description>https://arxiv.org/abs/2410.15509&lt;br /&gt;This paper introduces a Generalized Probabilistic Attention Mechanism (GPAM) in Transformers, addressing the issues of rank-collapse and gradient vanishing while improving performance in natural language processing tasks such as language modeling and machine translation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Pruning Foundation Models for High Accuracy without Retraining</title><link>https://arxiv.org/abs/2410.15567</link><description>https://arxiv.org/abs/2410.15567&lt;br /&gt;The paper introduces a generalized probabilistic attention mechanism (GPAM) and its dual-attention implementation for Transformers, effectively mitigating issues related to rank-collapse and gradient vanishing, and demonstrating improved performance in natural language processing tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>OpenMU: Your Swiss Army Knife for Music Understanding</title><link>https://arxiv.org/abs/2410.15573</link><description>https://arxiv.org/abs/2410.15573&lt;br /&gt;The paper introduces a generalized probabilistic attention mechanism (GPAM) for Transformers, designed to address the issues of rank-collapse and gradient vanishing inherent in conventional attention mechanisms, while demonstrating improvements in natural language processing tasks such as language modeling and translation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Generalized Probabilistic Attention Mechanism in Transformers</title><link>https://arxiv.org/abs/2410.15578</link><description>https://arxiv.org/abs/2410.15578&lt;br /&gt;This paper introduces the Generalized Probabilistic Attention Mechanism (GPAM) in Transformers, which effectively addresses the issues of rank-collapse and gradient vanishing in conventional attention mechanisms, enhancing the performance of natural language processing tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias</title><link>https://arxiv.org/abs/2212.10678</link><description>https://arxiv.org/abs/2212.10678&lt;br /&gt;This paper presents FACTUAL, a Counterfactual Augmented Calibration Network designed to mitigate biases in stance detection by calibrating predictions from Large Language Models (LLMs) and utilizing counterfactual augmented data to enhance debiasing and generalizability.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Benchmarking Machine Translation with Cultural Awareness</title><link>https://arxiv.org/abs/2305.14328</link><description>https://arxiv.org/abs/2305.14328&lt;br /&gt;The paper introduces FACTUAL, a Counterfactual Augmented Calibration Network designed to mitigate biases in Large Language Models (LLMs) during stance detection, thereby enhancing their performance and generalizability across different topics.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Persona-aware Generative Model for Code-mixed Language</title><link>https://arxiv.org/abs/2309.02915</link><description>https://arxiv.org/abs/2309.02915&lt;br /&gt;This paper presents FACTUAL, a Counterfactual Augmented Calibration Network designed to mitigate biases in stance detection by calibrating stance predictions from Large Language Models (LLMs) and using counterfactual augmented data for improved generalizability.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Machine-assisted quantitizing designs: augmenting humanities and social sciences with artificial intelligence</title><link>https://arxiv.org/abs/2309.14379</link><description>https://arxiv.org/abs/2309.14379&lt;br /&gt;The paper proposes a Counterfactual Augmented Calibration Network (FACTUAL) to mitigate biases in stance detection predictions made by Large Language Models (LLMs), addressing spurious correlations that impair performance and enhancing generalizability through counterfactual augmented data.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators</title><link>https://arxiv.org/abs/2311.07879</link><description>https://arxiv.org/abs/2311.07879&lt;br /&gt;This paper presents the Counterfactual Augmented Calibration Network (FACTUAL), a novel approach to mitigate biases in stance detection performed by Large Language Models (LLMs) by implementing counterfactual augmented data and a calibration network, resulting in improved stance detection performance and reduced bias.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Self-Contradictory Reasoning Evaluation and Detection</title><link>https://arxiv.org/abs/2311.09603</link><description>https://arxiv.org/abs/2311.09603&lt;br /&gt;This paper introduces FACTUAL, a Counterfactual Augmented Calibration Network designed to mitigate biases in stance detection by calibrating the stance predictions of Large Language Models (LLMs) and enhancing generalizability through counterfactual augmented data.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Downstream Trade-offs of a Family of Text Watermarks</title><link>https://arxiv.org/abs/2311.09816</link><description>https://arxiv.org/abs/2311.09816&lt;br /&gt;This paper presents a Counterfactual Augmented Calibration Network (FACTUAL) to mitigate biases in stance detection performed by Large Language Models (LLMs), improving their performance by addressing spurious correlations inherent in their training data.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Latent Skill Discovery for Chain-of-Thought Reasoning</title><link>https://arxiv.org/abs/2312.04684</link><description>https://arxiv.org/abs/2312.04684&lt;br /&gt;This paper introduces the Counterfactual Augmented Calibration Network (FACTUAL) to mitigate biases in stance detection performed by Large Language Models (LLMs), demonstrating that addressing these biases can significantly improve detection performance in both targeted and zero-shot tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Aligning Translation-Specific Understanding to General Understanding in Large Language Models</title><link>https://arxiv.org/abs/2401.05072</link><description>https://arxiv.org/abs/2401.05072&lt;br /&gt;This paper presents the Counterfactual Augmented Calibration Network (FACTUAL), a novel approach aimed at mitigating biases in stance detection by calibrating bias in the predictions of Large Language Models (LLMs) using counterfactual augmented data, ultimately improving their performance in stance detection tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models</title><link>https://arxiv.org/abs/2401.05618</link><description>https://arxiv.org/abs/2401.05618&lt;br /&gt;The paper proposes a Counterfactual Augmented Calibration Network (FACTUAL) to mitigate biases in stance detection by calibrating biases in Large Language Models (LLMs), thereby enhancing performance and generalization capabilities across different tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning</title><link>https://arxiv.org/abs/2401.17686</link><description>https://arxiv.org/abs/2401.17686&lt;br /&gt;This paper presents the Counterfactual Augmented Calibration Network (FACTUAL) to effectively mitigate biases in stance detection by calibrating LLMs and enhancing their performance through counterfactual augmented data, addressing the issues of sentiment-stance spurious correlations.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection</title><link>https://arxiv.org/abs/2402.03744</link><description>https://arxiv.org/abs/2402.03744&lt;br /&gt;The paper presents FACTUAL, a Counterfactual Augmented Calibration Network designed to mitigate biases in large language models (LLMs) during stance detection, improving their overall performance and generalizability in predicting stances by calibrating potential bias through counterfactual augmented data.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric</title><link>https://arxiv.org/abs/2402.06900</link><description>https://arxiv.org/abs/2402.06900&lt;br /&gt;This paper introduces a Counterfactual Augmented Calibration Network (FACTUAL) to mitigate biases in stance detection by Large Language Models (LLMs), demonstrating that enhanced calibration can improve detection performance and generalization in biases related to sentiment and preference.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Neural machine translation of clinical procedure codes for medical diagnosis and uncertainty quantification</title><link>https://arxiv.org/abs/2402.10940</link><description>https://arxiv.org/abs/2402.10940&lt;br /&gt;This paper introduces a Counterfactual Augmented Calibration Network (FACTUAL) to mitigate biases in stance detection by Large Language Models (LLMs), demonstrating that addressing these biases enhances performance and generalizability in stance detection tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Intervention Lens: from Representation Surgery to String Counterfactuals</title><link>https://arxiv.org/abs/2402.11355</link><description>https://arxiv.org/abs/2402.11355&lt;br /&gt;The paper introduces a Counterfactual Augmented Calibration Network (FACTUAL) designed to mitigate biases in stance detection by calibrating bias in predictions and improving generalizability through counterfactual augmented data.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>SymBa: Symbolic Backward Chaining for Structured Natural Language Reasoning</title><link>https://arxiv.org/abs/2402.12806</link><description>https://arxiv.org/abs/2402.12806&lt;br /&gt;The paper introduces Counterfactual Augmented Calibration Network (FACTUAL) to mitigate biases in Large Language Models (LLMs) during stance detection, demonstrating that biased stances adversely affect detection performance and showcasing FACTUAL's effectiveness in achieving state-of-the-art results through debiasing techniques.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Survey on Knowledge Distillation of Large Language Models</title><link>https://arxiv.org/abs/2402.13116</link><description>https://arxiv.org/abs/2402.13116&lt;br /&gt;This paper introduces the Counterfactual Augmented Calibration Network (FACTUAL), which aims to mitigate biases in stance detection by calibrating bias in Large Language Models (LLMs) using counterfactual augmented data, improving both their bias mitigation and generalization performance.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>CriticEval: Evaluating Large Language Model as Critic</title><link>https://arxiv.org/abs/2402.13764</link><description>https://arxiv.org/abs/2402.13764&lt;br /&gt;This paper introduces the Counterfactual Augmented Calibration Network (FACTUAL) to mitigate biases in Large Language Models (LLMs) during stance detection, demonstrating that addressing these biases can significantly enhance performance and generalization in stance recognition tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Mitigating Biases of Large Language Models in Stance Detection with Counterfactual Augmented Calibration</title><link>https://arxiv.org/abs/2402.14296</link><description>https://arxiv.org/abs/2402.14296&lt;br /&gt;The paper introduces the Counterfactual Augmented Calibration Network (FACTUAL) to mitigate biases in stance detection by calibrating bias in predictions of Large Language Models (LLMs) while using counterfactual augmented data for improved generalization.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Beware of Words: Evaluating the Lexical Diversity of Conversational LLMs using ChatGPT as Case Study</title><link>https://arxiv.org/abs/2402.15518</link><description>https://arxiv.org/abs/2402.15518&lt;br /&gt;This paper investigates the inner workings of Mixture-of-Experts (MoE) in large language models, revealing insights about parameter activation, expert selection mechanisms, and the diversity of experts across layers, ultimately providing guidance for future MoE architectures and implementations.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization</title><link>https://arxiv.org/abs/2403.00067</link><description>https://arxiv.org/abs/2403.00067&lt;br /&gt;This paper investigates the inner workings of Mixture-of-Experts (MoE) models in large language contexts, revealing insights into neuron behavior, expert selection, and the dynamics of expert diversity across layers, while also providing practical recommendations for MoE practitioners.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Log Probabilities Are a Reliable Estimate of Semantic Plausibility in Base and Instruction-Tuned Language Models</title><link>https://arxiv.org/abs/2403.14859</link><description>https://arxiv.org/abs/2403.14859&lt;br /&gt;This paper investigates the inner workings of Mixture-of-Experts (MoE) architectures in large language models, revealing insights into expert behavior, router selection mechanisms, and the increase in expert diversity, which can guide future research and practical applications in MoE frameworks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>GPT-4 Understands Discourse at Least as Well as Humans Do</title><link>https://arxiv.org/abs/2403.17196</link><description>https://arxiv.org/abs/2403.17196&lt;br /&gt;This paper investigates the internal mechanisms of Mixture-of-Experts (MoE) in large language models, revealing how neuron activity resembles expert behavior, examining the router's selection dynamics, and suggesting design improvements based on observed expert diversity.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Bi-consolidating Model for Joint Relational Triple Extraction</title><link>https://arxiv.org/abs/2404.03881</link><description>https://arxiv.org/abs/2404.03881&lt;br /&gt;This paper investigates the inner workings of Mixture-of-Experts (MoE) in large language models, revealing key observations about neuron behavior, router performance, and expert diversity, while providing guidance for optimizing MoE architectures.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Claim Check-Worthiness Detection: How Well do LLMs Grasp Annotation Guidelines?</title><link>https://arxiv.org/abs/2404.12174</link><description>https://arxiv.org/abs/2404.12174&lt;br /&gt;This paper investigates the inner workings of Mixture-of-Experts (MoE) in large language models, revealing findings about expert selection, diversity, and implications for future MoE research and architecture design.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models</title><link>https://arxiv.org/abs/2405.01943</link><description>https://arxiv.org/abs/2405.01943&lt;br /&gt;This paper investigates the inner workings of Mixture-of-Experts (MoE) in large language models, revealing insights about expert selection, behavior, and the unique contributions of neurons within the architecture, and offering practical suggestions for MoE implementation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>MedReadMe: A Systematic Study for Fine-grained Sentence Readability in Medical Domain</title><link>https://arxiv.org/abs/2405.02144</link><description>https://arxiv.org/abs/2405.02144&lt;br /&gt;This paper explores the inner workings of Mixture-of-Experts (MoE) in large language models, investigating their parametric and behavioral features, and providing insights into expert selection and diversity across model layers.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>DOLOMITES: Domain-Specific Long-Form Methodical Tasks</title><link>https://arxiv.org/abs/2405.05938</link><description>https://arxiv.org/abs/2405.05938&lt;br /&gt;This paper investigates the inner workings of Mixture-of-Experts (MoE) architectures in large language models, revealing key behavioral and parametric features, including expert selection processes and the impact of layer depth on expert diversity.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys</title><link>https://arxiv.org/abs/2405.19323</link><description>https://arxiv.org/abs/2405.19323&lt;br /&gt;This paper investigates the inner workings of Mixture-of-Experts (MoE) architectures in large language models, revealing insights on expert selection, neuron behavior, and recommendations for future MoE implementations.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Improving code-mixed hate detection by native sample mixing: A case study for Hindi-English code-mixed scenario</title><link>https://arxiv.org/abs/2405.20755</link><description>https://arxiv.org/abs/2405.20755&lt;br /&gt;This paper explores the inner workings of Mixture-of-Experts (MoE) in large language models, revealing behavioral features and providing insights into expert selection and diversity in neural architectures.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Persian Homograph Disambiguation: Leveraging ParsBERT for Enhanced Sentence Understanding with a Novel Word Disambiguation Dataset</title><link>https://arxiv.org/abs/2406.00028</link><description>https://arxiv.org/abs/2406.00028&lt;br /&gt;This paper investigates the internal mechanisms of Mixture-of-Experts (MoE) architectures in large language models, revealing unique observations about neuron behavior, expert selection, and diversity across layers, while providing insights and suggestions for future practitioners in MoE design.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Analyzing Social Biases in Japanese Large Language Models</title><link>https://arxiv.org/abs/2406.02050</link><description>https://arxiv.org/abs/2406.02050&lt;br /&gt;This paper explores the mechanisms and characteristics of Mixture-of-Experts (MoE) in large language models, revealing insights into the behavior of neurons as experts, the routing processes, and implications for model design.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Deconstructing The Ethics of Large Language Models from Long-standing Issues to New-emerging Dilemmas: A Survey</title><link>https://arxiv.org/abs/2406.05392</link><description>https://arxiv.org/abs/2406.05392&lt;br /&gt;This paper investigates the inner workings of Mixture-of-Experts (MoE) in large language models, revealing insights into its parametric and behavioral features while offering guidelines for enhancing router design and expert allocation in MoE architecture.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Non-autoregressive Generation Framework for End-to-End Simultaneous Speech-to-Speech Translation</title><link>https://arxiv.org/abs/2406.06937</link><description>https://arxiv.org/abs/2406.06937&lt;br /&gt;This paper explores the inner workings of Mixture-of-Experts (MoE) architectures in large language models, revealing insights into their parametric behaviors and highlighting the modularization challenges and performance trade-offs associated with this design.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Concentrate Attention: Towards Domain-Generalizable Prompt Optimization for Language Models</title><link>https://arxiv.org/abs/2406.10584</link><description>https://arxiv.org/abs/2406.10584&lt;br /&gt;This paper explores the inner workings of Mixture-of-Experts (MoE) architecture in large language models, revealing key observations about parametric and behavioral features that highlight how neurons function as experts and how routing mechanisms operate within these models.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>StrucText-Eval: Evaluating Large Language Model's Reasoning Ability in Structure-Rich Text</title><link>https://arxiv.org/abs/2406.10621</link><description>https://arxiv.org/abs/2406.10621&lt;br /&gt;This paper explores the inner workings of Mixture-of-Experts (MoE) architectures in large language models, revealing insights into parametric and behavioral features, including how neurons function as fine-grained experts and the selection process of the router for activating experts.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation</title><link>https://arxiv.org/abs/2406.13114</link><description>https://arxiv.org/abs/2406.13114&lt;br /&gt;This paper investigates the mechanisms and performance of Mixture-of-Experts (MoE) architectures in large language models, revealing key behavioral features, the role of routers in selecting experts, and providing insights for optimizing MoE implementations.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for Gaussian-Noise-free Text-Image Corruption and Evaluation</title><link>https://arxiv.org/abs/2406.16320</link><description>https://arxiv.org/abs/2406.16320&lt;br /&gt;This paper investigates the internal mechanisms and effectiveness of Mixture-of-Experts (MoE) architectures in large language models, revealing insights into expert selection, the behavior of neurons as fine-grained experts, and suggestions for improving router designs and expert allocation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A Closer Look into Mixture-of-Experts in Large Language Models</title><link>https://arxiv.org/abs/2406.18219</link><description>https://arxiv.org/abs/2406.18219&lt;br /&gt;This paper investigates the mechanisms of Mixture-of-Experts (MoE) architectures in large language models, revealing insights into their parametric and behavioral features, as well as offering practical guidance for their implementation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems</title><link>https://arxiv.org/abs/2406.19170</link><description>https://arxiv.org/abs/2406.19170&lt;br /&gt;The CLC-UKET dataset is developed to benchmark case outcome prediction for the UK Employment Tribunal by utilizing large language models for automatic legal annotation and demonstrating that finetuned transformer models outperform zero-shot and few-shot models in predicting case outcomes.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees</title><link>https://arxiv.org/abs/2407.00499</link><description>https://arxiv.org/abs/2407.00499&lt;br /&gt;The CLC-UKET dataset facilitates the prediction of case outcomes in the UK Employment Tribunal by utilizing large language models for automatic annotation of approximately 19,000 cases, providing a benchmark for comparison with human predictions and enhancing access to justice.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Language Model Alignment in Multilingual Trolley Problems</title><link>https://arxiv.org/abs/2407.02273</link><description>https://arxiv.org/abs/2407.02273&lt;br /&gt;The CLC-UKET dataset facilitates case outcome prediction for the UK Employment Tribunal by leveraging a large language model for automatic annotation of around 19,000 cases, resulting in superior performance from finetuned transformer models compared to zero-shot and few-shot approaches.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Data, Data Everywhere: A Guide for Pretraining Dataset Construction</title><link>https://arxiv.org/abs/2407.06380</link><description>https://arxiv.org/abs/2407.06380&lt;br /&gt;The CLC-UKET dataset introduces a benchmark for predicting case outcomes in the UK Employment Tribunal by leveraging a large language model for automatic annotation, encompassing approximately 19,000 cases and their comprehensive legal metadata to enhance access to justice and facilitate case outcome prediction tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts</title><link>https://arxiv.org/abs/2407.09447</link><description>https://arxiv.org/abs/2407.09447&lt;br /&gt;The CLC-UKET dataset establishes a benchmark for predicting case outcomes in the UK Employment Tribunal using a large language model for automatic annotation of 19,000 cases, facilitating comparison of model performance in legal predictions.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse Mixture-of-Experts</title><link>https://arxiv.org/abs/2407.09590</link><description>https://arxiv.org/abs/2407.09590&lt;br /&gt;The CLC-UKET dataset provides a benchmark for predicting case outcomes in the UK Employment Tribunal, utilizing a large language model for automated annotation of approximately 19,000 cases, which aids in enhancing access to justice through effective model comparisons.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>SPINACH: SPARQL-Based Information Navigation for Challenging Real-World Questions</title><link>https://arxiv.org/abs/2407.11417</link><description>https://arxiv.org/abs/2407.11417&lt;br /&gt;The CLC-UKET dataset is created to benchmark case outcome prediction in the UK Employment Tribunal by employing a large language model for automatic annotations, which includes 19,000 cases and aims to enhance the accessibility and efficiency of legal dispute resolution processes.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Truth is Universal: Robust Detection of Lies in LLMs</title><link>https://arxiv.org/abs/2407.12831</link><description>https://arxiv.org/abs/2407.12831&lt;br /&gt;The CLC-UKET dataset is developed to benchmark case outcome prediction in the UK Employment Tribunal using automatic annotation via large language models, consisting of approximately 19,000 cases with comprehensive legal annotations.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?</title><link>https://arxiv.org/abs/2407.15711</link><description>https://arxiv.org/abs/2407.15711&lt;br /&gt;The CLC-UKET paper presents the CLC-UKET dataset for predicting case outcomes in the UK Employment Tribunal using a large language model for automatic annotation, evaluating multiple models to benchmark performance in legal prediction tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models</title><link>https://arxiv.org/abs/2407.16470</link><description>https://arxiv.org/abs/2407.16470&lt;br /&gt;The CLC-UKET dataset is developed to benchmark case outcome prediction in the UK Employment Tribunal by leveraging a large language model for automatic annotation, resulting in a dataset of approximately 19,000 cases and comprehensive legal metadata.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation</title><link>https://arxiv.org/abs/2407.18698</link><description>https://arxiv.org/abs/2407.18698&lt;br /&gt;The study introduces the CLC-UKET dataset, comprising approximately 19,000 UK Employment Tribunal cases and their metadata, to benchmark case outcome prediction using large language models, revealing that fine-tuned transformer models outperform zero-shot and few-shot approaches.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>mbrs: A Library for Minimum Bayes Risk Decoding</title><link>https://arxiv.org/abs/2408.04167</link><description>https://arxiv.org/abs/2408.04167&lt;br /&gt;The CLC-UKET dataset provides a benchmark for predicting case outcomes in the UK Employment Tribunal, utilizing a large language model for automatic annotation and facilitating comparison between human predictions and various model performances.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection</title><link>https://arxiv.org/abs/2408.04284</link><description>https://arxiv.org/abs/2408.04284&lt;br /&gt;The CLC-UKET dataset is developed to benchmark case outcome prediction for the UK Employment Tribunal, employing a large language model for automatic annotation of approximately 19,000 cases and assessing the performance of various prediction models.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Persona is a Double-edged Sword: Mitigating the Negative Impact of Role-playing Prompts in Zero-shot Reasoning Tasks</title><link>https://arxiv.org/abs/2408.08631</link><description>https://arxiv.org/abs/2408.08631&lt;br /&gt;The CLC-UKET dataset is created to benchmark case outcome prediction in the UK Employment Tribunal by utilizing a large language model for automatic annotation, comprising around 19,000 cases with comprehensive legal data, which facilitates the evaluation of various prediction models including fine-tuned and zero-shot approaches.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>BnSentMix: A Diverse Bengali-English Code-Mixed Dataset for Sentiment Analysis</title><link>https://arxiv.org/abs/2408.08964</link><description>https://arxiv.org/abs/2408.08964&lt;br /&gt;The CLC-UKET dataset provides a benchmark for predicting case outcomes in the UK Employment Tribunal by employing large language models for automatic annotation of approximately 19,000 cases, and it demonstrates that finetuned transformer models outperform zero-shot and few-shot LLMs in this prediction task.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Crafting Tomorrow's Headlines: Neural News Generation and Detection in English, Turkish, Hungarian, and Persian</title><link>https://arxiv.org/abs/2408.10724</link><description>https://arxiv.org/abs/2408.10724&lt;br /&gt;The CLC-UKET dataset provides a benchmark for predicting case outcomes in the UK Employment Tribunal by employing a large language model for automatic annotation, resulting in approximately 19,000 cases with comprehensive legal metadata and annotations to facilitate multi-class outcome prediction tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>How Well Do LLMs Handle Cantonese? Benchmarking Cantonese Capabilities of Large Language Models</title><link>https://arxiv.org/abs/2408.16756</link><description>https://arxiv.org/abs/2408.16756&lt;br /&gt;The CLC-UKET dataset is developed to benchmark case outcome prediction in the UK Employment Tribunal by leveraging a large language model for automatic annotation of approximately 19,000 cases, ultimately facilitating improved performance in legal predictions based on empirical findings.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>TinyAgent: Function Calling at the Edge</title><link>https://arxiv.org/abs/2409.00608</link><description>https://arxiv.org/abs/2409.00608&lt;br /&gt;The CLC-UKET dataset is developed to benchmark case outcome prediction in the UK Employment Tribunal by utilizing a large language model for automatic annotation of approximately 19,000 cases, enabling multi-class prediction tasks and indicating that finetuned transformer models outperform LLMs in this context.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A corpus-based investigation of pitch contours of monosyllabic words in conversational Taiwan Mandarin</title><link>https://arxiv.org/abs/2409.07891</link><description>https://arxiv.org/abs/2409.07891&lt;br /&gt;The CLC-UKET dataset introduces a benchmark for predicting case outcomes in the UK Employment Tribunal, utilizing a large language model for automatic annotation of around 19,000 cases and demonstrating that finetuned transformer models outperform LLMs in this task.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal</title><link>https://arxiv.org/abs/2409.08098</link><description>https://arxiv.org/abs/2409.08098&lt;br /&gt;The CLC-UKET dataset facilitates the benchmarking of case outcome prediction in the UK Employment Tribunal by utilizing a large language model for automatic annotation, encompassing approximately 19,000 cases and comprehensive legal metadata to enhance access to justice.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>On the Role of Context in Reading Time Prediction</title><link>https://arxiv.org/abs/2409.08160</link><description>https://arxiv.org/abs/2409.08160&lt;br /&gt;This paper investigates how well language models capture long-range contextual information using perturbation methods and demonstrates that different model architectures show varying capacities for encoding complex sequences over long contexts.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation</title><link>https://arxiv.org/abs/2409.12941</link><description>https://arxiv.org/abs/2409.12941&lt;br /&gt;This paper investigates how well contextualized representations in neural autoregressive language models encode long-range context, revealing that different architectures and training configurations significantly affect the ability to handle long-range dependencies, which impacts downstream task performance.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Do Large Language Models Need a Content Delivery Network?</title><link>https://arxiv.org/abs/2409.13761</link><description>https://arxiv.org/abs/2409.13761&lt;br /&gt;This paper investigates how well neural autoregressive language models encode long-range contextual information, highlighting differences in performance related to representation geometry and contextualization across various model architectures and training configurations.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Rephrase and Contrast: Fine-Tuning Language Models for Enhanced Understanding of Communication and Computer Networks</title><link>https://arxiv.org/abs/2409.19007</link><description>https://arxiv.org/abs/2409.19007&lt;br /&gt;This paper investigates how contextual representations in neural autoregressive language models capture long-range context, showing that differing perplexity can lead to varied performance in downstream tasks and suggesting improvements for encoding high-complexity sequences.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>INC-Math: Integrating Natural Language and Code for Enhanced Mathematical Reasoning in Large Language Models</title><link>https://arxiv.org/abs/2409.19381</link><description>https://arxiv.org/abs/2409.19381&lt;br /&gt;This paper investigates the capability of various neural autoregressive language models to encode long-range context by analyzing contextual representations and their performance on downstream tasks, revealing significant differences in how models capture long-range dependencies.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models</title><link>https://arxiv.org/abs/2409.19667</link><description>https://arxiv.org/abs/2409.19667&lt;br /&gt;This paper investigates how contextualized representations in neural autoregressive language models, particularly those spanning long-range contexts, affect downstream task performance, revealing significant differences based on representation geometry and model architectures.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models</title><link>https://arxiv.org/abs/2410.02355</link><description>https://arxiv.org/abs/2410.02355&lt;br /&gt;This paper analyzes how effectively neural autoregressive language models, particularly decoder-only Transformers, encode long-range contexts by employing a perturbation setup and a metric to assess the contextual representation geometry, revealing insights about model performance differences driven by long-range context capacity.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs</title><link>https://arxiv.org/abs/2410.03071</link><description>https://arxiv.org/abs/2410.03071&lt;br /&gt;This paper investigates how well contextualized representations in neural autoregressive language models encode long-range context, revealing significant differences in task performance despite similar perplexity metrics depending on the degree of contextualization.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>One2set + Large Language Model: Best Partners for Keyphrase Generation</title><link>https://arxiv.org/abs/2410.03421</link><description>https://arxiv.org/abs/2410.03421&lt;br /&gt;This study investigates the ability of neural autoregressive language models to encode long-range contextual information, revealing that different architectures and training configurations significantly affect performance on downstream tasks related to this encoding.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Efficient Few-shot Learning for Multi-label Classification of Scientific Documents with Many Classes</title><link>https://arxiv.org/abs/2410.05770</link><description>https://arxiv.org/abs/2410.05770&lt;br /&gt;This paper investigates the ability of neural autoregressive language models to encode long-range contextual information, revealing that models with different architectures and training configurations exhibit varying capacities for representing complex sequences and contextual relationships across thousands of tokens.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>When and Where Did it Happen? An Encoder-Decoder Model to Identify Scenario Context</title><link>https://arxiv.org/abs/2410.07567</link><description>https://arxiv.org/abs/2410.07567&lt;br /&gt;This paper examines the capability of neural autoregressive language models to encode long-range contextual information, demonstrating varying performance in downstream tasks based on the degree of contextualization achieved, particularly in reference to perplexity and representation geometry.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic Analysis of Annotators and Targets</title><link>https://arxiv.org/abs/2410.07991</link><description>https://arxiv.org/abs/2410.07991&lt;br /&gt;This paper investigates how well neural autoregressive language models, particularly decoder-only Transformers, encode long-range contextual information over several thousand tokens, revealing substantial differences in performance based on representation geometry and model architecture.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>On the token distance modeling ability of higher RoPE attention dimension</title><link>https://arxiv.org/abs/2410.08703</link><description>https://arxiv.org/abs/2410.08703&lt;br /&gt;This paper investigates how well contextualized representations in neural autoregressive language models capture long-range context, revealing significant differences in downstream task performance based on the degree of contextualization and suggesting ways to improve language models.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Effi-Code: Unleashing Code Efficiency in Language Models</title><link>https://arxiv.org/abs/2410.10209</link><description>https://arxiv.org/abs/2410.10209&lt;br /&gt;This paper analyzes how contextual representations in neural autoregressive language models, particularly decoder-only Transformers, encode long-range contexts spanning thousands of tokens, revealing differences in downstream performance linked to the contextualization of such content and suggesting future directions for model improvement.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning</title><link>https://arxiv.org/abs/2410.10360</link><description>https://arxiv.org/abs/2410.10360&lt;br /&gt;This paper investigates how well different neural autoregressive language models encode long-range contexts, revealing that varying perplexity can result in significantly different downstream task performance due to differences in representation geometry and contextualization of long-range content.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Translation Canvas: An Explainable Interface to Pinpoint and Analyze Translation Systems</title><link>https://arxiv.org/abs/2410.10861</link><description>https://arxiv.org/abs/2410.10861&lt;br /&gt;This paper investigates how well contextualized representations in neural autoregressive language models, particularly decoder-only Transformers, encode long-range context, revealing significant differences in performance based on the degree of contextualization achieved by various architectures and training configurations.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning</title><link>https://arxiv.org/abs/2410.11020</link><description>https://arxiv.org/abs/2410.11020&lt;br /&gt;This paper analyzes the capability of neural autoregressive language models to encode long-range contextual information, demonstrating that while models may achieve similar perplexity scores, their performance on downstream tasks can vary significantly based on their representation of long-range contexts.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Multi-round jailbreak attack on large language models</title><link>https://arxiv.org/abs/2410.11533</link><description>https://arxiv.org/abs/2410.11533&lt;br /&gt;This paper investigates how contextualized representations in neural language models capture long-range context, revealing that different model architectures and training configurations can significantly influence the model's ability to encode complex sequences and long-range dependencies.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability</title><link>https://arxiv.org/abs/2410.11786</link><description>https://arxiv.org/abs/2410.11786&lt;br /&gt;This paper analyzes how contextual representations in neural autoregressive language models capture long-range contexts, revealing significant variations in downstream task performance based on the models' ability to contextualize complex sequences.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>How much do contextualized representations encode long-range context?</title><link>https://arxiv.org/abs/2410.12292</link><description>https://arxiv.org/abs/2410.12292&lt;br /&gt;This paper investigates how contextual representations in neural autoregressive language models encode long-range contexts, revealing that different architectures exhibit varying capacities to handle high-complexity sequences, which influences their performance on downstream tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying Real-World Claims</title><link>https://arxiv.org/abs/2410.12377</link><description>https://arxiv.org/abs/2410.12377&lt;br /&gt;RACCooN is a versatile instructional video editing framework that utilizes a two-stage process of Video-to-Paragraph and Paragraph-to-Video to automatically generate narratives and allow users to edit videos through descriptive prompts, enhancing flexibility and reducing the need for labor-intensive input.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Building Better: Avoiding Pitfalls in Developing Language Resources when Data is Scarce</title><link>https://arxiv.org/abs/2410.12691</link><description>https://arxiv.org/abs/2410.12691&lt;br /&gt;RACCooN is a versatile video editing framework that utilizes an instructional pipeline to automatically describe video scenes and allow users to modify videos through generated narratives, significantly enhancing the flexibility and ease of editing personal or raw video content.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>In-context KV-Cache Eviction for LLMs via Attention-Gate</title><link>https://arxiv.org/abs/2410.12876</link><description>https://arxiv.org/abs/2410.12876&lt;br /&gt;RACCooN is a versatile instructional video editing framework that enables users to edit videos by first generating well-structured natural language descriptions of scenes and then allowing modifications based on user-defined changes, enhancing flexibility in video content manipulation.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>PromptExp: Multi-granularity Prompt Explanation of Large Language Models</title><link>https://arxiv.org/abs/2410.13073</link><description>https://arxiv.org/abs/2410.13073&lt;br /&gt;RACCooN is a versatile instructional video editing framework that automatically generates detailed narratives from input videos, allowing users to modify video content efficiently through a video-to-paragraph-to-video pipeline.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A new approach for fine-tuning sentence transformers for intent classification and out-of-scope detection tasks</title><link>https://arxiv.org/abs/2410.13649</link><description>https://arxiv.org/abs/2410.13649&lt;br /&gt;RACCooN is a versatile video editing framework that leverages a two-stage process to automatically generate natural language descriptions from video content and then allows users to edit videos based on these descriptions, enhancing flexibility and precision in video modifications.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Unconstrained Model Merging for Enhanced LLM Reasoning</title><link>https://arxiv.org/abs/2410.13699</link><description>https://arxiv.org/abs/2410.13699&lt;br /&gt;RACCooN is a versatile video editing framework that auto-generates narratives to simplify the editing process by converting videos into structured natural language descriptions, allowing users to modify content through a unified pipeline.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors</title><link>https://arxiv.org/abs/2410.13776</link><description>https://arxiv.org/abs/2410.13776&lt;br /&gt;RACCooN is a versatile instructional video editing framework that utilizes a two-stage process to automatically generate structured natural language descriptions of video content, allowing users to refine these descriptions for various modifications to the input video.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Paths-over-Graph: Knowledge Graph Empowered Large Language Model Reasoning</title><link>https://arxiv.org/abs/2410.14211</link><description>https://arxiv.org/abs/2410.14211&lt;br /&gt;RACCooN is a versatile instructional video editing framework that utilizes a two-stage pipeline to automatically generate narrations for videos and enable modifications based on user-customized texts, thus enhancing user interaction while simplifying video editing processes.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Institutional Grammar 2.0 Codebook</title><link>https://arxiv.org/abs/2008.08937</link><description>https://arxiv.org/abs/2008.08937&lt;br /&gt;RACCooN is a versatile instructional video editing framework that utilizes a two-stage process of automatically generating natural language descriptions from videos and then allowing users to refine these descriptions to guide modifications in the video, enhancing video editing capabilities without requiring labor-intensive textual prompts.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Enhancing Robustness of AI Offensive Code Generators via Data Augmentation</title><link>https://arxiv.org/abs/2306.05079</link><description>https://arxiv.org/abs/2306.05079&lt;br /&gt;RACCooN is a versatile framework for instructional video editing that automates the generation of natural language descriptions from videos and allows users to modify videos based on these descriptions through a two-stage process of Video-to-Paragraph and Paragraph-to-Video.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Reverse Stable Diffusion: What prompt was used to generate this image?</title><link>https://arxiv.org/abs/2308.01472</link><description>https://arxiv.org/abs/2308.01472&lt;br /&gt;RACCooN is a versatile instructional video editing framework that automatically generates narratives to facilitate video editing by converting videos into structured descriptions and allowing users to modify the content through a unified pipeline.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Tandem Transformers for Inference Efficient LLMs</title><link>https://arxiv.org/abs/2402.08644</link><description>https://arxiv.org/abs/2402.08644&lt;br /&gt;RACCooN is a versatile instructional video editing framework that supports multiple editing capabilities through a unified pipeline consisting of video-to-paragraph and paragraph-to-video stages, enabling users to generate natural language descriptions of video scenes and modify videos based on these descriptions.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines</title><link>https://arxiv.org/abs/2403.05846</link><description>https://arxiv.org/abs/2403.05846&lt;br /&gt;RACCooN is a versatile video-to-paragraph-to-video generative framework that automates video editing by generating natural language descriptions of video scenes and enabling users to refine these descriptions for modifications such as removal, addition, and alteration, enhancing flexibility and usability in video editing tasks.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering</title><link>https://arxiv.org/abs/2404.01476</link><description>https://arxiv.org/abs/2404.01476&lt;br /&gt;RACCooN is a versatile video editing framework that utilizes a two-stage process to automatically generate structured narratives from videos and enables users to refine these descriptions for various editing modifications, significantly improving the flexibility and ease of video content editing.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding</title><link>https://arxiv.org/abs/2404.07989</link><description>https://arxiv.org/abs/2404.07989&lt;br /&gt;RACCooN is a versatile instructional video editing framework that allows users to generate structured narratives from video content and edit videos through a unified video-to-paragraph-to-video workflow, enhancing flexibility and user adaptability in video editing.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Stepwise Alignment for Constrained Language Model Policy Optimization</title><link>https://arxiv.org/abs/2404.11049</link><description>https://arxiv.org/abs/2404.11049&lt;br /&gt;RACCooN is a user-friendly instructional video editing framework that automates the process of generating natural language descriptions for videos, enabling versatile modifications through a unified pipeline that includes video-to-paragraph and paragraph-to-video stages.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments</title><link>https://arxiv.org/abs/2405.07960</link><description>https://arxiv.org/abs/2405.07960&lt;br /&gt;RACCooN is a versatile instructional video editing framework that streamlines the process of video editing by automatically generating textual narratives from videos, allowing for user-guided modifications and enhancements through a unified video-to-paragraph-to-video pipeline.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Du-IN: Discrete units-guided mask modeling for decoding speech from Intracranial Neural signals</title><link>https://arxiv.org/abs/2405.11459</link><description>https://arxiv.org/abs/2405.11459&lt;br /&gt;RACCooN is a versatile framework for video editing that uses a two-stage process—Video-to-Paragraph (V2P) for auto-generating natural language descriptions of videos and Paragraph-to-Video (P2V) for refining these descriptions to guide video modifications, thereby simplifying video content editing without requiring labor-intensive user input.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>RACCooN: A Versatile Instructional Video Editing Framework with Auto-Generated Narratives</title><link>https://arxiv.org/abs/2405.18406</link><description>https://arxiv.org/abs/2405.18406&lt;br /&gt;RACCooN is a versatile instructional video editing framework that automates video-to-paragraph generation and allows users to refine narratives for various video editing modifications, enhancing flexibility in adapting personal videos to user specifications.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision Models For Video Captioning and Summarization</title><link>https://arxiv.org/abs/2405.20648</link><description>https://arxiv.org/abs/2405.20648&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to develop and evaluate attacker systems against voice anonymization, with participants training automatic speaker verification systems and measuring performance using equal error rate (EER) metrics.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Verbalized Machine Learning: Revisiting Machine Learning with Language Models</title><link>https://arxiv.org/abs/2406.04344</link><description>https://arxiv.org/abs/2406.04344&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to establish a competitive evaluation for developing attacker systems against voice anonymization technologies, providing datasets and a baseline for participants to gauge their systems' effectiveness based on equal error rates.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Bypass Back-propagation: Optimization-based Structural Pruning for Large Language Models via Policy Gradient</title><link>https://arxiv.org/abs/2406.10576</link><description>https://arxiv.org/abs/2406.10576&lt;br /&gt;The First VoicePrivacy Attacker Challenge focuses on developing and evaluating attacker systems against voice anonymization, providing datasets and a baseline for participants to create automatic speaker verification systems to assess their effectiveness against anonymization methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Exploring the Zero-Shot Capabilities of LLMs Handling Multiple Problems at once</title><link>https://arxiv.org/abs/2406.10786</link><description>https://arxiv.org/abs/2406.10786&lt;br /&gt;The First VoicePrivacy Attacker Challenge is an initiative aimed at developing systems to evaluate the effectiveness of voice anonymization techniques by assessing attacker systems through automatic speaker verification against various anonymization systems.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>IoT-Based Preventive Mental Health Using Knowledge Graphs and Standards for Better Well-Being</title><link>https://arxiv.org/abs/2406.13791</link><description>https://arxiv.org/abs/2406.13791&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to develop and evaluate systems that attack voice anonymization techniques by utilizing automatic speaker verification systems, with a focus on assessing their effectiveness against various anonymization systems.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>TAGLAS: An atlas of text-attributed graph datasets in the era of large graph and language models</title><link>https://arxiv.org/abs/2406.14683</link><description>https://arxiv.org/abs/2406.14683&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to develop and evaluate attacker systems against voice anonymization methods, providing training datasets and a baseline system for participants to enhance automatic speaker verification systems.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>DaLPSR: Leverage Degradation-Aligned Language Prompt for Real-World Image Super-Resolution</title><link>https://arxiv.org/abs/2406.16477</link><description>https://arxiv.org/abs/2406.16477&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to evaluate systems designed to break voice anonymization techniques by developing automatic speaker verification systems and assessing their performance against a set of anonymization systems using a specified equal error rate metric.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Retrieval-Enhanced Machine Learning: Synthesis and Opportunities</title><link>https://arxiv.org/abs/2407.12982</link><description>https://arxiv.org/abs/2407.12982&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to develop and evaluate automatic speaker verification systems that can assess the effectiveness of voice anonymization methods, with a focus on improving privacy and security in voice data.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks</title><link>https://arxiv.org/abs/2408.03615</link><description>https://arxiv.org/abs/2408.03615&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to develop evaluation metrics and attacker systems for voice anonymization protections, culminating in an event at ICASSP 2025 where participants will showcase their systems against various anonymization methods.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>LongVILA: Scaling Long-Context Visual Language Models for Long Videos</title><link>https://arxiv.org/abs/2408.10188</link><description>https://arxiv.org/abs/2408.10188&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to develop and evaluate attacker systems against voice anonymization as part of the VoicePrivacy initiative, providing datasets and a baseline system for participants to test their models.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities</title><link>https://arxiv.org/abs/2408.13296</link><description>https://arxiv.org/abs/2408.13296&lt;br /&gt;The First VoicePrivacy Attacker Challenge is organized to develop and evaluate systems that attack voice anonymization techniques, providing a framework for participants to test their systems against various anonymization methods and evaluate their performance using automatic speaker verification metrics.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Exploring the Potential of Large Language Models for Heterophilic Graphs</title><link>https://arxiv.org/abs/2408.14134</link><description>https://arxiv.org/abs/2408.14134&lt;br /&gt;The First VoicePrivacy Attacker Challenge focuses on developing and evaluating attacker systems against voice anonymization, providing datasets and a baseline attacker system for participants to enhance their automatic speaker verification systems.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Planning In Natural Language Improves LLM Search For Code Generation</title><link>https://arxiv.org/abs/2409.03733</link><description>https://arxiv.org/abs/2409.03733&lt;br /&gt;The First VoicePrivacy Attacker Challenge is an initiative aimed at developing automatic speaker verification systems to challenge voice anonymization methods, with participants evaluated based on their systems' performance against provided anonymization systems using specified datasets and metrics.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context Scenarios</title><link>https://arxiv.org/abs/2409.10593</link><description>https://arxiv.org/abs/2409.10593&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to evaluate systems designed to attack voice anonymization methods by comparing attacker systems against submissions from the VoicePrivacy 2024 Challenge, utilizing provided datasets and a baseline system to assess performance based on equal error rate (EER).</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>A New Perspective on ADHD Research: Knowledge Graph Construction with LLMs and Network Based Insights</title><link>https://arxiv.org/abs/2409.12853</link><description>https://arxiv.org/abs/2409.12853&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to develop and evaluate systems that attack voice anonymization techniques, assessing their effectiveness against anonymization systems in a competitive format supported by ICASSP 2025.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching</title><link>https://arxiv.org/abs/2409.14038</link><description>https://arxiv.org/abs/2409.14038&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to develop and evaluate attacker systems against voice anonymization using automatic speaker verification methodologies, providing necessary datasets and a baseline system for participants to refine their approaches.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Show and Guide: Instructional-Plan Grounded Vision and Language Model</title><link>https://arxiv.org/abs/2409.19074</link><description>https://arxiv.org/abs/2409.19074&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to develop and evaluate attacker systems against voice anonymization techniques, providing datasets and baseline systems for participants to improve upon and compete in measuring equal error rates during ICASSP 2025.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>From Text to Multimodality: Exploring the Evolution and Impact of Large Language Models in Medical Practice</title><link>https://arxiv.org/abs/2410.01812</link><description>https://arxiv.org/abs/2410.01812&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to develop and evaluate automatic speaker verification systems against voice anonymization techniques, providing datasets and a baseline for participants to create effective attack systems which will be evaluated based on equal error rate.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>DEPT: Decoupled Embeddings for Pre-training Language Models</title><link>https://arxiv.org/abs/2410.05021</link><description>https://arxiv.org/abs/2410.05021&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to develop and evaluate systems that can effectively attack voice anonymization methods, with participants creating automatic speaker verification systems to assess their performance against existing anonymization technologies.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>The First VoicePrivacy Attacker Challenge Evaluation Plan</title><link>https://arxiv.org/abs/2410.07428</link><description>https://arxiv.org/abs/2410.07428&lt;br /&gt;The First VoicePrivacy Attacker Challenge aims to develop and evaluate automatic speaker verification systems against various voice anonymization methods, providing datasets and metrics for participants to assess their effectiveness.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal Large Language Models</title><link>https://arxiv.org/abs/2410.08474</link><description>https://arxiv.org/abs/2410.08474&lt;br /&gt;MathGAP is a framework for evaluating the generalization ability of large language models (LLMs) on complex arithmetic proofs, revealing that most models struggle significantly with deeper and more intricate proof structures despite the use of in-context learning.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Towards Trustworthy Knowledge Graph Reasoning: An Uncertainty Aware Perspective</title><link>https://arxiv.org/abs/2410.08985</link><description>https://arxiv.org/abs/2410.08985&lt;br /&gt;MathGAP is a framework introduced to evaluate Large Language Models (LLMs) on arithmetic problems with complex proofs, addressing issues of contamination in evaluation data and the arbitrary complexity of proof structures by generating systematic test problems.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>$\alpha$-DPO: Adaptive Reward Margin is What Direct Preference Optimization Needs</title><link>https://arxiv.org/abs/2410.10148</link><description>https://arxiv.org/abs/2410.10148&lt;br /&gt;MathGAP is a framework designed for evaluating Large Language Models (LLMs) on arithmetic problems with arbitrarily complex proofs, revealing that most models exhibit decreased performance on deeper and wider proof structures, highlighting challenges in generalization.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>FLARE: Faithful Logic-Aided Reasoning and Exploration</title><link>https://arxiv.org/abs/2410.11900</link><description>https://arxiv.org/abs/2410.11900&lt;br /&gt;MathGAP introduces a framework for evaluating large language models (LLMs) on arithmetic problems with complex proofs, revealing that LLM performance significantly declines as proof complexity increases, particularly in nonlinear structures while also challenging existing evaluation methods due to contaminated data.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical Decision-Support Setting</title><link>https://arxiv.org/abs/2410.12284</link><description>https://arxiv.org/abs/2410.12284&lt;br /&gt;MathGAP is a new evaluation framework designed to assess the generalization capabilities of Large Language Models (LLMs) on arithmetic problems with arbitrarily complex proofs, revealing significant drops in performance as proof complexity increases.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>Chain of Ideas: Revolutionizing Research in Novel Idea Development with LLM Agents</title><link>https://arxiv.org/abs/2410.13185</link><description>https://arxiv.org/abs/2410.13185&lt;br /&gt;MathGAP is a novel evaluation framework designed to assess how well large language models (LLMs) generalize to arithmetic problems with arbitrarily complex proofs, revealing a significant performance decrease as proof complexity increases.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item><item><title>MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily Complex Proofs</title><link>https://arxiv.org/abs/2410.13502</link><description>https://arxiv.org/abs/2410.13502&lt;br /&gt;MathGAP is a framework designed to evaluate large language models' generalization capabilities on problems involving arbitrarily complex arithmetic proofs, revealing significant performance drops with increasing proof complexity and challenging nonlinear structures.</description><pubDate>Tue, 22 Oct 2024 10:17:07 GMT</pubDate></item></channel></rss>