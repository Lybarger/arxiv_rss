<?xml version="1.0" encoding="iso-8859-1"?>
<rss version="2.0"><channel><title>Irrelevant arXiv Papers</title><link>https://Lybarger.github.io/arxiv_rss/rss_irrelevant.xml</link><description>Irrelevant arXiv Papers</description><lastBuildDate>Mon, 21 Oct 2024 09:52:53 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Boosting LLM Translation Skills without General Ability Loss via Rationale Distillation</title><link>https://arxiv.org/abs/2410.13944</link><description>https://arxiv.org/abs/2410.13944
RaDis is a novel approach that enhances the translation skills of Large Language Models (LLMs) through rationale distillation, allowing the models to learn new tasks while preserving their general capabilities and aligning with human preferences.</description><pubDate>Mon, 21 Oct 2024 09:52:53 GMT</pubDate></item><item><title>From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization</title><link>https://arxiv.org/abs/2410.13961</link><description>https://arxiv.org/abs/2410.13961
This paper investigates how hallucinations occur in Large Language Models (LLMs) during multi-document summarization (MDS), revealing that up to 75% of content in generated summaries can be fabricated and highlighting the need for improved methods to mitigate these hallucinations.</description><pubDate>Mon, 21 Oct 2024 09:52:53 GMT</pubDate></item><item><title>Detecting AI-Generated Texts in Cross-Domains</title><link>https://arxiv.org/abs/2410.13966</link><description>https://arxiv.org/abs/2410.13966
This paper introduces RoBERTa-Ranker, a modified ranking classifier designed to improve the detection of AI-generated texts across different domains by fine-tuning with minimal labeled data, outperforming existing tools like DetectGPT and GPTZero.</description><pubDate>Mon, 21 Oct 2024 09:52:53 GMT</pubDate></item><item><title>Are LLMs Models of Distributional Semantics? A Case Study on Quantifiers</title><link>https://arxiv.org/abs/2410.13984</link><description>https://arxiv.org/abs/2410.13984
This study investigates whether large language models (LLMs) align with distributional semantics by evaluating their performance on vague versus exact quantifiers, revealing that LLMs better reflect human judgments on exact quantifiers than previously expected.</description><pubDate>Mon, 21 Oct 2024 09:52:53 GMT</pubDate></item><item><title>RiTeK: A Dataset for Large Language Models Complex Reasoning over Textual Knowledge Graphs</title><link>https://arxiv.org/abs/2410.13987</link><description>https://arxiv.org/abs/2410.13987
RiTeK is a dataset designed to improve Large Language Models' (LLMs) reasoning capabilities over textual knowledge graphs by synthesizing complex user queries and introducing an enhanced Monte Carlo Tree Search method to extract relational paths from these graphs, primarily focusing on the medical domain.</description><pubDate>Mon, 21 Oct 2024 09:52:53 GMT</pubDate></item><item><title>LLMs are Biased Teachers: Evaluating LLM Bias in Personalized Education</title><link>https://arxiv.org/abs/2410.14012</link><description>https://arxiv.org/abs/2410.14012
This paper evaluates the biases present in large language models (LLMs) used as personalized educators, revealing significant disparities in educational content generation based on various demographic factors and introducing metrics to quantify these biases.</description><pubDate>Mon, 21 Oct 2024 09:52:53 GMT</pubDate></item><item><title>Measuring and Modifying the Readability of English Texts with GPT-4</title><link>https://arxiv.org/abs/2410.14028</link><description>https://arxiv.org/abs/2410.14028
This paper investigates the ability of Large Language Models (LLMs) like GPT-4 to assess and modify the readability of English texts, finding that their estimates correlate well with human judgments and showing evidence that they can adjust text readability, although with some unexplained variances in human assessments.</description><pubDate>Mon, 21 Oct 2024 09:52:53 GMT</pubDate></item><item><title>Efficient Retrieval of Temporal Event Sequences from Textual Descriptions</title><link>https://arxiv.org/abs/2410.14043</link><description>https://arxiv.org/abs/2410.14043
The paper presents TPP-LLM-Embedding, a unified model for efficiently embedding and retrieving temporal event sequences from textual descriptions using large language models and temporal point processes, demonstrating superior performance on various datasets.</description><pubDate>Mon, 21 Oct 2024 09:52:53 GMT</pubDate></item></channel></rss>